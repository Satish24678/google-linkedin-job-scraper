Job Title,Company,Location,Job Description,Job URL,Job Posting Time
Data Engineer - Periscope,McKinsey & Company,"Gurugram, Haryana, India (+1 other)","Your Growth You’ll work with our Periscope team in our Gurgaon and Bangalore, India office. Periscope is the technology backbone of McKinsey’s Growth, Marketing & Sales Practice. Founded in 2007, it combines world-leading Intellectual Property, prescriptive analytics, and cloud-based tools, with expert support and training. This unique combination drives revenue growth – now, and in the future. The platform offers a suite of Growth, Marketing & Sales solutions that accelerate and sustain commercial transformation for businesses.

The Growth, Marketing & Sales Practice strives to help clients in both consumer and business-to-business environments on a wide variety of marketing and sales topics. Our clients benefit from our experience in core areas of sales and marketing topics such as sales and channel management, branding, customer insights, marketing ROI, digital marketing, CLM, and pricing. Our Practice offers an exceptional opportunity to work at the intersection of sales, marketing, and consulting. Focusing on issues like redefining sales and marketing operations and commercial transformation, our people help clients build capabilities and transform how companies go to market-moving them to customer-centric organizations.

Periscope leverages its world-leading IP (largely from McKinsey but also other partners) and best-in-class technology to enable transparency into Big Data, create actionable insights, and new ways of working that drive lasting performance improvement, and typically sustain a 2-7% increase in return on sales (ROS). With a truly global reach, the portfolio of solutions is comprised of: Marketing Solutions, Customer Experience Solutions, Category Solutions, B2C Pricing Solutions, B2B Pricing Solutions, and Sales Solutions. These are complemented by ongoing client service and custom capability building programs.

Periscope has a presence in 27 locations across 16 countries with a team of 800+ IT and business professionals and a network of 300+ experts. To learn more about how Periscope’s solutions and experts are helping businesses continually drive better performance, visit www.mckinsey.com/periscope

Your ImpactYou will be part of the data delivery team and will have the opportunity to develop a deep understanding of the domain/function.

You will design and drive the work plan for the optimization/automation and standardization of the processes incorporating best practices to achieve efficiency gains.

You will run data engineering pipelines, link raw client data with data model, conduct data assessment, perform data quality checks, and transform data using ETL tools.

You will perform data transformations, modeling, and validation activities, as well as configure applications to the client context. You will also develop scripts to validate, transform, and load raw data using programming languages such as Python/PySpark/SparkSQL. You’ll also be required to analyze and visualize data using Power BI to provide insights to support business decisions.

In this role, you will determine database structural requirements by analyzing client operations, applications, and programming.

Given the pace of this role, you will develop cross-site relationships to enhance idea generation, and manage stakeholders. Lastly, you will collaborate with the team to support ongoing business processes by delivering high-quality end products on-time and perform quality checks wherever required.

Your qualifications and skills
• Bachelor’s degree/master's degree with high rankings, with 3+ years of professional work experience
• Excellent written and verbal communication skills in English
• Self-motivated with strong sense of problem-solving, ownership and action-oriented mindset
• Able to cope with pressure and demonstrate a reasonable level of flexibility/adaptability
• Track record of strong problem-solving, requirement gathering, and leading by example
• Able to work well within teams across continents/time zones with a collaborative mindset",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Tj0hmhPFvTY9OVviAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMTQrCMBBAYdz2CK5m5UJqUgQ3ulTxD8EblGkc0kg7EzIptKfxqtbN41u94rsoqhNmhDP7wEQJNvCiFNRJpNl3aUAJk2tBGC4ivqPloc056t5a1c54zZiDM056K0yNjPYjjf5Ta4uJYoeZ6u2uGk1kvzZP9wisNMEKjtJH5AnCvB7S4BP2JVwxTchYwo3fAX_kGvoWogAAAA&shmds=v1_AUFQtONBwCIMRA0kYrQxYWH9PGw4DgWQrhDXUen7rFAkQgntPw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Tj0hmhPFvTY9OVviAAAAAA%3D%3D,N/A
Staff IT Data Engineer,Palo Alto Networks,"Bengaluru, Karnataka, India","Our Mission

At Palo Alto Networks® everything starts and ends with our mission:

Being the cybersecurity partner of choice, protecting our digital way of life.
Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.

Who We Are

We take our mission of protecting the digital way of life seriously. We are relentless in protecting our customers and we believe that the unique ideas of every member of our team contributes to our collective success. Our values were crowdsourced by employees and are brought to life through each of us everyday - from disruptive innovation and collaboration, to execution. From showing up for each other with integrity to creating an environment where we all feel included.

As a member of our team, you will be shaping the future of cybersecurity. We work fast, value ongoing learning, and we respect each employee as a unique individual. Knowing we all have different needs, our development and personal wellbeing programs are designed to give you choice in how you are supported. This includes our FLEXBenefits wellbeing spending account with over 1,000 eligible items selected by employees, our mental and financial health resources, and our personalized learning opportunities - just to name a few!

At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work full time from our office with flexibility offered where needed. This setup fosters casual conversations, problem-solving, and trusted relationships. Our goal is to create an environment where we all win with precision.

Your Career

Our Data & Analytics group is responsible for working with various business owners/stakeholders from Sales, Marketing, People, GCS, Infosec, Operations, and Finance to solve complex business problems which will have a direct impact on the metrics defined to showcase the progress of Palo Alto Networks. We leverage the latest technologies from the Cloud & Big Data ecosystem to improve business outcomes and create through prototyping, Proof-of-Concept projects and application development. We are looking for a Staff IT Data Engineer with extensive experience in Data engineering, SQL, Cloud engineering and business intelligence (BI) tools. The ideal candidate will be responsible for designing, implementing, and maintaining scalable data transformations and analytical solutions that support our business objectives. This role requires a strong understanding of data engineering principles, as well as the ability to collaborate with cross-functional teams to deliver high-quality data solutions.

Your Impact
• Design, develop, and maintain data pipelines to extract, transform, and load (ETL) data from various sources into our data warehouse or data lake environment.
• Collaborate with stakeholders to gather requirements and translate business needs into technical solutions.
• Optimize and tune existing data pipelines for performance, reliability, and scalability.
• Implement data quality and governance processes to ensure data accuracy, consistency, and compliance with regulatory standards.
• Work closely with the BI team to design and develop dashboards, reports, and analytical tools that provide actionable insights to stakeholders.
• Mentor junior members of the team and provide guidance on best practices for data engineering and BI development.

Your Experience
• Bachelor's degree in Computer Science, Engineering, or a related field.
• 5+ years of experience in data engineering, with a focus on building and maintaining data pipelines and analytical solutions.
• Expertise in SQL programming and database management systems
• Hands-on experience with ETL tools and technologies (e.g. Apache Spark, Apache Airflow).
• Familiarity with cloud platforms such as Google Cloud Platform (GCP), and experience with relevant services (e.g. GCP Dataflow, GCP DataProc, Biq Query, Procedures, Cloud Composer etc).
• Experience with Big data tools like Spark, Kafka, etc.
• Experience with object-oriented/object function scripting languages: Python/Scala, etc
• Experience with BI tools and visualization platforms (e.g. Tableau) is a plus.
• Experience with SAP HANA, SAP BW, SAP ECC, or other SAP modules is a plus
• Strong analytical and problem-solving skills, with the ability to analyze complex data sets and derive actionable insights.
• Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.

The Team

We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.

At Palo Alto Networks, you'll have the opportunity to work on challenging projects that have a direct impact on our business success. We offer a collaborative and dynamic work environment where your contributions are valued and recognized. Join us in shaping the future of data-driven decision-making in our organization and unlock your full potential as a Staff IT Data Engineer.

We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.

Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.

All your information will be kept confidential according to EEO guidelines.

Our Commitment

We’re problem solvers that take risks and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.

We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.

Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.

All your information will be kept confidential according to EEO guidelines.

Is role eligible for Immigration Sponsorship? No. Please note that we will not sponsor applicants for work visas for this position.

Covid-19 Vaccination Information for Palo Alto Networks Jobs
• Vaccine requirements and disclosure obligations vary by country.
• Unless applicable law requires otherwise, you must be vaccinated for COVID or qualify for a reasonable accommodation if:
• The job requires accessing a company worksite
• The job requires in-person customer contact and the customer has implemented such requirements
• You choose to access a Palo Alto Networks worksite
• If you have questions about the vaccine requirements of this particular position based on your location or job requirements, please inquire with the recruiter.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=kYl8oOdKrIjPRwl_AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOsQrCMBAAUFz7CU43S9uo4KKTokgVRNC9XOs1jY13Jbli_8ZfVZc3v-QzSbKbYtNAcYc9KsKBrWOiABmcpIJIGOoWhOEoYj1NN61qH9fGxOhzGxXV1XktLyNMlYzmKVX8U8YWA_Uelcrlaj7mPdvZ4opeYOtV4EL6ltBFcAw7Yot-CEMKZwz8a3SYQsEPh1_vd9brogAAAA&shmds=v1_AUFQtOMLA7la21hIyhJOhtP8Rn91sQ89YWl9YEuTgM2mbts8_g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=kYl8oOdKrIjPRwl_AAAAAA%3D%3D,N/A
Data Engineer,Chubb External,"Madhavaram, Telangana, India","• Minimum of 4 -9 years of experience in ETL development using IICS-CDI (Cloud Data Integration),

including experience with IICS Cloud Console and PowerCenter Designer
• Design, develop and implement ETL solutions using IICS - CDI to extract, transform and load data from various sources into data warehouse
• Strong understanding of data warehousing concepts, ETL frameworks, and best practices
• Solid experience with SQL and database technologies such as SQL Server, Oracle etc. (Preferably Azure Synapse)
• Should be Familiar with data modeling and data integration techniques
• Be able to Work with cross-functional teams to understand business requirements and translate them into technical specifications for ETL development
• Develop and maintain ETL mappings, workflows, and schedules using IICS - CDI
• Ensure data quality, integrity, and consistency by performing data validation, cleansing, and enrichment activities
• Monitor ETL jobs to ensure successful completion, identify and resolve errors and performance issues
• Nice to have - Experience with Informatica Cloud Secure Agents
• Familiarity with cloud technologies such as Azure, AWS or Google Cloud Platform
• Experience with scripting languages such as Python or Shell scripting
• Excellent analytical and problem-solving skills, with a keen attention to detail
• Ability to work independently and in a team-oriented, collaborative environment
• Strong communication and interpersonal skills
• Familiarity with Agile development methodologies
• Preferred Certifications:
• Informatica Cloud Data and Application Integration R38, Professional Certification
• AZ-900: Microsoft Azure Fundamentals
• DP-900: Microsoft Azure Data Fundamentals
• DP-203: Data Engineering on Microsoft Azure",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=T7XmDq0EDh5_s4FpAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAGYFz7CE43OIkmIrjoqEUU3NzLn_ZIIuldSaL0PXxh8Ru-5rtoVhdUUCs-CnOmLd3VUWHkPpAKXVV94uUp1DqVo7WlJONLRY296XW0Kux0ti915V9XAjJPCZW7_WE3m0n82pzD2zlq58pZkCgKPTAEfJAxbujJCeIh2NBNhogfkn1_D5YAAAA&shmds=v1_AUFQtONAakVCdb09CDT8fet-mtg6VovZD9SDgxU2wYwNnYowOw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=T7XmDq0EDh5_s4FpAAAAAA%3D%3D,N/A
Data Solutions Engineer III,OpenGov,"Pune, Maharashtra, India","OpenGov is home to an exceptional team - passionate about our mission to power more effective and accountable government. By bringing the OpenGov Cloud to our nation's state and local government, we’re transforming communities so they can thrive!

Imagine yourself being able to help small business owners open their doors faster, ensuring our tax dollars are accounted for, creating safer infrastructure, modernizing the permitting process, and assisting with disaster recovery. The work you do here every day has a meaningful impact on people's lives!

🌟 OpenGov is a 2023 Top Workplaces USA award winner and a Forbes America's Best Startup Employer 🌟

Join our smart, fun, and humble team to experience the most rewarding career of your life!

Job Summary:

The Data Solutions Engineer III is a fully qualified, experienced professional responsible for driving complex customer data integration implementations from start to finish. This role requires sophisticated analytical and problem solving techniques to assess and address unique customer needs by way of database schema knowledge, ETL tooling, and scripting to maximize the value for the customer.As a trusted advisor, the Data Solutions Engineer combines their deep expertise in database systems and data migrations with a strategic perspective to guide customers through their challenging integration and migration needs. By building strong relationships with senior internal and other external stakeholders, this position influences projects to deliver impactful solutions.

Responsibilities:
• Serve as a key technical advisor and thought leader, leveraging expertise to guide and influence stakeholders on complex data integration projects.
• Lead the end-to-end implementation of advanced SQL and ETL solutions, addressing diverse and often unique challenges in customer workflows.
• Proactively assess customer needs, using sophisticated problem-solving techniques to propose creative and scalable solutions.
• Partner with senior government personnel and internal cross-functional teams to design and deliver tailored integration strategies.
• Identify and drive opportunities for process improvement, collaborating with internal teams to simplify, enhance, and automate workflows that optimize customer experience.
• Independently manage a diverse portfolio of 10-15 projects, using strong organizational and time management skills to meet critical deadlines.
• Create and refine detailed requirements documentation to ensure alignment with customer goals and technical specifications.
• Mentor junior team members and provide guidance to peers on best practices and innovative approaches for data integrations and migrations.
• Stay informed on the latest trends in SaaS, APIs, and cloud technologies, integrating this knowledge into customer projects to drive innovation and value.

Requirements and Preferred Experience:
• Minimum of 5 years of related experience with a Bachelor’s degree
• 1+ years of experience in professional services delivery
• 2+ years SQL scripting experience
• 1+ years of Python, or similar, experience
• Experience with different database systems
• Ability to adapt to a rapidly changing product and respond strategically to customer needs
• Strong interpersonal skills
• Experience with budgeting concepts and practices and/or finance/budgeting solutions nice to have
• Organize own work for daily, weekly, and monthly work plans
• Proactively communicate deadlines, updates, and issues to relevant parties
• Experience with SaaS solutions, APIs, or Cloud technologies
• Experience with Agile & Scrum methodologies",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=eAoQDSITMbtQt9c_AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7CU4HblIbKbjoqpQIouAHlGs9kki8C7lU-iV-r7q86VWfRdUesSDcJU4lCCuc2AUmymCthQ2cZQAlzKMHYehEXKTlwZeSdG-MamycFixhbEZ5GWEaZDZPGfRPrx4zpYiF-na3nZvEbr26JuJO3hAYbhNTDRf8NVRfMtZg-RHwC1koVp-ZAAAA&shmds=v1_AUFQtOMnaMerhD-x6uV-W3YPo8TwecDujmySSUzZp2thRuOZ7Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=eAoQDSITMbtQt9c_AAAAAA%3D%3D,19 hours ago
Clinical Data Engineer (RWD),Astellas,"Maharashtra, India","Through close collaboration with study leads, research scientists, statisticians, clinicians, regulatory experts, and DigitalX professionals, this role establishes and upholds robust data architecture frameworks that align harmoniously with business objectives, regulatory mandates, and industry standards. Expertise in Data Engineering, Data Modeling, and adeptly managing governance processes is essential for maintaining the integrity, security, and accessibility of data assets. This role holds a strategic position in advancing the life sciences company's mission by using data to drive scientific progress, improve patient outcomes, and efficiently and securely introduce ground-breaking therapies to the market.

Responsibilities and Accountabilities:
• Responsible for data operations on AWS architecture. Activities include creating and monitoring the data load ETL processes, ensuring data quality, automation of the loads and continuous improvement.
• Collaborate with internal and external teams for data curation, cataloging and metadata management.
• Strong knowledge of RWD assets including data sources like IQVIA, SYMPHONY and various other OMICS sources.
• Ensure the operations schedule and knowledge base are maintained and communicated promptly to the end users.
• Assist in enforcing data governance and information management policies, SOPs, and standards to manage patient data in a secure and compliant manner.
• Strong proficiency in Python and Django framework including experience with web technologies such as HTML, CSS, JavaScript, and AJAX.
• Understanding of RESTful APIs and web services integration. Additionally, Familiarity with deployment tools such as Docker, Heroku, AWS, or Azure.
• Knowledge of database management systems, particularly PostgreSQL or MySQL.
• Experience with unit testing and test-driven development (TDD).
• Experience with JavaScript frameworks like React, Angular, or Vue.js and other modern API technologies.
• Identify areas for improvement with current analytics tool sets and propose future state to support DigitalX use cases, analytics best practices, and regulations.
• Improving the delivery of large complex projects by implementing standard processes, tools, and templates that will be used consistently across Digital.
• Participate in several concurrent projects in Advanced Analytics Solutions for the various functions across Astellas in a fast-paced environment.
• Ensure the RWD Analytics environments are running at an optimal state and quickly resolve any technical issues.
• Participate in process improvements initiatives involving business areas.
• Collaborate with Advanced Analytics Solution teams to identify required technology architecture needs to design and implement a solution delivering near-term impact and aligned with long term strategies.
• Implement security requirements, metadata standards, data quality and testing procedures for the data lake/warehouse consistent with analytic and RWD best practices.

Required Qualifications:
• Bachelor of Science degree in Computer Science, Information Systems, Data Science, or a related field.
• 5+ years of relevant experience working in in data architecture, engineering roles or related roles within a healthcare industry.
• Data architecture and engineering experience

Preferred Qualifications:
• Master of Science degree in Computer Science, Information Systems, Data Science, or a related field.
• 3+ years’ experience in Life Sciences industry
• Expertise in ETL, data modelling, and data integration techniques.
• Proficiency in programming languages commonly used in RWD data ingestion, such as Python, and Django framework.
• Expertise in working with web technologies such as HTML, CSS, JavaScript, and AJAX
• Strong command in RESTful APIs and web services integration.
• Knowledge of database management systems, particularly PostgreSQL or MySQL
• In-depth understanding of life sciences business processes, adept at translating business requirements into effective technical solutions.
• Innovative problem-solving abilities for addressing complex data-related challenges.
• Experience with Agile methodology and mindset.
• Excellent communication and interpersonal skills, enabling effective collaboration with cross-functional teams, business stakeholders, and technical experts.
• Project management capabilities, ensuring adherence to timelines for successful solution delivery.
• Relevant certifications in cloud computing and data engineering tools/platforms are advantageous.

Category Bold X

Astellas is committed to equality of opportunity in all aspects of employment.

EOE including Disability/Protected Veterans",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=7KTvu-YcG0M0aW9VAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQ4BQRAA0GjvExQyJcKtEA2VOBESjUZ5mVuT3ZU1c9mZ4r7DF6N51as-o2pzzImTxwwNGsKJQ2KiAtP7o5nBEq7SgRIWH0EYziIh03gfzXrdOaea66CGlnzt5e2EqZPBvaTTP61GLNRnNGrX29VQ9xzmk4Ma5YwKieGGv4EareACLvxM-AWaufWTlQAAAA&shmds=v1_AUFQtOP8ZlZxTmo2FIbgcCcVuiGXTF7st57sHk-lE9G3x6F4RQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=7KTvu-YcG0M0aW9VAAAAAA%3D%3D,6 hours ago
Manager Data Engineering,Mr. Cooper,"Chennai, Tamil Nadu, India","At Mr. Cooper Group, You Make the Dream Possible.

Our purpose is simple: Keeping the dream of homeownership alive. As a Mr. Cooper Group team member, you play a big role in making that dream possible. Around here, we know our roles and work together, volunteer to make a difference, and challenge the status quo when needed. Everything we do is in the care and service of our teammates and our customers.

Join us and make the dream of home ownership possible!
Job Description:

The Manager, Data Engineering will lead a team of data engineers responsible for designing, developing, and maintaining robust data systems and pipelines. This role is critical for ensuring the smooth collection, transformation, and storage of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with data scientists, analysts, and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.
Key Responsibilities:
• Team Leadership: Manage, mentor, and guide a team of data engineers, ensuring their professional development and optimizing team performance.
• Data Infrastructure Management: Oversee the design, development, and maintenance of data pipelines, ensuring that data is collected, cleaned, and stored efficiently.
• Collaboration: Work closely with data scientists, analysts, and other departments to ensure the data infrastructure supports organizational goals and provides reliable data for business decisions.
• Process Improvement: Continuously evaluate and improve data processes, ensuring scalability, performance, and reliability of data systems.
• Data Governance: Ensure data quality, integrity, and security standards are met, and collaborate with stakeholders to enforce best practices in data management.
• Technology Selection: Evaluate and implement new tools, technologies, and best practices to improve the efficiency of data engineering processes.
• Project Management: Lead and manage data engineering projects, ensuring they are delivered on time and within scope.
Skills and Qualifications:
• Experience: 5+ years of experience in data engineering, with at least 2 years in a managerial or leadership role.
• Technical Expertise: Strong knowledge of data engineering concepts, including data warehousing, ETL processes, and data pipeline design. Proficiency in SQL, Python, and other data engineering tools.
• Leadership Skills: Proven ability to lead and motivate a team of engineers while managing cross-functional collaborations.
• Problem-Solving: Strong analytical and troubleshooting skills to address complex data-related challenges.
• Communication: Excellent verbal and written communication skills to effectively interact with technical and non-technical stakeholders.
• Data Architecture: Experience with designing scalable, high-performance data systems and understanding cloud platforms such as AWS, Google Cloud, or Azure is a plus.
Preferred Qualifications:
• Familiarity with Big Data technologies like Hadoop, Spark, or similar frameworks.
• Experience with data modeling, data warehousing, and data governance best practices.
• Knowledge of machine learning concepts and integration with data pipelines is a plus.

Mr. Cooper Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or status as a protected veteran. EOE/M/F/D/V

Job Requisition ID:
023563

Job Category:
Information Technology

Primary Location City:
Chennai

Primary Location Region:
Tamil Nadu

Primary Location Postal Code:
600089

Primary Location Country:
India

Additional Posting Location(s):",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=onmff74EQ97vH7wrAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU43itREBBcdq4hCndzLtT2SlPQu5CL0U_xcdXnrqz6ryrbI6CjDBQvClV1gohzYwQ4e0oMS5sGDMNxEXKT12ZeS9GStajROC5YwmEFmK0y9LHaSXv906jFTilioOxz3i0nstps2G2hE0i8MDI0nZgw1vHAOEZ44vmu48xjwC-rXK7ibAAAA&shmds=v1_AUFQtONW7-cjUGkYsr-GvZ4fj9V2nYEn-jMLDmveAmzP1c_JKw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=onmff74EQ97vH7wrAAAAAA%3D%3D,8 days ago
Big Data Engineer- Aftersales,Mercedes-Benz Research and Development India Private Limited,"Bengaluru, Karnataka, India","Tasks

Key roles and responsibilities include:

· Work with Business & IT teams to gather, understand and define requirements and break down Demands into EPICS and User Stories, where needed.

· Design and build end-to-end CI/CD data pipelines / deployments to get the relevant data and for the deliverables

· Reviewing design, code and other deliverables created by your team to guarantee high-quality results

· Define acceptance criteria, manage & perform testing of own pipelines to ensure expected quality

· Own the status of Change requirements throughout the lifecycle & adapt changes to the existing scripts, codes and pipelines.

· Own PoCs and deliver the results in reasonable time

· Ensuring appropriate use of MB IT project management tools, governance, processes, policies and methodologies

· Bring value adds by contributing to continuous improvement and innovation, encourage best practices, challenge current practices, provide feedback to colleagues

· Presents user stories in demand concept meetings together with respective stakeholders

· Align with architects if and as needed

· Detect dependencies to other systems/demands

· Conduct regular status meetings with all stakeholders, keeping the stakeholders needs and requirements continuously in view

Qualifications

Mandatory Requirements

· Total 3-6 Years of Experience in cloud with data engineering & analytics

· Bachelor's Degree in computer science or equivalent

· Hands on experience in Azure and Azure Data Factory, Storage accounts and Databricks, PySpark, DeltaLake, SQL

· Hands on experience in Azure DevOps/GIT, creating CI/CD pipelines and setting up branching / deployment strategies of data engineering

· Good knowledge in Azure key vaults, Authentication methods, Data security and error handling in Azure Data Factory and Azure DataBricks

· Good experience with DWH, ETL/ELT process, hands-on expertise ETL tools, relational DB or MPP, query tuning and performance tuning, and Apache Kafka

· Good knowledge in Agile(Scrum, DevOps Boards etc.)

· Exposure to log analytics and debugging

· Good presentation, communication (written and verbal English) skills, and interpersonal skills and comfortable in liaising with stakeholders independently

· Ability to juggle and prioritize multiple tasks within a collaborative team environment

· Drive and desire to learn and grow both technical and functional skill sets

· Exposure to client facing roles both from IT and Business

· Proven ability leveraging analytical and problem-solving skills in a fast paced environment

Preferred Experience and Skills

· Microsoft Azure Certifications

· Good knowledge of data architecture patterns (Data lakehouse, delta lake, streaming, Lambda/Kappa architecture)

· Experienced in Powershell scripting for orchestration",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=V4bIyehjnHAxV89_AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_y2OwU7CUBBF45ZPcDVrQ1tDZKMrCIQgmhi2Lsi0vb4-eZ1p3gyE-JV-kiW4Oat7bs7k927ytIyBVuxMawlRgFzQ4suRjROMCnrVmgycm45UaKMaEu5fOvfBnqvKLJXBnD02ZaN9pYJaL9W31nbFwTrOGBI7DrP546UcJDx8viM3aGHFEvJDe_zfs7S0whlJhx7itJU2Mn3keB51eot9dLQUhUYtcDrl05R2nGWMP_L0Nv8DZwfwnNMAAAA&shmds=v1_AUFQtOOHQZx7yBbm69leHMBen3-jr_VmClTXgdPgALxdtwvNPQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=V4bIyehjnHAxV89_AAAAAA%3D%3D,N/A
"Senior Software Engineer - C#, Data Engineer",Capgemini,"Pune, Maharashtra, India","Job Description
• Software Design and Development: Design, development, and maintenance of complex software applications using C# WPF
• Code Quality and Review: Ensure high standards of code quality through regular code reviews and adherence to best practices
• Problem-Solving: Analyze and resolve complex software issues, ensuring efficient and scalable solutions
• Collaboration: Work closely with cross-functional teams, including product managers, designers, and other stakeholders, to understand project requirements and deliver technical solutions
• Technical Documentation: Develop and maintain comprehensive technical documentation for reference and reporting
• Innovation and Improvement: Stay updated with industry trends and integrate new technologies and methodologies to drive innovation and continuous improvement
• C# WPF

Works in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.
1. Applies scientific methods to analyse and solve software engineering problems.
2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.
3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.
4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.
5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.

Skills (competencies)

Verbal Communication",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=m2AR-F6SphaEXiFkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_02NMQrCQBAAsc0TrBbERmIigo2WUURBEPIA2cT17iTZPW5XzJf8pbGzmWIYmOwzyaqaOEiCWh72xkRwYBeYKMESqlkOezT8d2dpQAlT60EYjiKuo-nOm0XdlqVqVzg1tNAWrfSlMDUylE9p9Ieb-nEROzS6rTeroYjsFvMKo6M-cIDAcH0x5XDBMUT1ljCHE98DfgG1RT2SrAAAAA&shmds=v1_AUFQtOOHBwj9_wiUNqHMdsB-ygPBFG89XiTYjK81xRXCzKmh9w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=m2AR-F6SphaEXiFkAAAAAA%3D%3D,6 days ago
TCS Hiring for Azure Data Engineer with Snowflake Pune,Tata Consultancy Services,"Salem, Tamil Nadu, India","TCS Hiring for Azure Data Engineer with Snowflake

Experience: 4 to 14 Years Only

Job Location: Pune Only

Azure Data Engineer with Snowflake

Role & Responsibilities:

Must Have:
• Snowflake
• Azure Data factory
• AzureDataLake
• Power BI
• SQL

Desired Competencies (Technical/Behavioral Competency)

Must-Have

Primary Skills:

Azure Data factory (Pipelines/Data flows),AzureDataLake-Gen2Storage,Logic Apps,SQL Server Analysis Services(Tabular),Power BI(Dataset Creation only),DAX,T-SQL (Stored procedure, views etc), Azure Devops and Snowflake
• 2 to 4 Years of Experience in Snowflake
• 4 to 6 Years of Hands-on Experience with SQL Server in writing complex T-SQL queries and logics
• 4 to 6 Years of Experience using SSIS as a ETL tool - must have experience in complex ETL's building, Deployment, configuration of SSIS Packages
• 2 to 4 Years of Experience in Microsoft Azure & Azure SQL DB's
• 2 to 4 Years of Hands-on Experience in using Azure Data Factory as a ETL/ELT Tool
• Must know about connecting to various Azure Storages and other On-prem and cloud sources and destinations using ADF.
• Must have knowledge about Azure DevOps
• Must Have worked on Agile Methodology.

Good-to-Have

Secondary Skills:
• Data Modelling, SQL Server Integration Services (SSIS), Team Foundation Server (TFS) through Visual Studio, ServiceNow, Power BI, Streamlit, Fivetran
• Good To have hands on experience with SSAS, Power Bi
• Added Advantage if there is any relevant C .Net experience.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=XcHXVU6gK8xY-7IGAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNwU7CQBAG4HjlETz9Z4MtMeGiJwMGJdGYtHcyXYbtyHam2dkK-nq8GOHyXb_Z5W722a4avEsWjThYxuv_lBlrKoQ3jaLMGScpPRq10yHRkfE9KeMRW-vgTDn0MMXGLCa-f-lLGf25rt1TFb1QkVAFG2pT7uxc_1jnN3beU-YxUeHd03JxrkaND8v21q5MfUqFNPyh4fwrgR2iaCjxMEdLgyR80X6a40P3QleT4NyKxgAAAA&shmds=v1_AUFQtOMGP2D7THMDItRxtk-9rmIT-oDke27LyviDukLohrISdw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=XcHXVU6gK8xY-7IGAAAAAA%3D%3D,11 hours ago
Sr.Data Engineer,Huron,"Lingarajapura, Karnataka, India","Huron is redefining what a global consulting organization can be. Advancing new ideas every day to build even stronger clients, individuals and communities. We’re helping our clients find new ways to drive growth, enhance business performance and sustain leadership in the markets they serve. And, we’re developing strategies and implementing solutions that enable the transformative change they need to own their future.
As a member of the Huron corporate team, you’ll help to evolve our business model to stay ahead of market forces, industry trends and client needs. Our accounting, finance, human resources, IT, legal, marketing and facilities management professionals work collaboratively to support Huron’s collective strategies and enable real transformation to produce sustainable business results.
Join our team and create your future.

In the Senior Data Engineer role, you will be working with the data engineers, data Architect, team leads and practice leadership to contribute to the operational effectiveness of the Software Engineering function, team, and strategy. This role is based in Bengaluru, India, and will work with Software Development and Corporate IT, and practice resources based in the US and India.
Under limited direction, this individual is responsible for designing and developing solutions that improve how Huron does business. This individual participates in the full software development life cycle including requirements development, analysis, design, implementation, and support. In this role, you will also create technical specifications based on conceptual design and business requirements and will consult with project and business teams to prototype, refine, test, and debug solutions. You will use current programming languages and technologies to build solutions and integrations between systems, develop documentation and procedures for installation, support, and maintenance. All solutions will be built based on Huron’s enterprise platforms and technologies.

Role Overview:

We are seeking a highly skilled and experienced Senior Data Engineer with expertise in Azure cloud services, PowerShell scripting, and Power Automate. The ideal candidate will be responsible for designing, implementing, and managing data pipelines and automation workflows while ensuring scalability, performance, and reliability.

Key Responsibilities:
• Design, develop, and maintain scalable data pipelines and ETL processes in Azure.
• Leverage Azure services like Data Factory, Log Analytics workspace, Storage table, Blob storage Databricks, and Azure SQL Database for end-to-end data engineering solutions.
• Automate routine tasks and workflows using PowerShell and Power Automate to enhance operational efficiency.
• Develop robust data solutions, including data integration, transformation, and optimization, to support analytics and business intelligence.
• Collaborate with cross-functional teams to understand business requirements and deliver tailored data solutions.
• Monitor, troubleshoot, and optimize data pipelines and automated workflows for performance and reliability.
• Ensure data quality, security, and compliance with organizational and regulatory standards.
• Create and maintain technical documentation for data engineering processes and solutions.
• Stay updated on the latest advancements in Azure services, PowerShell, and automation tools to recommend and implement best practices.
• Proficiency in PowerShell scripting for automation, task scheduling, and system management.
• Hands-on experience with Power Automate for designing and managing workflow automations.
• Hands-on experience in Power Apps UI/UX design, including creating intuitive, user-friendly interfaces and optimizing user experiences for custom applications.
• Works on other projects and tasks as deemed necessary.
• Works in collaboration with the offshore development team.
• Works closely with the QA team in the testing ETL/Product and remediation of issues.

Required Skills and Qualifications:
• Bachelor’s degree in computer science (or related field) with 4+ years relevant industry experience as a Data engineer with a strong focus on Azure cloud services.
• Proficiency in programming languages such as Python, SQL, and shell scripting. Must be able to author highly efficient, reusable, and robust code.
• Expertise in PowerShell scripting for task automation and system management.
• Proficient in SQL, encompassing basic to advanced concepts, including creating stored procedures and SQL functions.
• Expertise in Azure services, including Azure Data Factory, Log Analytics Workspace, Databricks, Azure SQL Database, Azure Functions, and Logic Apps.
• Proficiency in PowerShell scripting for automation, task scheduling, and system management.
• Hands-on experience with Power Automate for designing and managing workflow automations.
• Hands-on experience in Power Apps UI/UX design, creating intuitive and user-friendly interfaces.
• Hands on experience with relational OLTP database systems (MS SQL Server, PostgreSQL)
• Applied knowledge of data modelling concepts (dimensional modelling, normalization), and working with large-scale data systems.
• Excellent problem-solving, communication, and team collaboration skills.

Certifications (Good to Have):
• Azure Certifications (DP-203)
• Relevant certifications in Azure, Power Apps, or Data Engineering

Posting CategoryCorporate

Opportunity TypeRegular

CountryIndia",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=aweaJsqP_NMxMjJNAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNOw7CMAxAxdojMHkDIUgQEgtdQXw3DlC5xUpTih3ZqdRjcGTK8vSm94rvrFg81R0xI5w4RCZS2MBNajBCbVoQhrNI6GletjknO3hv1rtgGXNsXCMfL0y1jL6T2v6orEWl1GOmarffji5xWC0vg06pyPCIHFCxwzQoruGOytP9PemVXxF_FzVme5MAAAA&shmds=v1_AUFQtOPbcNydiTKpaT-xz0ndb4YsfPiHwpaSd7WqK3JSSjV4OQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=aweaJsqP_NMxMjJNAAAAAA%3D%3D,16 days ago
data-engineer-21068,IBM,"Mumbai, Maharashtra, India",No job summary available.,https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=AwtG_oM4BhMHFgttAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7CU7nWpqmFhTRzUUq9BvKpT2SSHtXchH6EX60urzpFZ9dUU6Y0RD7yETJtMfmfAEDT3GghGkMIAwPET_T_hZyXvVqrepce82Y41iPslhhcrLZlzj9M2jAROuMmYb21Gz1yr48dPceIkP_XhzGCnr8JdSQE1bQ8RTxC96mqkSPAAAA&shmds=v1_AUFQtON1VRTonnxNzVjmGdjbFLpUgJC3bwYYXGl0Q_-9qWSycg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=AwtG_oM4BhMHFgttAAAAAA%3D%3D,8 days ago
Senior Data API Engineer- Data Platform,Charles River,Anywhere,"At Charles River, we are passionate about improving the quality of people’s lives. When you join our global family, you will help create healthier lives for millions of patients and their families.

Charles River employees are innovative thinkers, who are dedicated to continuous learning and improvement. We will empower you with the resources you need to grow and develop in your career.

As a Charles River employee, you will be part of an industry-leading, customer-focused company at the forefront of drug development. Your skills will play a key role in bringing life-saving therapies to market faster through simpler, quicker, and more digitalized processes. Whether you are in lab operations, finance, IT, sales, or another area, when you work at Charles River, you will be the difference every day for patients across the globe.

Job Summary

There has never been a more exciting time to be part of the Enterprise Data Analytics team at Charles River Labs. We are on a mission to position data as the core driver of our business, empowering leaders to make informed, data-driven decisions that accelerate revenue, enhance productivity, and keep us ahead of the competition. Our recently launched Enterprise Data Hub serves as the company's digital backbone, and we are looking for visionary people in data analytics to help us further expand and refine this hub. Your role will be key in integrating, mastering, and ensuring the quality of our data across all business functions, ultimately transforming how Charles River operates through data science and advanced analytics.

You will be joining a team that is deeply committed to our purpose: Together We Create Healthier Lives. This unwavering focus on patients makes our global technology team uniquely inspiring. As we look to the future, we reimagine how we do business through our Digital Journey. This journey is central to advancing our position in the market, unlocking new growth opportunities, and positioning us as a leading, digitally powered Contract Research Organization (CRO) that enables our clients to deliver innovative, safe, and effective treatments to patients faster and more efficiently than ever before.

Note: It’s a fully remote home-based role for professionally qualified and experienced candidates based in India, who are willing and open to work UK shifts.

Key Responsibilities:

• Design, develop, and maintain RESTful and GraphQL APIs to support scalable, high-performance applications.
• Collaborate with data engineers and developers to create seamless integrations between APIs, data pipelines, and web applications.
• Build and maintain applications using frameworks like Java Spring Boot, Hibernate, Python Fast API, and ReactJS.
• Ensure system scalability, security, and reliability by implementing best practices in multi-threading, concurrency, and microservices architecture.
• Drive architectural decisions to optimize performance across application, data, and infrastructure domains.
• Develop and deploy solutions on Azure, utilizing tools like SQL Server, Redis, App Services, and API Management.
• Create and maintain test automation frameworks to ensure data quality and service reliability.
• Mentor junior engineers and foster a collaborative team environment.

Essential Qualifications:

• Bachelor’s degree in computer engineering, Computer Science or related discipline, Master’s Degree preferred
• 7+ years of software development experience in Big Data
• 7+ years of experience in modern programming languages - Java, Java-based frameworks (Spring, Maven), Python
• 3+ Years of experience in application, data, and infrastructure architecture disciplines
• 3+ years of experience with multi-threading, concurrency, and highly scalable Microservices and REST web Services
• Ability to work collaboratively in teams and develop meaningful relationships to achieve common goals
• Extensive experience with message driven software patterns such as Service Bus, Kafka, Tibco, EMS, and MQ

About Corporate Functions
The Corporate Functions provide operational support across Charles River in areas such as Human Resources, Finance, IT, Legal, Sales, Quality Assurance, Marketing, and Corporate Development. They partner with their colleagues across the company to develop and drive strategies and to set global standards. The functions are essential to providing a bridge between strategic vision and operational readiness, to ensure ongoing functional innovation and capability improvement.

About Charles River
Charles River is an early-stage contract research organization (CRO). We have built upon our foundation of laboratory animal medicine and science to develop a diverse portfolio of discovery and safety assessment services, both Good Laboratory Practice (GLP) and non-GLP, to support clients from target identification through preclinical development. Charles River also provides a suite of products and services to support our clients’ clinical laboratory testing needs and manufacturing activities. Utilizing this broad portfolio of products and services enables our clients to create a more flexible drug development model, which reduces their costs, enhances their productivity and effectiveness to increase speed to market.

With over 20,000 employees within 110 facilities in 20 countries around the globe, we are strategically positioned to coordinate worldwide resources and apply multidisciplinary perspectives in resolving our client’s unique challenges. Our client base includes global pharmaceutical companies, biotechnology companies, government agencies and hospitals and academic institutions around the world.

At Charles River, we are passionate about our role in improving the quality of people’s lives. Our mission, our excellent science and our strong sense of purpose guide us in all that we do, and we approach each day with the knowledge that our work helps to improve the health and well-being of many across the globe. We have proudly worked on 80% of the drugs approved by the U.S. Food and Drug Administration (FDA) in the past five years.

Equal Employment Opportunity

Charles River Laboratories is an Equal Opportunity Employer - M/F/Disabled/Vet.

If you are interested in applying to Charles River Laboratories and need special assistance or an accommodation due to a disability to complete any forms or to otherwise participate in the resume submission process, please contact a member of our Human Resources team by sending an e-mail message to crrecruitment_US@crl.com. This contact is for accommodation requests for individuals with disabilities only and cannot be used to inquire about the status of applications.

For more information, please visit www.criver.com.
226596",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=AWArqefb_5VzlK0ZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXMwQqCQBCAYbr6CJ3mLOpG0KUgkAqxk9QDyKxO68q6Iztb9Eq9ZUaX__Ad_uSzSo538pYDnDEilE0NF2-sJwr5nxqH8cFhghyurEEIQzcAe6iYjaP1YYhxlr1SIq4wEjHaruh4UuxJ81uNrOWXVgYMNC83are7zbuYvUnT04KOBG72RQGsh3KYqEeNfQbVc8SAMYPa9xa_3PEvZawAAAA&shmds=v1_AUFQtOO3sSRyR7RKdithOWcdE6XiORICxHWBviXj4B7cImXTnA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=AWArqefb_5VzlK0ZAAAAAA%3D%3D,9 days ago
Senior Data Engineer,HealthEdge,"Varthur, Bengaluru, Karnataka, India","Overview
Sr. Data Engineer
What will you do:

Build data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements
Develop ETL solutions using Python, Powershell, SQL, SSIS etc. to load and automate complex datasets in PDF, Excel, flat files, JSON, XML, EDI etc.
Take the full ownership of end-to-end data processes on Azure Cloud environments
Work closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap
Collaborate with the backend developers independently to understand the legacy applications and implement the features in a new system.
Troubleshoot issues and other operational bottlenecks to support continuous data delivery for various applications.
Take initiatives to make changes and improvements, work on technical debt, new and complex challenges.
Implement complex warehouse views, make database design decisions to support the UI need, optimize scripts to periodically refresh large volume datasets.
Perform code reviews and coach team members
Develop reports on data dictionary, server metadata, data files and implement the reporting tools as needed.
Implement best practices for data updates and development, troubleshoot performance and other data related issues on multiple product applications.
Keep current on big data and data visualization technology trends, evaluate, work on proof-of concept and make recommendations on cloud technologies.

What you bring:

7+ years of data engineering experience working in partnership with large data sets and cloud architecture
Deep experience in building data pipelines using the ETL tools and paradigms and loading data to and from RDBMS such Postgres, SQL Server, Oracle or similar.
Proficient in cloud services technologies such as Microsoft Fabric, Azure Data Factory, Data Lake, and other related technologies

Proficient in using SSDT tools for building SQL server relational databases, databases in Azure SQL, Analysis Services data models, Integration Services packages and Reporting Services reports
Solid experience building data solutions with the programming languages such as Python, Powershell
Advanced T-SQL and ETL automation experience
Experience working with orchestration tools such as Airflow and building complex dependency workflows
Self-motivated with the ability to work and learn new technology independently
Great problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.
Excellent communication and presentation skills

Bonus points:

Hands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud
Certification in one of the cloud platforms (AWS/GCP/Azure)
Experience with real-time data streaming tools like Kafka, Kinesis or any similar tools.
Experience with the US health care reimbursement-related terminology and data is a plus",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=zxmZixzP--Q4X2iaAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCQAwAYFz7CE6Ztfak4KKbWPwbBdeS1nB3eiYll0KfxmdVl2_-is-sWN6Ioygc0BAa9pGJFFZwkQ4yofYBhOEo4hPNd8FsyFvnck6Vz4YW-6qXtxOmTib3lC7_aXNApSGhUVtv1lM1sF_UJ8JkoXl4gshwR7Uwagl7Yo9p1LGEKyr_Hi8s4cyPiF_3o0T1oQAAAA&shmds=v1_AUFQtOMlgCOhLoHTmoiHjz1oFVPMuwjhURj8G8nQFah45Y6snQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=zxmZixzP--Q4X2iaAAAAAA%3D%3D,28 days ago
Senior Snowflake Data Engineer,Bright Vision Technologies,"Pune, Maharashtra, India","Bright Vision Technologies has an immediate Full-time opportunity for Senior Snowflake Data Engineer to join our team in United States.

Job Location: USA

Bright Vision Technologies

Your Path to a Successful Career in the U.S. Starts Here!

💼 Now Accepting Applications for the H1B 2025 Lottery!

At Bright Vision Technologies, we specialize in turning aspirations into reality. Whether you're an IT professional looking for growth or a recent graduate planning your next step, we’ve got you covered!

Why Partner with Us?

🔹 Proven Success: A trusted partner in successful H1B filings for over a decade.

🔹 End-to-End Support: From documentation to final approval, we handle it all.

🔹 Top Clients: Access premium opportunities with Fortune 500 companies.

🔹 Transparent Process: Regular updates and personalized attention at every stage.

🔹 Green Card Sponsorship: Pathways to long-term residency for eligible candidates.

Must have 10+ years of experience

Job Summary:

We are seeking an experienced Snowflake Data Engineer with expertise in DBT (Data Build Tool) and Python to join our team. As a Snowflake Data Engineer, you will be responsible for designing, building, and maintaining large-scale data warehouses using Snowflake, a cloud-based data warehousing platform.

Required Skills:
• Bachelor’s degree in computer science, Data Science, or a related field
• 10+ years of experience in data engineering, data warehousing, or related fields
• Strong understanding of Snowflake architecture, data modeling, and data warehousing concepts
• Proficiency in SQL, including Snowflake's SQL dialect
• Experience with DBT, including data transformation, testing, and deployment
• Understanding and Experience of snowpark for ETL transformations
• Strong programming skills in Python, including data processing, transformation, and loading tasks
• Experience with data visualization tools, such as Tableau, Power BI, or D3.js
• Strong analytical and problem-solving skills, with the ability to troubleshoot complex data issues
• Excellent communication and collaboration skills, with the ability to work with cross-functional teams
• Experience with Agile development methodologies and version control systems, such as Git

Important Notes: As part of the H-1B visa sponsorship process, there are various fees and expenses associated with filing and processing the application. While we are committed to supporting you through this process, we kindly request that candidates cover some of the costs like premium processing (if applicable), and dependent visa fees. This ensures a smooth and efficient process while aligning with our policies.

However, as a gesture of goodwill, we will reimburse 50% of the fee after 4 years of continuous employment with us.

If you are not comfortable with this arrangement, we kindly ask that you refrain from applying, as this is a necessary condition for sponsorship.

Would you like to know more about this opportunity? For immediate consideration, please send your resume directly to venkat.r@bvteck.com

At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.

Powered by JazzHR

lyil3DcmxD",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Nn1Lj3-XTIzUlLftAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsQrCQAyAYVz7CE6ZRVsR6qCbKKIgCBXXktZ4Fz2TcjmxD-VDWpd_-_7sO8rKioQ1QiX6uQd8EmwxIezEsRBFmMFRGzDC2HpQgb2qCzRe-5Q6WxWFWcidJUzc5q2-ChVqtC8e2tg_tXmM1AVMVC_KeZ934ibLTWTnE1zZeFheqPWiQR2TAQuc30JTOOEg0XyKOIWD3Bh_lFQiPa8AAAA&shmds=v1_AUFQtOPrY6taldEplXdMrog-tUWH3sJhl7oVqXc10Dj-h8qamg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Nn1Lj3-XTIzUlLftAAAAAA%3D%3D,6 days ago
Marketing Data & Analytics Engineer Manager & Product Owner,Kimberly-Clark,"Bengaluru, Karnataka, India (+2 others)","About Us

Huggies®. Kleenex®. Cottonelle®. Scott®. Kotex®. Poise®. Depend®. Kimberly-Clark Professional®. You already know our legendary brands—and so does the rest of the world. In fact, millions of people use Kimberly-Clark products every day. We know these amazing Kimberly-Clark products wouldn’t exist without talented professionals, like you.

At Kimberly-Clark, you’ll be part of the best team committed to driving innovation, growth and impact. We’re founded on 150 years of market leadership, and we’re always looking for new and better ways to perform – so there’s your open door of opportunity. It’s all here for you at Kimberly-Clark; you just need to log on!

Led by Purpose. Driven by You.

About the role

You were made to do this work: designing new technologies, diving into data, optimizing digital experiences, and constantly developing better, faster ways to get results. You want to be part of a performance culture dedicated to building technology for a purpose that matters. You want to work in an environment that promotes sustainability, inclusion, wellbeing, and career development. In this role, you’ll help us deliver better care for billions of people around the world. It starts with YOU.
• Leverage Snowflake, a premier cloud-based data warehousing solution, to revolutionize the digital customer experience. Our commitment to integrating Snowflake with advanced technologies such as cloud services, AI/ML, and data analytics is pivotal to our strategy. This approach allows us to extract meaningful, actionable insights from extensive customer data, enabling personalized interactions and groundbreaking solutions.
• Be at the forefront of our data and analytics strategy. The chosen candidate will lead the direction, design, and execution of initiatives focused on harnessing the power of Snowflake and other data platforms. Your work will improve transparency, enhance revenue management practices, and boost profitability across our brands and product portfolios.
• Lead people and be deeply invested in fostering robust relationships with stakeholders and empowering our team.
• Collaborate and lead teams with a hands-on, empathetic approach, ensuring alignment with our strategic objectives. With a passion for data-driven solutions, this leader will oversee the integration of Snowflake and other advanced technologies, enhancing our capabilities to connect with and serve our clients effectively.
• By focusing on people-centric leadership, this role is pivotal in growing our Kimberly-Clark interactions, making our technology platforms more intuitive, responsive, and aligned with the needs of our customers and teams.
• Kimberly-Clark has an amazing opportunity to continue leading the market, and DTS is poised to deliver compelling and robust digital capabilities, products, and solutions to support it. This role will have substantial influence in this endeavor.
• If you are excited to make a difference applying cutting-edge technologies to solve real business challenges and add value to a global, market-leading organization, please come join us!

Key Responsibilities:
• Lead Data Strategy Development: Develop and implement a comprehensive data strategy that aligns with marketing goals and drives business growth.
• Data Governance: Ensure data quality, accuracy, and compliance with relevant regulations and best practices.
• Technology Adoption: Stay updated with the latest advancements in data analytics, AI/ML, and cloud technologies, and implement them to enhance marketing capabilities.
• Lead Data and Analytics Initiatives: Spearhead initiatives within the Snowflake environment and other advanced data platforms, collaborating closely with Product Managers and stakeholders. You will focus on defining and prioritizing the product backlog in alignment with our strategic mission to revolutionize the consumer experience through data-driven insights.
• Strategic Product Backlog Management: Manage the Data and Analytics product backlog with precision, ensuring that each item aligns with our long-term vision and immediate business needs. Balance new developments, operational tasks, and bug resolutions to deliver continuous value, driving our mission of enhancing transparency and profitability across brands.
• Release Planning and Execution: Develop and maintain release plans for data and analytics enhancements, incorporating financial costs and Total Cost of Ownership (TCO). Work with Product Managers and delivery teams to set realistic release goals, aligning them with our strategic mission to deliver innovative, market-leading solutions.
• Collaborative Product Development: Engage continuously with the Scrum team and subject matter experts to refine backlog items, ensuring all developments align with our strategic mission and stakeholder expectations. Provide necessary support during sprint sessions to maintain focus on delivering strategic objectives.
• Sprint Management and Execution: Define clear priorities and sprint goals, focused on crucial data and analytics projects that advance our mission. Offer ongoing support to overcome obstacles and ensure smooth workflow, maintaining alignment with our overarching business goals.
• Quality Assurance and Stakeholder Engagement: Evaluate and maintain the integrity of all deliverables within the Snowflake environment, ensuring they meet our standards and mission. Actively participate in agile events, championing stakeholder needs and facilitating effective communication across all levels of the organization.
• Cross-functional collaboration: Coordinate resources across various departments for meticulous planning and execution of product backlog items. Engage in successful sprint reviews, user acceptance testing, and deployment of new features, ensuring each step aligns with our mission to provide exceptional customer-centric solutions.
• Cultural and Team Development: Promote a culture of continuous learning and improvement within the Data and Analytics team. Advocate for professional development and knowledge exchange, building a high-performing team that is dedicated to our mission of delivering innovative solutions.

Technical Expertise:
• Snowflake Ecosystem Proficiency: Over 5 years of hands-on of hands-on experience in tailoring, configuring, and crafting solutions within the Snowflake environment. This includes a profound grasp of Snowflake's data warehousing capabilities, data architecture, SQL optimization for Snowflake, and leveraging Snowflake's unique features such as Snowpipe, Streams, and Tasks for real-time data processing and analytics. My background also encompasses a strong foundation in data migration strategies, performance tuning, and securing data within the Snowflake ecosystem.
• Snowflake Architecture Design: Demonstrated expertise in architecting solutions within the Snowflake ecosystem, adhering to best practices in data architecture and design patterns. This includes leveraging Snowflake's advanced features such as secure data sharing, stored procedures, and integration with external functions. Skilled in optimizing data workflows and enhancing user experiences by utilizing Snowflake's scalable compute and storage capabilities.
• Snowflake Security Proficiency: Comprehensive knowledge of Snowflake's security model, including role-based access control, data encryption at rest and in transit, and implementing Snowflake's security best practices. Familiarity with configuring Snowflake to meet industry security standards and compliance requirements, ensuring the protection of sensitive data within the Snowflake environment.
• Scalability & High Availability: Proven expertise in architecting and implementing Snowflake solutions that are highly scalable and resilient, ensuring business continuity and optimal performance even under high load.
• Snowflake Cloud Services Implementation: Strong track record in leveraging Snowflake’s cloud-based architecture to develop scalable and highly available services, optimizing the use of Salesforce's multi-tenant architecture for efficient resource utilization.
• Agile Development: Profound familiarity with Agile methodologies in the context of Data and Analytics development projects, adept at managing the product backlog, user stories, sprints, and releases within the Snowflake ecosystem.
• Strategic Problem Solving & Analytical Thinking: Ability to tackle complex data-related challenges with strategic foresight and analytical precision, employing data-driven decision-making to navigate and optimize Snowflake implementations for business impact.

Leadership and Management:
• Team Leadership: 7+ years of experience leading large, remote, global IT teams, with 5+ years focused on development lifecycle leadership within a product team framework.
• Cross-functional Collaboration: Background in coordinating hardware & software development teams and business owners, with the ability to influence SMEs directly.
• Global Team Management: Experience leading multi-cultural and global teams in a matrix organization, adept at influencing stakeholders and working proactively with business and IT peers globally.

Specialized Skills:
• Prototyping: Ability to engage in rapid prototype development using iterative techniques.
• Vendor and Partner Engagement: Direct experience managing relationships with outsourcing and consulting partners.
• AI, Machine Learning and Data Management: Expertise in leveraging AI and ML for predictive analytics, automating processes, and enhancing decision-making, ensuring solutions are both innovative and data driven.
• Cloud Data Architecture Design: Deep understanding of cloud solutions and architectures, enabling scalable, flexible, and resilient digital infrastructures that support rapid innovation and development.
• AMC: Understanding the Amazon Marketing Cloud platform.

Soft Skills and Others:
• Agile Collaboration: Demonstrated experience in effectively working with Agile and product teams, including involvement with epics, user stories, sprints, and backlog management. Ability to collaborate closely with product owners, engineers, and scrum master’s to foster a productive and agile environment.
• Leadership in Communication and Reporting: Skilled in leading communication efforts and reporting, adept at preparing and presenting comprehensive updates and reports to senior leaders and stakeholders. Ability to succinctly highlight key metrics, progress, and insights, ensuring transparency and alignment at all levels of the organization.
• Expertise in Backlog Prioritization: Proficient in prioritizing the product backlog as a core responsibility of the Product Owner role, ensuring that items are assessed and ordered based on their value, complexity, and impact to the business. Ability to balance various factors such as stakeholder needs, team capacity, market trends, and strategic objectives to guide decision-making and optimize product delivery.

Knowledge and Experience:
• Bachelor’s degree in management information systems/technology, Computer Science, Engineering, or related discipline. MBA or equivalent is preferred.
• 10+ years of experience as part of large, remote, global IT teams.
• 7+ years focused on development lifecycle product architecture design.
• 3+ years demonstrable experience in Agile and Product team workflows, including epics, features, stories, sprints, and backlog management.​
• Financial Fluency: P&L understanding, business case development, OKRs, KPIs.​
• 5+ years of experience tailoring, configuring, and crafting solutions within the Snowflake environment, including a profound grasp of Snowflake's data warehousing capabilities, data architecture, SQL optimization for Snowflake, and leveraging Snowflake's unique features such as Snowpipe, Streams, and Tasks for real-time data processing and analytics. A strong foundation in data migration strategies, performance tuning, and securing data within the Snowflake ecosystem is essential.
• 5+ years demonstrated expertise in architecting solutions within the Snowflake ecosystem, adhering to best practices in data architecture and design patterns.
• 10+ years of experience in designing large-scale data solutions, performing design assessments, crafting design options and analysis, finalizing preferred solution choice working with IT and Business stakeholders.
• 10+ years of data engineering or design experience, designing, developing, and deploying scalable enterprise data analytics solutions from source system through ingestion and reporting.
• Expertise in data modeling principles/methods including, Conceptual, Logical & Physical Data Models for data warehouses, data lakes and/or database management systems.
• Expertise in Amazon Marketing Cloud (AMC) integration and data analysis for marketing insights and campaign performance tracking.
• Expertise in programming languages like Python/JavaScript.
• Expertise in Tableau/PowerBI exposure.
• 5+ years of hands-on experience designing, building, and operationalizing data solutions and applications using cloud data and analytics services in combination with 3rd parties.
• 10+ years of hands-on relational, dimensional, and/or analytic experience (using RDBMS, dimensional, NoSQL data platform technologies, and ETL and data ingestion protocols).
• 10+ years of experience with database development and scripting.

Professional Skills:
• Strong multi-cultural leadership and management skills.
• Strong communication and interpersonal skills.
• Strong analytical and problem-solving skills and passion for product development.
• Strong understanding of Agile methodologies and open to working in agile environments with multiple stakeholders.
• Professional attitude and service orientation; team player.
• Ability to translate business needs into potential analytics solutions.
• Strong work ethic: ability to work at an abstract level and gain consensus.
• Ability to build a sense of trust and rapport to create a comfortable and effective workplace.
• Self-starter who can see the big picture, prioritize work to make the largest impact on the business and customer's vision and requirements.

Scope/Categories:
• Role will report to the Data & Analytics Senior Manager. This position has direct reports, and oversight on external resources.
• Travel may include approximately 15% of work time.

Key Interfaces
• Internal: Data & Analytics Product Managers, Data & Analytics Product Teams, Data Science Team, Data & Analytics COE, Cloud COE Team, Enterprise Architecture.
• External: Contractors, Consulting Partners, 3rd Party service providers.

Total Benefits

Here are just a few of the benefits you’d enjoy working in this role for Kimberly-Clark. For a complete overview, see www.mykcbenefits.com.

Great support for good health with medical, dental, and vision coverage options with no waiting periods or pre-existing condition restrictions. Access to an on-site fitness center, occupational health nurse, and allowances for high-quality safety equipment.

Flexible Savings and spending accounts to maximize health care options and stretch dollars when caring for yourself or dependents.

Diverse income protection insurance options to protect yourself and your family in case of illness, injury, or other unexpected events.

Additional programs and support to continue your education, adopt a child, relocate, or even find temporary childcare.

To Be Considered

Click the Apply button and complete the online application process. A member of our recruiting team will review your application and follow up if you seem like a great fit for this role.

In the meantime, please check out the careers website. You’ll want to review this and come prepared with relevant questions when you pass GO and begin interviews.

And finally, the fine print….

For Kimberly-Clark to grow and prosper, we must be an inclusive organization that applies the diverse experiences and passions of its team members to brands that make life better for people all around the world. We actively seek to build a workforce that reflects the experiences of our consumers. When you bring your original thinking to Kimberly-Clark, you fuel the continued success of our enterprise. We are a committed equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation, gender, identity, age, pregnancy, genetic information, citizenship status, or any other characteristic protected by law. The statements above are intended to describe the general nature and level of work performed by employees assigned to this classification. Statements are not intended to be construed as an exhaustive list of all duties, responsibilities and skills required for this position.

Additional information about the compensation and benefits for this role are available upon request. You may contact kcchrprod@service-now.com for assistance. You must include the six-digit Job # with your request.

This role is available for local candidates already authorized to work in the role’s country only. Kimberly-Clark will not provide relocation support for this role.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=iiIYJijcxRpKooiVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOTQrCMBBGcesRXM3KhdhGBDe68g_RUhQvIJN2SGPTmZKkqLfzaMbN-3ibjzf-jsb3En1L0bKBA0aEKWwZ3SfaKsCRjWUiDyUymrRTuHmphyrC9cXJM7iIhkDoqwaE4SRiHE02TYx9WCsVgstNiJje8ko6JUxa3uopOvzxCA166h1GeixXi3fes5llhe00effJ9i6lgWXYERt0gx_mUKDnlNniHM5cW_wBVL-75cMAAAA&shmds=v1_AUFQtOO49pEzzp7CrT-AenisYsdgkOYX7U5MPQetylJZ4dxrRA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=iiIYJijcxRpKooiVAAAAAA%3D%3D,N/A
"Big Data Engineer, CorpFPA",Amazon,"Bengaluru, Karnataka, India","Senior Big Data Engineer role at Amazon's FP&A team, building enterprise-scale financial analytics systems using AWS technologies",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Ojf7Z2pwAs4RuBwVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOPQ7CMAxAYbH2CExeWFBpAIkFppY_AQs3qNxipYHUjuJUqrgI1wWWN3562WeSrSpn4YAJ4cjWMVHMYS8xnO4lLOAqDShhbDsQhrOI9TTddSkF3Rqj6gurCZNri1Z6I0yNjOYpjf5Ta4eRgsdE9XqzHIvAdj4re3z_LMdQEVv0QxxyuGHk38MLc7jww-EX_wqYxpoAAAA&shmds=v1_AUFQtONEOmzWTJZOtJeALA6i_uW9AbUjbLmjAd4B4envnqc9ig&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Ojf7Z2pwAs4RuBwVAAAAAA%3D%3D,N/A
Principal Data Engineer (MTS4),Nielsen,"Gurugram, Haryana, India","At Nielsen, we are passionate about our work to power a better media future for all people by providing powerful insights that drive client decisions and deliver extraordinary results. Our talented, global workforce is dedicated to capturing audience engagement with content - wherever and whenever it’s consumed. Together, we are proudly rooted in our deep legacy as we stand at the forefront of the media revolution. When you join Nielsen, you will join a dynamic team committed to excellence, perseverance, and the ambition to make an impact together. We champion you, because when you succeed, we do too. We enable your best to power our future.

As a Principal Data Engineer(MTS 4), you will drive the strategy, architecture, and execution of large-scale data solutions across our function. This role involves tackling highly ambiguous, complex challenges where the business problem may not be fully defined at the outset. You will partner closely with cross-functional teams (Engineering, Product, Operations) to shape and deliver our data roadmap. Your work will have a profound impact on the company’s data capabilities, influencing multiple teams’ technical and product direction.

You should bring deep expertise in designing and developing robust data pipelines and platforms, leveraging technologies such as Spark, Airflow, Kafka, and other emerging tools. You will set standards and best practices that raise the bar for engineering excellence across the organization.

Key Responsibilities
• Architect & Define Scope
• Own end-to-end design of critical data pipelines and platforms in an environment characterized by high ambiguity.
• Translate loosely defined business objectives into a clear technical plan, breaking down complex problems into achievable milestones.
• Technology Leadership & Influence
• Provide thought leadership in data engineering, driving the adoption of Spark, Airflow, Kafka, and other relevant technologies (e.g., Hadoop, Flink, Kubernetes, Snowflake, etc.).
• Lead design reviews and champion best practices for coding, system architecture, data quality, and reliability.
• Influence senior stakeholders (Engineers, EMs, Product Managers) on technology decisions and roadmap priorities.
• Execution & Delivery
• Spearhead strategic, multi-team projects that advance the organization’s data infrastructure and capabilities.
• Deconstruct complex architectures into simpler components that can be executed by various teams in parallel.
• Drive operational excellence, owning escalations and ensuring high availability, scalability, and cost-effectiveness of our data solutions.
• Mentor and develop engineering talent, fostering a culture of collaboration and continuous learning.
• Impact & Technical Complexity
• Shape how the organization operates by introducing innovative data solutions and strategic technical direction.
• Solve endemic, highly complex data engineering problems with robust, scalable, and cost-optimized solutions.
• Continuously balance short-term business needs with long-term architectural vision.
• Process Improvement & Best Practices
• Set and enforce engineering standards that elevate quality and productivity across multiple teams.
• Lead by example in code reviews, automation, CI/CD practices, and documentation.
• Champion a culture of continuous improvement, driving adoption of new tools and methodologies to keep our data ecosystem cutting-edge.

Qualifications
• Bachelor’s or Master’s degree in Computer Science, Engineering, or related field (or equivalent experience).
• 6+ years of software/data engineering experience, with significant exposure to large-scale distributed systems.
• Technical Expertise:
• Demonstrated proficiency with Spark, Airflow, Kafka, and at least one major programming language (e.g., Python, Scala, Java).
• Experience with data ecosystem technologies such as Hadoop, Flink, Snowflake, Kubernetes, etc.
• Proven track record architecting and delivering highly scalable data infrastructure solutions.
• Leadership & Communication:
• Ability to navigate and bring clarity in ambiguous situations.
• Strong cross-functional collaboration skills, influencing both technical and non-technical stakeholders.
• Experience coaching and mentoring senior engineers.
• Problem-Solving:
• History of tackling complex, ambiguous data challenges and delivering tangible results.
• Comfort making informed trade-offs between opportunity vs. architectural complexity""

Please be aware that job-seekers may be at risk of targeting by scammers seeking personal data or money. Nielsen recruiters will only contact you through official job boards, LinkedIn, or email with a nielsen.com domain. Be cautious of any outreach claiming to be from Nielsen via other messaging platforms or personal email addresses. Always verify that email communications come from an @nielsen.com address. If you're unsure about the authenticity of a job offer or communication, please contact Nielsen directly through our official website or verified social media channels.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=1uz9cIqZfNrqy-gjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMvQrCMBAAYFz7CE4HLiq1EbGLroo_oAi6l2s80kh6V3Ip1EfxbdXlG7_sM8rKW_RsfYcBdpgQ9uw8E0WYXh739QwWcJYalDDaBoThIOICjbdNSp1ujFENhdOEydvCSmuEqZbBvKTWP5U2GKkLmKhalcuh6NjNJ1dPQYnB_74-9i5im8MR4xsZczjx0-MXZp2yOJwAAAA&shmds=v1_AUFQtOOeq3qA58KPZTHN36ZUt3UHI4anOrgTkTiAuO9B_256UQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=1uz9cIqZfNrqy-gjAAAAAA%3D%3D,N/A
"Data Engineer, Surveillance Analytics","F337 Deutsche India Private Limited, Pune Branch",India,"Job Description: Job Title: Data Engineer, Surveillance Analytics Corporate Title: Associate Location: Pune, India Role Description Surveillance & Regulatory: We are responsible for delivering solutions that protect Deutsche Bank’s financial and reputational interests from criminal or inappropriate behaviour. Through real-time sanction and embargo filtering, operational risk controls and sophisticated fraud detection that will increasingly deploy artificial intelligence, our systems protect both clients and the bank, allow risks to be managed according to risk appetite, and keep Deutsche Bank in compliance with global and local regulations. You will be a key member of Surveillance analytics squad in Surveillance & Regulatory tribe. Surveillance data analytics offers data analytics as a service which combines Surveillance control events along with data from Payment systems and Surveillance providers to support informed decision making and detect potential gaps in surveillance controls. What we’ll offer you As part of our flexible scheme, here are just some of the benefits that you’ll enjoy Best in class leave policy Gender neutral parental leaves 100% reimbursement under childcare assistance benefit (gender neutral) Sponsorship for Industry relevant certifications and education Employee Assistance Program for you and your family members Comprehensive Hospitalization Insurance for you and your dependents Accident and Term life Insurance Complementary Health screening for 35 yrs. and above Your key responsibilities Surveillance Analytics team within Corporate Bank (CB) Technology, which provides a holistic view of clients across all businesses and systems globally to a range of key stakeholders in the front office, middle office and operations. This team provides a key supporting role in ensuring that Deutsche Bank complies with regulatory commitments. Working with Subject Matter Experts (SMEs) globally across functions to respond to data inquiries as quickly as possible with a view toward addressing them in the strategic roadmap. Keeping track of all data issues to initiate end to end Root Cause Analysis (RCA) Developing Key Performance Indicators (KPIs) and Key Risk Indicators (KRIs) and other regular Management Information (MI) that can be published to show progress. Become a business SME to support Tech teams by the development of application capabilities, best practices, and issue resolutions. Experience of working in an Agile development methodology supporting remote as well as local delivery teams Manage and elaborate the backlog items and work with other squad members to enable cross-skilling. Your skills and experience Experience in data analysis and techniques with good knowledge in SQL (and Tableau) Developed Data models and data integration of BI tools with various types Data sources. Experience in Designing BI dashboards including UX/UI Design, technical experience in using BI tools, configuration / tuning usage or optimizing performance of dashboards. Experience in developing interactive Tableau/dbLooker or any BI Dashboards and Data Models to meet business needs. Working with RDBMS/ Cloud based databases/warehouses Knowledge of Access governance framework/ user management. Experience in a payment domain and/or related support functions including understanding of products, business, and operational processes, with exposure to business analysis will be preferred. Experience in understanding of data model, data specifications and data formats (e.g., JSON) Understanding of Cash Management Products and Payment Standards, e.g., SWIFT MT, ISO20022 An ability to understand and translate control requirements into business and data requirements whilst adhering to the agreed specification quickly and effectively. Experience of working in an Agile development methodology supporting remote as well as local delivery teams How we’ll support you Training and development to help you excel in your career Coaching and support from experts in your team A culture of continuous learning to aid progression. A range of flexible benefits that you can tailor to suit your needs. About us and our teams Please visit our company website for further information: https://www.db.com/company/company.htm We strive for a culture in which we are empowered to excel together every day. This includes acting responsibly, thinking commercially, taking initiative and working collaboratively. Together we share and celebrate the successes of our people. Together we are Deutsche Bank Group. We welcome applications from all people and promote a positive, fair and inclusive work environment. For over 150 years, our dedication to being the Global Hausbank for our clients has been driven by our people – in around 60 countries and across more than 150 nationalities. Their deep understanding, insights, expertise, and passion help our clients navigate an increasingly complex world – be it in our Corporate Bank, our Private Bank, our Investment Bank or our Asset Management (DWS) division. Together we can make a great impact for our clients at home and abroad, securing their lasting success and financial security. More information at: Deutsche Bank Careers (db.com)",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=3VTuYoHdALwWNexHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXOPQrCQBBAYWxzBKupJSZiEPGnUaKiWAgeIEw2Q3ZlMxt2NiFezPMZsXnVV7zoM4n2OQaEE9eGiXwMz873ZKxFVgQHRvsORgnM4eZKEEKvNDiGi3O1pelOh9DKNk1FbFJLwBEnyjWpYyrdkL5cKb8UotFTazFQsVwthqTlerY5Z9kacuqCKE1w5cogPLzpRwV305hAVQyPjgmOfvzRYPivvoyWnrO5AAAA&shmds=v1_AUFQtOMXnRmbZ2CKn5RNggzXVQcGvbk0tFWC0aDFbxXic0DGcw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=3VTuYoHdALwWNexHAAAAAA%3D%3D,12 days ago
"Data Engineer,Azure",Fractal Analytics,"Bengaluru, Karnataka, India","About Fractal

What makes Fractal a GREAT fit for you? When you join Fractal, youll be part of a fast-growing team that helps our clients leverage AI together with the power of behavioural sciences to make better decisions. Were a strategic analytics partner to most admired fortune 500 companies globally, we help them power every human decision in the enterprise by bringing analytics, AI and behavioural science to the decision.
Our people enjoy a collaborative work environment, exceptional training and career development as well as unlimited growth opportunities. We have a Glassdoor rating of 4 / 5 and achieve customer NPS of 9/ 10. If you like working with a curious, supportive, high-performing team, Fractal is the place for you.

Role Brief

If you are an extraordinary developer and who loves to push the boundaries to solve complex business problems using creative solutions, then we wish to talk with you. As an Analytics Technology Engineer, you will work on the Technology team that helps deliver our Data Engineering offerings at large scale to our Fortune clients worldwide. The role is responsible for innovating, building and maintaining technology services.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=9GP2UNI3QQQ8hVFFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOTQrCMBBAYdz2CK5mXWpSBDe6qviDeogyjUMajTMlk0L1Kl7Wunm7D17xXRTlATPCkX1golQ1nzERrOAqHShhcj0Iw1nER1ru-pwH3VqrGo3XjDk44-RlhamTyT6k039a7THREDFTu97UkxnYl_UpocsYoWGM71kqBIY9scc4prGCGyaeX55YwYXvAX8v0oyKngAAAA&shmds=v1_AUFQtOOjwT8crdR18wcTD0INhLqHzhIqynv2HB679J4YQwPZGA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=9GP2UNI3QQQ8hVFFAAAAAA%3D%3D,N/A
Sr Data Engineer,Servcrust,"Hyderabad, Telangana, India","Sr Data Engineer

Experience Level: 6+ years

About Us:
ServCrust is revolutionizing the Construction and Mining Industry with its one-of-a-kind eCommerce, ERP,
and Fintech platform. Our cutting-edge cloud-based SaaS technologies are driven by a strong data-first
approach and AI-powered workflows. We are seeking Senior Engineers with a passion for technology and
a proven track record in Agile product development. Join us on our journey to build India's leading
eCommerce platform.
Responsibilities:
• Lead the design and development of scalable, robust data solutions that meet cross-functional teams' data requirements.
• Architect, develop, and optimize ETL (Extract, Transform, Load) processes to seamlessly integrate data from multiple sources into our data warehouse.
• Implement advanced data modeling techniques and design efficient, optimized databases to support complex business needs.
• Ensure the highest levels of data quality and integrity through rigorous data validation and cleansing processes.
• Collaborate closely with data scientists and analysts to provide clean, structured data for advanced analytics and machine learning models.
• Oversee and manage big data technologies and frameworks, including Hadoop, Spark, and Apache Airflow, to handle large-scale data processing tasks.
• Optimize, troubleshoot, and enhance the performance, scalability, and reliability of data processing pipelines.
• Mentor junior engineers and lead initiatives to integrate industry best practices and emerging technologies in data engineering into our workflows.
• Contribute to the strategic planning of data infrastructure to support the company's growing data needs.

Requirements:
• Bachelor's degree in Computer Science, Engineering, or a related field.
• 6+ years of experience in data engineering or a related role.
• Expertise in programming languages such as Python, Java, or Scala.
• Advanced experience with SQL and database technologies (e.g., PostgreSQL, MySQL).
• Proven expertise in designing and optimizing ETL processes and tools.
• Deep understanding of data modeling, database design principles, and data architecture.
• Extensive hands-on experience with big data technologies and distributed computing environments.
• Strong problem-solving and analytical skills, with a proven ability to lead complex projects. 9. Demonstrated ability to optimize and scale data processing pipelines for performance and reliability.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=fPALZJDLkgmH19DiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEOw7CMAwAULH2CEzeQKg0CIkFVhCfteyVk1ppULCr2KByC46MeMOrvrNq0RY4oiGcOCYmKrCGm3hQwhIGEIazSMw0Pwxmo-6dU81NVENLoQnydMLkZXIP8fqv0wELjRmNuu1uMzUjx9WypfIO5aUGieHy6amgx76GO2XkiIw1XLlP-AN1GmtckwAAAA&shmds=v1_AUFQtONBrrcjlvGZoyKnSTEcG8w2GhN5Q5K_-nz-X3l7b_p8LA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=fPALZJDLkgmH19DiAAAAAA%3D%3D,N/A
Data Engineer: Data Platforms-Google,IBM,"Chennai, Tamil Nadu, India","Introduction
In this role, you'll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.

A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.
You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.
Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.

Your Role and Responsibilities
• Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.
• Must have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation
• Ability to analyse data for functional business requirements & front face customer

Required Technical and Professional Expertise
• 5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.
• Skilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer
• You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies
• Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work

Preferred Technical and Professional Expertise
• Intuitive individual with an ability to manage change and proven time management
• Proven interpersonal skills while contributing to team effort by accomplishing related results as needed
• Up-to-date technical knowledge by attending educational workshops, reviewing publications",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=oY3Dr_v52pLGDb26AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWMuwrCQBAAsc0nWK2tJDkRbJLOBxJBsbAPm2S9O7nshuwJ-Rn_1VczMFNM8pol5R4jwoGtZ6KxgJ9eA8a7jL1mRxEbCDI4SQNKOLYOhOGf56WLcdDCGNWQW40YfZu30hthamQyD2n0i1odjjR8rlSvN6spH9guF9X2DJ5h54gZfQo37H2AC3bPFCruPL4ByLb2OKAAAAA&shmds=v1_AUFQtOM6xjiRzVHFMOAINVdz83E_trYdHZSxCXRmMkIYUg59gA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=oY3Dr_v52pLGDb26AAAAAA%3D%3D,5 days ago
Sr. Data Engineer (Machine Learning),Visa,"Bengaluru, Karnataka, India","Company Description

Visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.

Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.

Job Description

Visa AI as a Service (AIaS) operationalizes the delivery of AI and decision intelligence to ensure their ongoing business values. Built with composable AI capabilities, privacy-enhancing computation, and cloud native platforms, AIaS powers and automates industrialization of data, models, and applications. Combined with strong governance, AIaS optimizes the performance, scalability, interpretability and reliability of AI models and services. If you want to be in the exciting payment and AI space, learn fast, and make big impacts, Visa AI as a Service is an ideal place for you!

This position is for a Sr. ML Engineer with solid background in development and production of AI and ML systems for batch and real time predictive use cases. The right candidate is one who will focus on creating and maintaining new capabilities for AI as a Service while maturing our platforms and processes. In this position, you are first a passionate and talented engineer who can work in a dynamic environment, has strong problem-solving, coding, testing and troubleshooting skills. You must be willing to go beyond the routine, challenge yourself and be prepared to do a bit of everything during the entire platform development lifecycle.

You will be an integral part of the AIaS engineering team, often investigating new requirements, designing, refactoring, implementing, and documenting continuously, in a continuous effort to provide better solutions to your consumers. You must be a self-organized individual should be flexible and willing to switch tasks based on business needs.

If this sounds exciting, we want to chat and tell you more about our work culture and environment and see if this will be a good fit for both of us.

This is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.

Qualifications

3 or more years of work experience with a Bachelor’s Degree or Advanced Degree (e.g. Masters, MBA, JD, MD)

• Strong Development experience in Java and Python on Linux environments

• Knowledge of standard Big data with Batch and Real Time ecosystems such as Hadoop, Spark, Kafka, Redis, Flink, Airflow and similar technologies

• Hands on experience with Kubernetes, Docker with distributed frameworks like Ray and Spark

• Hands on experience in building and maintaining Data and Model engineering pipelines, feature engineering pipelines and should be comfortable with core Machine Learning / Data Science concepts, including popular frameworks like Tensorflow, Keras, Pytorch etc.

• Hands on experience in engineering, testing, validating and productizing AL/ML models for high performance use cases

• Knowledge about DR / HA topologies

• Knowledge of using and maintaining DevOps tools (Jenkins, Ansible, Shell Scripts) and implementing automations for production

Additional Information

Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=DIKt7bxdpHaCOIM-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOuwoCMRBFsd1PsBqwUVkTEWzcThTxVQm2MhtDEo0zSyYL-zf-qrG5nFOdW31HVXNLCnaYEfbkAlmbYHpF4wvCxWKiQG4GCzhxC1LceGCCA7OLdtz4nDvZaC0SlZOMORhl-KOZbMuDfnEr_3mIx2S7iNk-VuvloDpy88k9CEIg2FpyGPvU13AuwfLljTUc6RnwB39dkA2iAAAA&shmds=v1_AUFQtOPYA1_ZhMNLQMBoEZgZ4y4CR3vhr6G6j0OeFJnDPG-ygw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=DIKt7bxdpHaCOIM-AAAAAA%3D%3D,11 days ago
"Data Engineer-SQL,Python,GCP,ETL-5-10Yrs-Gurgaon(Hybrid)",Crescendo Global,"Gurugram, Haryana, India","Exciting opportunity for a Data Engineer with 4–7 years' experience to lead analytic strategies, technical requirements, and data solutions for a major insurance client in our dynamic team.

Location-PAN INDIA(Remote)

Your Future Employer-This company is a leading operations management and analytics provider that empowers businesses to achieve strategic objectives through data-driven insights and innovative solutions. With a focus on sectors like insurance, healthcare, and financial services, it combines domain expertise with advanced analytics to optimize performance and drive growth. Committed to collaboration and continuous improvement, it helps clients navigate complex challenges and seize new opportunities in an ever-evolving market.

Responsibilities:
• Define analytic strategies to meet business requirements.
• Determine technical and data requirements for analytic solutions.
• Acquire data from various sources; implement data collection systems and maintain databases.
• Identify, develop, and implement reporting and analytic tools.
• Develop dashboards for performance visualization using reporting tools.
• Engage in technical problem-solving across multiple technologies; develop new solutions as needed.
• Work independently to prepare client-ready deliverables with minimal supervision.
• Collaborate with stakeholders as a subject matter expert in data engineering.

Requirements:
• Bachelor's or Master's degree in economics, mathematics, actuarial sciences, computer science/engineering, operations research, or related analytics fields.
• 4+ years of experience in analytics ETL and data engineering roles.
• Experience with Google Cloud Platform (GCP) services.
• Proficient in Python scripting and data wrangling.
• Experience with process flow tools like Airflow.
• Strong knowledge of SQL, BigQuery, and GCP; data modeling experience is a plus.
• Ability to understand data models and identify ETL optimization opportunities; exposure to ETL tools preferred.
• Strong grasp of advanced SQL functionalities (joins, nested queries, procedures).
• Superior analytical and problem-solving skills.
• Outstanding written and verbal communication skills.
• Able to work in a fast-paced, continuously evolving environment and tackle challenges head-on.

What's in It for You:
• Work in an exciting, fast-paced, and innovative environment.
• Collaborate with highly experienced, world-class analytics consultants.
• Learn various aspects of client businesses and industry practices.

Reach Us:

If you are interested in this role, you can reach out to us at mini.dang@crescendogroup.in

Disclaimer: Crescendo Global is specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.

Note: We receive numerous applications daily, making it challenging to respond to each candidate. If you do not hear back from us within a week, please assume your profile has not been shortlisted. Your patience is highly appreciated.

Profile Keywords:

Data Engineer, Analytics ETL, Data Engineering, Google Cloud Platform, Python Scripting, Airflow, SQL, BigQuery, Data Modeling, ETL Optimization, Property and Casualty Insurance, Actuarial Projects, Data Analysis, Problem Solving, Communication Skills, Client Interaction, Fast-Paced Environment.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=dhT1Qv-1a-9HzoSrAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMsW7CMBAAUHXlEzrdCJWd0Eos7QgogJAAtUsndE5OjpG5i-4cifxbP67t8sY3-3manTZYELYcExOp_7wc3XkqvbBr1me3_Tr6lX9dfqv5ZtSIwvPdFDR1C_BwkABGqG0PwtCIxEzPH30pg73XtVmuohUsqa1audfCFORR3yTYP1frUWnIWOj6tlo-qoHji1srWUvcCTRZAmZIf_GoY1S8O9ihTsjoYM9dwl8VuM4evwAAAA&shmds=v1_AUFQtOMH-WOX_3uRJIm0KEcy_SN1ZIDcTPRyfzOZA9Qq4V2AUQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=dhT1Qv-1a-9HzoSrAAAAAA%3D%3D,12 days ago
Software Engineer - Data Platform,Qube Research & Technologies,"Mumbai, Maharashtra, India","Qube Research & Technologies (QRT) is a global quantitative and systematic investment manager, operating in all liquid asset classes across the world. We are a technology and data driven group implementing a scientific approach to investing. Combining data, research, technology, and trading expertise has shaped QRT’s collaborative mindset which enables us to solve the most complex challenges. QRT’s culture of innovation continuously drives our ambition to deliver high quality returns for our investors.

Your future role at QRT

We are looking for an exceptional Software Engineer to be an integral member of the Data Engineering team. We work closely to solve challenging problems that have direct impact on our business and trading.

In this role, you will play a key part in designing, building, and expanding our data platform, with a focus on the development of our new data pipeline framework. You will have the opportunity to develop highly scalable and performant solutions, while partnering with various stakeholders within QRT.

Prior experience with financial data is beneficial but not necessary; we welcome talented developers without a background in financial services and are happy to provide training and guidance.

Your responsibilities will include:
• Designing and developing scalable and robust services using best practices
• Writing automated tests
• Maintaining, and scaling delivery/integration pipelines
• Developing reusable libraries for the Data Engineering team
• Collaborating with engineering teams and researchers to propose and develop new framework features

Your present skillset
• Deep experience and passion for programming, design, and architecture (Python preferred)
• Knowledge of testing as part of continuous delivery and integration pipelines
• Familiarity with SQL and relational databases
• Knowledge of data lakes and data lakehouses (e.g. Apache Iceberg, Delta Lake, Snowflake)
• Excellent communication skills, and ability to collaborate effectively in a small, close-knit team
• Familiarity with cloud technologies (AWS preferred)
• Fluency in English

QRT is an equal opportunity employer. We welcome diversity as essential to our success. QRT empowers employees to work openly and respectfully to achieve collective success. In addition to professional achievement, we are offering initiatives and programs to enable employees to achieve a healthy work-life balance.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=hdc3gcltgEZ47nu4AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_y3MywrCMBCFYdz2EVzNyoXURgRBdKuIQsHbvkzqNImkmZJJse_lC1rBzYF_cb7sM8k2d27SGyPBIRgXiCIsYI8J4eIxNRzbsc-sQQhjbYEDHJmNp-nOptTJVikRXxhJmFxd1NwqDqR5UC_W8ptK7Mh3o0bVar0cii6Y-fbaa4Ib_dUZPKi2gT0bRwIuQNm3Gl0OJY5vFJsi5nAKT4dfmv265rYAAAA&shmds=v1_AUFQtON9-gPHc1FeSUCRGU2XD28L7X3YWBagnPEOp9ZivfI6Zw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=hdc3gcltgEZ47nu4AAAAAA%3D%3D,14 days ago
Data Engineer - Technical Implementation,Mastercard,"Pune, Maharashtra, India","Are you a data enthusiast with a passion for turning complex information into actionable insights? Do you thrive in a fast-paced, dynamic environment where your technical skills are put to the test? If so, Mastercard is seeking a highly skilled Data Engineer - Technical Implementation to join our team. As a key member of our data engineering department, you will play a crucial role in implementing and maintaining our cutting-edge data solutions. We are looking for someone with a strong background in data engineering, who is detail-oriented, and has a track record of successfully delivering technical projects. If you are ready to take on new challenges and make a significant impact in the world of data, we want to hear from you!

Develop and implement data solutions to support business objectives and drive actionable insights.
Collaborate with cross-functional teams to understand data requirements and design efficient and scalable data pipelines.
Utilize technical expertise and knowledge of data engineering best practices to ensure high-quality and accurate data delivery.
Troubleshoot and resolve technical issues related to data processing, storage, and analysis.
Continuously monitor, optimize, and improve data pipelines for efficiency, reliability, and performance.
Develop and maintain documentation for data processes and procedures.
Stay up-to-date with emerging data technologies and industry trends to continuously improve data solutions.
Work closely with stakeholders to understand their needs and provide technical guidance and support for data-related projects.
Collaborate with data scientists and analysts to ensure data accuracy and support their data-driven initiatives.
Take ownership of technical implementation projects from end-to-end, ensuring timely delivery and adherence to project timelines.
Participate in code reviews and provide constructive feedback to improve code quality.
Mentor and train junior members of the team on data engineering best practices and processes.
Proactively identify opportunities for process improvement and automation to increase efficiency and scalability.
Communicate effectively with team members and stakeholders to provide updates on project progress and escalate any issues or concerns.
Adhere to data security and privacy policies and ensure compliance with regulatory requirements.

Mastercard is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=WvEo02X5Q-nQb_xEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWKsQrCMBBAce0nON3kILURwUUHF0UqCA7u5ZoeSSS9hNwJ_SK_03Z58Hiv-q2qyxUV4cYuMFGBHbzJeg4WI7RjjjQSK2pIPKdH6kEIi_Uw-z0lF2l99qpZTsaIxMbJMtvGptEkpj5N5pN6WdCJx0I5olJ3OO6nJrPbbp4oSsViGSAwvL5MNTxxPlG8Fqyh5SHgH6LXcUipAAAA&shmds=v1_AUFQtOPTJebNexWxidi5DYUpBv6huOSFsP9CHTpCo7cmJ8rAWw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=WvEo02X5Q-nQb_xEAAAAAA%3D%3D,N/A
"Data Engineer- Python, Spark, SQL",NEC Software Solutions (India),"Maharashtra, India","Company Description

Our philosophy is to understand our customers’ business first before we get to the technology.
This approach leads to clever software; streamlining old processes, saving money and delivering positive change.

Our technology has helped the NHS screen millions of babies for hearing loss, ensures hundreds of housing providers are managing their homes efficiently and helps officers in over a dozen different police forces to make better decisions at the frontline.

Based in the UK but working around the world, our 2,000 employees help improve the services that matter most.
We are now part of the NEC corporation, a leader in the integration of IT and network technologies that benefit businesses and people worldwide – this brings in new opportunities without limits for growth and innovation.

Job Description

Role: Data Engineer

Experience: 7-10years

Location: Mumbai Preferred, Open to PAN India

Job Summary:

Skills:
• Experience with programing in Python, Spark and SQL
• Prior experience in AWS services (Such as AWS Lambda, Glue, Step function, Cloud Formation, CDK)
• Knowledge of building bespoke ETL solutions
• Data modelling, and T-SQL for managing business data and reporting
• Capable of technical deep-dives into code and architecture
• Ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management
• Experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms.
• Effective communication skills

Responsibilities:
• Build data pipelines: Architecting, creating and maintaining data pipelines and ETL processes in AWS
• Support and Transition: Support and optimize our current desktop data tool set and Excel analysis pipeline to a transformative Cloud based highly scalable architecture.
• Work in an agile environment: within a collaborative agile cross-functional product team using Scrum and Kanban
• Collaborate across departments: Work in close relationship with data science teams and with business (economists/data) analysts in refining their data requirements for various initiatives and data consumption requirements
• Educate and train: Required to train colleagues such as data scientists, analysts, and stakeholders in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases
• Participate in ensuring compliance and governance during data use: To ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives.
• Work within, and encourages a Devops culture and Continuous Delivery process",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=tuAftDfKGUB4PdC-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2LzQ7BUBCFY9tHsJol0h8REmFJIwQhfYBmWlfvpWaaOyPqobyjsjnny_lygk8vmK9REVKqHBnjIzi91TKFkDXo712d9xDBjgsQg760wAQb5qo2_aVVbWSRJCJ1XImiujIu-ZEwmYLb5MaF_CIXi940NarJJ7NxGzdUjabHdAUZX_XVuQ7qpzomgcGWLg6H4AgO2P1QrHoM4T9_AaulGDiwAAAA&shmds=v1_AUFQtOMYlLo9bo7rxDHk8o_BFiY49k2rTkXMunRixivJGDUXPg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=tuAftDfKGUB4PdC-AAAAAA%3D%3D,4 days ago
Senior Data Engineer - Power BI,Tiger Analytics,"Hyderabad, Telangana, India (+4 others)","As a Senior Data Engineer, you will work to enhance our business intelligence system to help us make better decisions; Seamlessly switch between roles of an Individual Contributor, team member, and data science engineer as demanded by each project to create and manage BI and analytics solutions that turn data into knowledge. On a typical day, you might

● Engage with clients to understand their business context

● Translate business needs to technical specifications.

● Discuss the design of the BI solutions that can be built & deployed to solve the complex problem.

● Leverage Power BI to maintain & support data analytics platforms.

● Involve yourself in unit testing and troubleshooting.

● Develop and execute database queries and conduct analyses.

● Create visualizations and build reports that exceed the expectations of the clients.

● Work on enhancing the BI systems that exist.

● Develop or ensure that the tech documentation developed is updated.

● Involve in BI performance tuning & consumption patterns as you better understand BI SLAs.

● Ideate with your peers to design ground-breaking BI solutions.

● Interact and collaborate with multiple teams (Data Science, Consulting & Engineering) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.

External Skills And Expertise

What do we expect?

● 7+ years of experience in the BI space with high-level comfort in Power BI.

● Real-time experience working in Power BI that includes Model Creation, DAX (including Advanced), RLS Implementation, Performance Tuning, and Security.

● Comprehensive knowledge of SQL.

● Enthuse to collaborate with various stakeholders across the organization and take complete ownership of deliverables.

● Adept understanding of any of the cloud services is preferred (Azure).

You are important to us, let’s stay connected!

Every individual comes with a different set of skills and qualities so even if you don’t tick all the boxes for the role today, we urge you to apply as there might be a suitable/unique role for you tomorrow.

We are an equal opportunity employer. Our diverse and inclusive culture and values guide us to listen, trust, respect, and encourage people to grow the way they desire.

Note: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.

Skills extracted from Job Description

Analytical

Analytics

Azure

Business

Business Intelligence

CAN

Cloud

Compensation

Consulting

Culture

Data

Data Analytics

Data Science

Database

Deliverables",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=gw_P_InCA4X9Qd8oAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMMQrCQBBFsc0RrKaWuCuCFlopisZKMH2YTYbNyjoTdhY0B_Kexua_17xffGfF9kEcJMEJM8KZfWCiBEu4y3visZr0Jg6UMLU9CMNFxEea7_ucB91ZqxqN14w5tKaVlxUmJx_7FKf_abTHREPETM16s_qYgf3C1MFP7wfGOE6dQmC4jh0ldNiVUFNE9shYQsVdwB_K_pRvqAAAAA&shmds=v1_AUFQtON4qr3M-AoMy1NxeWRp7dnKPLsrlOJt821nBDbaJUPQHg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=gw_P_InCA4X9Qd8oAAAAAA%3D%3D,N/A
"Data Engineer, I",Zebra Technologies,"Bengaluru, Karnataka, India","Remote Work: Hybrid

Overview:

At Zebra, we are a community of innovators who come together to create new ways of working to make everyday life better. United by curiosity and care, we develop dynamic solutions that anticipate our customer’s and partner’s needs and solve their challenges.

Being a part of Zebra Nation means being seen, heard, valued, and respected. Drawing from our diverse perspectives, we collaborate to deliver on our purpose. Here you are a part of a team pushing boundaries to redefine the work of tomorrow for organizations, their employees, and those they serve.

You have opportunities to learn and lead at a forward-thinking company, defining your path to a fulfilling career while channeling your skills toward causes that you care about – locally and globally. We’ve only begun reimaging the future – for our people, our customers, and the world.

Let’s create tomorrow together.

A Data Engineer will be responsible for understanding the client's technical requirements, design and

build data pipelines to support the requirements. In this role, the Data Engineer, besides developing the

solution, will also oversee other Engineers' development. This role requires strong verbal and written

communication skills and effectively communicate with the client and internal team. A strong

understanding of databases, SQL, cloud technologies, and modern data integration and orchestration

tools like Azure Data Factory (ADF), Informatica, and Airflow are required to succeed in this role.

Responsibilities:
• Play a critical role in the design and implementation of data platforms for the AI products.
• Develop productized and parameterized data pipelines that feed AI products leveraging GPUs and

CPUs.
• Develop efficient data transformation code in spark (in Python and Scala) and Dask.
• Build workflows to automate data pipeline using python and Argo.
• Develop data validation tests to assess the quality of the input data.
• Conduct performance testing and profiling of the code using a variety of tools and techniques.
• Build data pipeline frameworks to automate high-volume and real-time data delivery for our data

hub.
• Operationalize scalable data pipelines to support data science and advanced analytics.
• Optimize customer data science workloads and manage cloud services costs/utilization.

Qualifications:
• Minimum Education:

o Bachelors, Master's or Ph.D. Degree in Computer Science or Engineering.
• Minimum Work Experience (years):

o 1+ years of experience programming with at least one of the following languages: Python,

Scala, Go.

o 1+ years of experience in SQL and data transformation

o 1+ years of experience in developing distributed systems using open source technologies

such as Spark and Dask.

o 1+ years of experience with relational databases or NoSQL databases running in Linux

environments (MySQL, MariaDB, PostgreSQL, MongoDB, Redis).
• Key Skills and Competencies:

o Experience working with AWS / Azure / GCP environment is highly desired.

o Experience in data models in the Retail and Consumer products industry is desired.

o Experience working on agile projects and understanding of agile concepts is desired.

o Demonstrated ability to learn new technologies quickly and independently.

o Excellent verbal and written communication skills, especially in technical communications.

o Ability to work and achieve stretch goals in a very innovative and fast-paced environment.

o Ability to work collaboratively in a diverse team environment.

o Ability to telework

o Expected travel: Not expected.

To protect candidates from falling victim to online fraudulent activity involving fake job postings and employment offers, please be aware our recruiters will always connect with you via @zebra.com email accounts. Applications are only accepted through our applicant tracking system and only accept personal identifying information through that system. Our Talent Acquisition team will not ask for you to provide personal identifying information via e-mail or outside of the system. If you are a victim of identity theft contact your local police department.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=bW-fZ1EuMUGfOQQXAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOsQrCMBAAUFz7CU63CVIbFVzsJoqoq5NLucQjica7kkuhv-LfWpc3v-o7qxZHLAgn9pGJcg0XWMFVLChhdgGE4SziE83bUEqve2NUU-O1YImucfIxwmRlNC-x-qfTgJn6hIW67W49Nj375eZBNiPcyQWWJD6SQmQ4EHtMQx5quGHmafLGqcDPiD8UdVAxnAAAAA&shmds=v1_AUFQtOPow90SB2ySoosWE4xgGxDLSiO5e-uEBTXc9ear99jQCA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=bW-fZ1EuMUGfOQQXAAAAAA%3D%3D,7 days ago
"Data Engineer with Python ,Bigdata & Aws-Gunjan(HM)",CodersBrain,"Bagaluru, Karnataka, India","Greetings from Coders Brain Technology Pvt. Ltd.

Coders Brain is a global leader in its services, digital, and business solutions that partners with its clients to simplify, strengthen, and transform their businesses. We ensure the highest levels of certainty and satisfaction through a deep-set commitment to our clients, comprehensive industry expertise, and a global network of innovation and delivery centers.

Location: Bangalore - Hybrid Mode( 3 days Office,2 days Work from Home)

Client Name: Happiest Minds

Position: Permanent with Happiest Minds

Experience: 7+ years

Notice Period: Immediate or 15 days(Serving Candidates)

Role:- Python Developer

7+ years of development experience in the core tools and technologies like SQL, Python, AWS (Lambda, Glue, S3, Redshift, Athena, IAM Roles & Policies) , PySpark used by the solution services team.
• Architect and build high-performance and scalable data pipelines adhering to data lakehouse, data warehouse & data marts standards for optimal storage, retrieval, and processing of data.
• 3+ years of experience in Agile Development and code deployment using Github & CI-CD pipelines.
• 2+ years of experience in job orchestration using Airflow.
• Expertise in the design, data modelling, creation, and management of large datasets/data models
• Ability to work with business owners to define key business requirements and convert to technical specifications
• Experience with security models and development on large data sets
• Ensure successful transition of applications to service management team through planning and knowledge transfer
• Develop expertise of processes and data used by business functions within the US Affiliate
• Responsible for system testing, ensuring effective resolution of defects, timely discussion around business issues and appropriate management of resources relevant to data and integration
• Partner with and influence vendor resources on solution development to ensure understanding of data and technical direction for solutions as well as delivery

Skills: python,bigdata,aws",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Wb1Z5KM4fY-euy0RAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOPQrCQBBGsc0RrKYSlZiIYKOVUYk_CN5AJsmwuxpnws6GxHt5QCN8vOa94ou-o-hywIBwZOOYyEPngoX7J1hhiDNnqr-dwK7TRd7yE3l6us1gARcpQAl9aWEocxFT03hrQ2h0k6aqdWI0YHBlUso7FaZC-vQphf7xUIuemhoDPVbrZZ80bOazvVTkNfPoGIZlaLBufRvDFT0PN14Yw5krhz9212bRtwAAAA&shmds=v1_AUFQtOOK4gPudu7b8CPOXpizvBCxOm-lp2BrrVeufPLwXFYLPw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Wb1Z5KM4fY-euy0RAAAAAA%3D%3D,N/A
Global Facilities Data Engineer,1840 Micron Tech. OP India LLP,India,"Our vision is to transform how the world uses information to enrich life for all. Micron Technology is a world leader in innovating memory and storage solutions that accelerate the transformation of information into intelligence, inspiring the world to learn, communicate and advance faster than ever. Broad knowledge and experience in: Understanding of Big Data Engineering/processing, Business Intelligence and Advanced analytics Developing ETL/ELT processes Knowledge in databases and Data warehouse modeling Knowledge in Cloud based data engineering and Machine Learning Models Knowledge in building APIs for application integration Experience with various frameworks and processes, such as Agile Determine transformation requirements and develop processes to bring structured and unstructured data from the source to a new physical Data Model Work with Data Scientists to implement strategies for cleaning and preparing data for analysis, to develop data imputation algorithms, and optimize performance of big data and machine learning systems Above average skills in: Big Data Engineering and Processing using Hadoop stack (Hadoop, Hive, HDFS, Spark and HBase etc.) Develop ETL/ELT processing using Apache Ni-Fi Strong background on SQL and databases Programming Skills in Python or Scala Data Analysis and Validation skills Demonstrated ability to: Work in a dynamic, fast-paced, work environment Self-motivated with the ability to work under minimal direction To adapt to new technologies and learn quickly A passion for data and information with strong analytical, problem solving, and organizational skills Work in multi-functional groups, with diverse interests and requirements, to a common objective Communicate very well with distributed teams (written, verbal and presentation) Education and Experience: Bachelor’s degree in computer science, Engineering, or a related field. 2+ years of experience in data engineering, with a strong background in data architecture and ETL processes. Big Data Engineering and Processing. Develop ETL/ELT processing using Apache Ni-Fi Strong background on SQL and databases Programming Skills in Python or Scala Data Analysis and Validation skills About Micron Technology, Inc. We are an industry leader in innovative memory and storage solutions transforming how the world uses information to enrich life for all. With a relentless focus on our customers, technology leadership, and manufacturing and operational excellence, Micron delivers a rich portfolio of high-performance DRAM, NAND, and NOR memory and storage products through our Micron® and Crucial® brands. Every day, the innovations that our people create fuel the data economy, enabling advances in artificial intelligence and 5G applications that unleash opportunities — from the data center to the intelligent edge and across the client and mobile user experience. To learn more, please visit micron.com/careers All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status. To request assistance with the application process and/or for reasonable accommodations, please contact hrsupport_india@micron.com Micron Prohibits the use of child labor and complies with all applicable laws, rules, regulations, and other international and industry labor standards. Micron does not charge candidates any recruitment fees or unlawfully collect any other payment from candidates as consideration for their employment with Micron.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=6Kfe9feATZWgyoK0AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXMsQrCMBCAYVz7CE63CYJJFRXRVS1KxQ7ucolHchJzJcnQp_FZVVx--Ja_eo-qdRPEYIAjWg5cmDLssSAcouNIlGAGZzGQCZP1IBEaERdovPOl9Hmrdc5BuVywsFVWXloiGRn0U0z-5Z49JuoDFrovVvWg-uimk_lmWcOFbfoOb2S9gmsHp_hghLbtgOMfHxG2UUmhAAAA&shmds=v1_AUFQtON9dXyUBRY52Y6_tW_Q-1JrdEcTR0zdyE5uq7xUPihM3A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=6Kfe9feATZWgyoK0AAAAAA%3D%3D,26 days ago
Principal – Software Engineering - Data Engineer/DBA,Fidelity Business Services India Private Limited,India,"Job Description: Job Title – Principal – Software Engineering Database Administration A value focused professional who can deliver benefits to the organization and gain from the Organization’s wealth of data and experience. The Purpose of This Role The FFIO Data Technology Services team provides Data services to the FFIO business unit. The team provides Database support, Design and Development support to Product Development teams and manages the data environments efficiently in the Cloud and on-premise environments. Apart from this, the Data Technology Services team also builds capabilities that help in increasing efficiency of the database platforms and increases productivity of the teams. In this role, you will be working with multiple teams including Architects, Product Development teams, Enterprise Cloud and Security teams. The Value You Deliver You will use your experience and subject matter expertise to help the product teams implement the best practices from a data perspective. You will interact with the product teams and provide technology guidelines / capabilities that contribute to their development and support activities. You will leverage your experience in providing analytical inputs and insights to the team when needed and your overall guidance to the team in line with the Business Unit priorities. Your experience and expertise will be leveraged by teams in their areas. You will define, own and drive initiatives within the team that increase productivity and add value in alignment with the Business Unit and Team priorities. Some examples include self service capabilities, automation of regular activities and reporting insights regularly. You have a very good understanding of the Cloud environments (AWS) and have managed large scale environments in Cloud. The Skills that are Key to this role Technical / Behavioral Demonstration of Experience and Expertise in the following areas. Collaborate with the Architecture teams on understanding the Application / Data requirements and providing necessary inputs or seeking clarifications. Work with Application Development and Performance Teams on Data & Database components in the Design, Development, Testing & Deployment phases in the areas of Oracle SQL and PLSQL, Snowflake, Aurora PostgreSQL & Golden Gate Extensive Hands-On Experience and Subject Matter Expertise of Oracle Performance Tuning, SQL Tuning and Snowflake Good Understanding of the Application schemas, their contribution to the business functions and the operational aspects of the applications. Application Areas such as Schema Design, Data Models, Best Practices & Database Features Identify opportunities on the Application side where the team can contribute to the Data Engineering activities within the application through automation capabilities. Proactively identify opportunities for automation and Optimization Challenge current state where applicable and be self-motivated to contribute to the solution & collaborate with other team members. To identify and execute improvements in database services, through automation, self-service enablement, and process improvements. Participate in Design reviews specific to Data components. Participate in level 3 on-call rotation to support critical applications based on business requirements. Work with multiple projects and product teams simultaneously. Traditional Database Technologies – Oracle, DB2 UDB, MS SQL Server, Sybase Big Data Technologies Exposure – No SQL Databases such as Cassandra, MongoDB AWS Cloud Technologies such as RDS, EC2, EKS AWS Cloud Concepts Excellent troubleshooting and error analysis skills Working knowledge of Information Security concepts Solid understanding of software development lifecycle Exceptional customer service skills Ability to learn and adapt to new technologies. Ability to work in fast paced, highly demanding environment. Exceptional Communication skills with the ability to articulate thoughts based on the audience level (from the Developer to the Senior BU Leadership) Qualified applicant should have 15+ years of Database Support, Design, Performance & Tuning experience working in a complex, multi-platform environment and should have done production support. The Skills that are Good to Have for this role. Strong verbal and written communication skills, both technical and non-technical Ability to communicate based on the target audience. Agile Methodology How Your Work Impacts the Organization The FFIO (Fidelity Fund and Investment Operations) Technology Group is responsible for delivering top quality scalable systems solutions for the Asset Management business functions within Fidelity Investments. The FFIO Technology Platform consists of mainframe and distributed architecture including multiple interfaces both internal and external to the FFIO Organization. The systems employed include the source fund accounting, pricing, tax, and data exchange platforms. The data is sourced from multiple accounting, trading, and reference data systems within Fidelity, stored in central database and shared with internal or external customer groups through system-to-system interfaces to support fund setup, reporting, analytic and oversight functions. The DTS team has visibility to senior management by virtue of their activities and their impact. The Expertise We’re Looking For Bachelors or Masters Engineering Degree or any equivalent degree with the relevant experience. 12-15 years of relevant experience. Certifications: Category: Information Technology At Fidelity, we are passionate about making our financial expertise broadly accessible and effective in helping people live the lives they want! We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associates for their unique perspectives and experiences. For information about working at Fidelity, visit FidelityCareers.com. Fidelity Investments is an equal opportunity employer. Fidelity will reasonably accommodate applicants with disabilities who need adjustments to participate in the application or interview process. To initiate a request for an accommodation please contact the following: For roles based in the US: Contact the HR Leave of Absence/Accommodation Team by sending an email to accommodations@fmr.com, or by calling 800-835-5099, prompt 2, option 2 For roles based in Ireland: Contact AccommodationsIreland@fmr.com For roles based in Germany: Contact Accommodationsgermany@fmr.com Fidelity Privacy Policy",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=inlX7U8GJye12kPTAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2OOwoCQRBEMfUIRh0L7ohgoEaKHxQFwQPI7Gw72zJOL9vtL_MOHsp7eBJHBJMKXlG8ar4azc22puiosgHejyfs-KBXWyPMoqeImEoPHZhatX9kppNxYivOQdDWrgSOsGD2AVujUrWSoTEiIfOiVslljk-GI-Z8M0fO5Rt7KZOlClZx3-t3b1kVfXswpwID6R0mZ0kqEdhhfSGHAstYkIV09pImsKYTKRZA8Vd8AEcbtR_KAAAA&shmds=v1_AUFQtOP-juSHihcrbA_i6H2MkjUW_LGMtyRWxz5CYckmNuUvfg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=inlX7U8GJye12kPTAAAAAA%3D%3D,23 days ago
data-engineer-14199,Siemens,"Bengaluru, Karnataka, India",No job summary available.,https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=qMlu0yPj4YRbrxSyAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOMQ6CQBBA0dhyBKupLAiwYrRAOxujlh6ADDDZXV1myM6ScA1vLDa_fT_7brJ8wIQlsfVMFMv6WDcNlPCQDpQw9g6E4SZiA20vLqVJz8aohspqwuT7qpfRCFMni3lLp_-06jDSFDBRezjtl2pim-9enkZiBc9wXT0Mc5wLeGLk9eCDBdx58PgDAhXyWpQAAAA&shmds=v1_AUFQtOP5g7mO7Sq7Nm-LRD_JoAzf4Tcl_Y18-D-m80M4JIuEbw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=qMlu0yPj4YRbrxSyAAAAAA%3D%3D,12 days ago
Senior Manager - Database Engineering and Operations,United Airlines,"Gurugram, Haryana, India","Description

United's Digital Technology team designs, develops, and maintains massively scaling technology solutions brought to life with innovative architectures, data analytics, and digital solutions.

Our Values: At United Airlines, we believe that inclusion propels innovation and is the foundation of all that we do. Our Shared Purpose: ""Connecting people. Uniting the world."" drives us to be the best airline for our employees, customers, and everyone we serve, and we can only do that with a truly diverse and inclusive workforce. Our team spans the globe and is made up of diverse individuals all working together with cutting-edge technology to build the best airline in the history of aviation.

With multiple employee-run ""Business Resource Group"" communities and world-class benefits like health insurance, parental leave, and space available travel, United is truly a one-of-a-kind place to work that will make you feel welcome and accepted. Come join our team and help us make a positive impact on the world.

Job overview and responsibilities

As a Senior Manager, you will lead a team of highly talented and motivated database administrators to
support mission critical SQL Server and Cloud databases which are critical data assets of the company.
Our team continually innovates to deliver digital solutions to support complex, dynamic operations.
• Applies a broad range of expertise in database technology principles, best practices, and procedures to complete difficult and complex assignments crossing multiple functional areas to resolve data conflicts for naming conventions, data privacy, and data ownership
• Develops and documents database processes and methodologies
• Maintains knowledge of DBMS trends, developments, and best practices and executes Cloud Migrations to IaaS, DBaaS & PaaS as needed
• Provides support in database design and application development by performing periodic exercises
to validate backup recovery procedures, maintain core database tuning, and monitor production
data processes, performance, and database capacity issues
• Owns and manages all aspects of day-to-day operations of the SQL Server, Oracle and Cloud
environment including administration, design, development, and delivery of database solutions
• Partners and collaborates with the infrastructure, server, storage, networking, security, and
applications team on database backup and recovery, disaster recovery, and high availability and
develops a multiyear strategy
• Leads and manages a team of technical experts on the architecture, design, development and
implementation of innovative data solutions that address business problems and help achieve
strategic goals
• Directs technical analysis and design activities on large, complex software development projects and
provides technical expertise to other technology team members engaged in design and development
activities
• Takes an active role in promoting and implementing a progressive, forward-moving organization by
proactively upgrading, maintaining, and monitoring the database to prevent potential issues;
diagnose and forecast the database's health and capacity
• Seek innovative ways to improve efficiency and quality related to data management and
administration

This position is offered on local terms and conditions. Expatriate assignments and sponsorship for employment visas, even on a time-limited visa status, will not be awarded. This position is for United Airlines Business Services Pvt. Ltd - a wholly owned subsidiary of United Airlines Inc.

Qualifications

Required
• * Bachelor’s degree in computer science, Mathematics, Information Systems, or other related field
• 10+ years of hands-on experience as SQL Server DBA, Oracle DBA, NoSQL DBA or Cloud DBA
• 5+ years of experience as technical manager or other leadership role
• Experience managing implementing enterprise-scale, operationally critical databases
• Strong understanding of database administration best practices, design patterns, and standards
• Comprehensive technical knowledge of database architecture, design, construction and
implementation to the oversight of significant assignment
• Must be legally authorized to work in India for any employer without sponsorship
• Must be fluent in English (written and spoken)
• Successful completion of interview required to meet job qualification
• Reliable, punctual attendance is an essential function of the position

Preferred
• * Master's degree
• Knowledge and Experience in AWS RDS and NoSQL Technologies",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=9QRrTuL7mKe11P-cAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOuwrCQBBFsc0nWE3tIyuCjVaC4guxEOswSYbNSjITZjYQv8zfc21OdTj3Zt9JdnsSB1G4I6MnhSUcMGKJRnBkH5hIA3tAruHRk2IMwpasq5RghFo1IAwnEd_SdNfE2NvWObM29xaTXeWVdE6YShndW0r7o7AGlfoWIxXrzWrMe_az-YtDpBr2Qdu0axBSd9DBK3YLOKN-0sUFXLgO-AONtlqOugAAAA&shmds=v1_AUFQtONRU83UeifKRZczASvc6CPyJ7NbHC5GNOut2A0v2l0qiQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=9QRrTuL7mKe11P-cAAAAAA%3D%3D,19 hours ago
Snowflake Data Engineer,KPI Partners,India,"About KPI Partners

KPI Partners is a 5 times Gartner recognized data, analytics, and AI consulting company. We are leaders in data engineering on Azure, AWS, Google, Snowflake and Databricks. Founded in 2006, KPI has over 500 consultants and has successfully delivered over 1,000 projects to our clients in the US. We are looking for skilled data engineers who want to work with the best team in data engineering!

About the Role:

We are seeking highly skilled and experienced Data Engineers to join our dynamic Snowflake team at KPI's Hyderabad office. You will work on challenging and multi-year data transformation projects for our clients. This is an excellent opportunity for a talented data engineer to play a key role in building innovative data solutions. If you are passionate about working with large-scale data systems and enjoy solving complex engineering problems, this role is for you.

Key Responsibilities:
• Design and build data engineering pipelines using SQL and Snowpark
• Collaboration: Work closely with cross-functional teams to understand business requirements and translate them into robust data solutions.
• Data Warehousing: Design and implement data warehousing solutions, ensuring scalability, performance, and reliability.
• Continuous Learning: Stay up to date with modern technologies and trends in data engineering and apply them
• Experience in agile delivery methodology in a leading role as part of a wider team
• Strong team collaboration and experience working with KPI team members and client team members in the US, India and other global locations
• Mentorship: Provide guidance and mentorship to junior data engineers, ensuring best practices in coding, design, and development.

Must-Have Skills & Qualifications:
• 3+ years of Snowflake experience in building data engineering pipelines.
• Proven expertise in SQL for querying, manipulating, and analyzing large datasets

Good-to-Have Skills:
• Snowflake Certification is a plus

Education:

BA/BS in Computer Science, Math, Physics, or other technical fields is a plus.

Apply Now!

If you’re ready to take on a key role in data engineering and work on transformative projects with a talented team, we encourage you to apply today.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=iWvP_WCVW4Svh4bDAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFy7uzjdLJiI4KKrItWl4AeUazyT1HhXcgf2A_xw8Q2v-S4ad2f5PAu-CE5oCGeOmYkqbOAqAyhhDQmE4SISC62OyWzSg_eqxUU1tBxckLcXpkFmP8qg_3pNWGkqaNTv9tvZTRzXy1vXQofVmKpCZmj5kfEHk8p8RocAAAA&shmds=v1_AUFQtONALN0lxP8tBJsMYo9ikjxuTp30TrEeOzvi0EN1M5HLIQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=iWvP_WCVW4Svh4bDAAAAAA%3D%3D,13 days ago
Senior Data Engineer - SQL/Python,Talescope,"Hyderabad, Telangana, India","Technical knowledge :

- AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins, and AWS CodePipeline.

Role Summary :

As a Senior Data Engineer,with over 5 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth.

You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.

Key Responsibilities :

- Design and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.

- Lead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.

- Optimize data processing and storage for performance and cost.

- Implement data security and compliance best practices, in collaboration with the IT security team.

- Build flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.

- Work closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.

- Collaborate with cross-functional teams to understand their data needs and translate them into technical requirements.

- Continuously evaluate new technologies and AWS services to enhance data capabilities and performance.

- Create and maintain comprehensive documentation of data pipelines, architectures, and workflows.

- Participate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.

- Present findings to executive leadership and recommend data-driven strategies for business growth.

- Communicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.

- Handle clients in multiple industries at the same time, balancing their unique needs.

- Provide mentoring and guidance to junior data engineers and team members.

Requirements :

- 5+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.

- Proven experience in designing and delivering large-scale data warehousing and data processing solutions.

- Lead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.

- Bachelor's or Master's degree in Computer Science, Engineering, or a related technical field.

- Deep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.

- Implement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.

- Experience with cloud platforms such as AWS, Azure, and GCP.

- Incorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.

- Implement continuous integration and delivery pipelines using GitLab, Jenkins, and AWS CodePipeline.

- Ensure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.

- Mentor junior engineers, fostering a culture of continuous learning and improvement.

- Excellent problem-solving and analytical skills, with a strategic mindset.

- Strong communication and leadership skills, with the ability to influence stakeholders at all levels.

- Ability to work independently as well as part of a team in a fast-paced environment.

- Advanced data visualization skills and the ability to present complex data in a clear and concise manner.

- Excellent communication skills, both written and verbal, to collaborate effectively across teams and levels.

Preferred Skills :

- Experience with Databricks, Snowflake, and machine learning pipelines.

- Exposure to real-time data streaming technologies and architectures.

- Familiarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=gcHNgHm9Ry5CoCPVAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHvQrCMBAAYFz7CE43itRGBEF0VfzBQWn3ckmPJBLvQpLBPo5val0--KrvrNq1xF4SHLEgnNh6JkqwgvZ5V4-xOOEpN9GQCZNxMP0sYgPND66UmPdK5RwamwsWbxojbyVMWj7qJTr_6bPDRDFgoX6zXX-ayHa56DBQNhIJPMNlHCihxqGGjgKyRcYarjx4_AGmqptgpAAAAA&shmds=v1_AUFQtOPWnh-o2BPkkPVtZkdjw-zaJGRmK93Dpc2F9lvaSh4urA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=gcHNgHm9Ry5CoCPVAAAAAA%3D%3D,N/A
"Senior Software Engineer, Data",Roku,"Bengaluru, Karnataka, India","Teamwork makes the stream work.Roku is changing how the world watches TV

Roku is the #1 TV streaming platform in the US and Mexico, and we've set our sights on powering every television in the world. Roku pioneered streaming to the TV. Our mission is to be the TV streaming platform that connects the entire TV ecosystem. We connect consumers to the content they love, enable content publishers to build and monetize large audiences, and provide advertisers unique capabilities to engage consumers.

From your first day at Roku, you'll make a valuable - and valued - contribution. We're a fast-growing public company where no one is a bystander. We offer you the opportunity to delight millions of TV streamers around the world while gaining meaningful experience across a variety of disciplines.

About the Role:

We are seeking a talented and experienced Senior Software Engineer with a strong background in big data technologies, including Apache Spark, Apache Airflow, and Trino. This hybrid role bridges software and data engineering, requiring expertise in designing, building, and maintaining scalable systems for both application development and data processing. You will collaborate with cross-functional teams to design and manage robust, production-grade, large-scale data systems. The ideal candidate is a proactive self-starter with a deep understanding of high-scale data services and a commitment to excellence.

Key Responsibilities:

Software Development:
• Write clean, maintainable, and efficient code, ensuring adherence to best practices through code reviews.

Big Data Engineering:
• Design, develop, and maintain data pipelines and ETL workflows using Apache Spark, Apache Airflow, and Trino. Optimize data storage, retrieval, and processing systems to ensure reliability, scalability, and performance.
• Develop and fine-tune complex queries and data processing jobs for large-scale datasets.
• Monitor, troubleshoot, and improve data systems for minimal downtime and maximum efficiency.

Collaboration & Mentorship:
• Partner with data scientists, software engineers, and other teams to deliver integrated, high-quality solutions.
• Provide technical guidance and mentorship to junior engineers, promoting best practices in data engineering.

We're excited if you have:
• 5+ years of experience in software and/or data engineering with expertise in big data technologies such as Apache Spark, Apache Airflow and Trino.
• Strong understanding of SOLID principles and distributed systems architecture.
• Proven experience in distributed data processing, data warehousing, and real-time data pipelines.
• Advanced SQL skills, with expertise in query optimization for large datasets.
• Exceptional problem-solving abilities and the capacity to work independently or collaboratively.
• Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent experience).
• Excellent verbal and written communication skills.

Preferred Skills:
• Experience with cloud platforms such as AWS, GCP, or Azure, and containerization tools like Docker and Kubernetes.
• Familiarity with additional big data technologies, including Hadoop, Kafka, and Presto.
• Strong programming skills in Python, Java, or Scala.
• Knowledge of CI/CD pipelines, DevOps practices, and infrastructure-as-code tools (e.g., Terraform).
• Expertise in data modeling, schema design, and data visualization tools.

#LI-GL1

Benefits

Roku is committed to offering a diverse range of benefits as part of our compensation package to support our employees and their families. Our comprehensive benefits include global access to mental health and financial wellness support and resources. Local benefits include statutory and voluntary benefits which may include healthcare (medical, dental, and vision), life, accident, disability, commuter, and retirement options (401(k)/pension). Our employees can take time off work for vacation and other personal reasons to balance their evolving work and life needs. It's important to note that not every benefit is available in all locations or for every role. For details specific to your location, please consult with your recruiter.

The Roku Culture

Roku is a great place for people who want to work in a fast-paced environment where everyone is focused on the company's success rather than their own. We try to surround ourselves with people who are great at their jobs, who are easy to work with, and who keep their egos in check. We appreciate a sense of humor. We believe a fewer number of very talented folks can do more for less cost than a larger number of less talented teams. We're independent thinkers with big ideas who act boldly, move fast and accomplish extraordinary things through collaboration and trust. In short, at Roku you'll be part of a company that's changing how the world watches TV. 

We have a unique culture that we are proud of. We think of ourselves primarily as problem-solvers, which itself is a two-part idea. We come up with the solution, but the solution isn't real until it is built and delivered to the customer. That penchant for action gives us a pragmatic approach to innovation, one that has served us well since 2002. 

To learn more about Roku, our global footprint, and how we've grown, visit https://www.weareroku.com/factsheet.

By providing your information, you acknowledge that you have read our Applicant Privacy Notice and authorize Roku to process your data subject to those terms.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=L0kUEmaAbe9mwT8-AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNMQ6CQBBA0dhyBKtJ7AyyxoRGO6MxaicHMAOOy8o6Q3aGyFW8rdj88v3sO8vKijhIgkqe9sFEcGQfmCjlcEBDWMFFalDC1LQgDCcRH2m-a8163TqnGguvhhaaopG3E6ZaRveSWv-5azuhfUSj-6Zcj0XPfrm4STdAYNgTe4xDGnK4YuLp12EOZ34E_AHl5bdPnAAAAA&shmds=v1_AUFQtOOcYMI2RGPrCeSss4xqgLKkTvG_tV1o8D5t1Av93KYFQA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=L0kUEmaAbe9mwT8-AAAAAA%3D%3D,14 days ago
Associate III - Data Engineering - Senior Data Engineer,UST Global,"Thiruvananthapuram, Kerala, India","3 - 5 Years

1 Opening

Trivandrum

Role description

Job Title: Data Engineer

Hiring Location: Bangalore, Chennai, Hyderabad, Kochi, Trivandrum, Pune, Noida

Experience Range:
• 4+ years of experience as a Data Engineer or in a similar role

Must-Have Skills:
• Big Data Processing: Hands-on experience with PySpark
• Cloud Platforms: Experience with Google Cloud Platform (GCP) or any other cloud provider
• Data Pipeline Development: Expertise in Spark, Hadoop, Hive
• Database & Querying: Strong proficiency in SQL
• Collaboration & Requirements Gathering: Ability to work with product managers and data stewards to translate data requirements into efficient workflows

Good-to-Have Skills:
• Experience with other cloud platforms like AWS or Azure
• Familiarity with ETL tools and frameworks
• Knowledge of data governance and data quality best practices
• Exposure to real-time data processing technologies (Kafka, Flink, etc.)

Key Responsibilities:
• Design, implement, and optimize scalable data pipelines using Spark, Hadoop, Hive, and other technologies
• Develop and maintain ETL processes for efficient data ingestion and transformation
• Monitor and troubleshoot data pipelines to ensure high availability and minimal downtime
• Work closely with cross-functional teams to understand data needs and deliver effective solutions
• Optimize query performance and data storage strategies for improved efficiency
• Ensure data integrity, quality, and security in compliance with best practices

Soft Skills & Competencies:
• Strong problem-solving skills and ability to work in a collaborative environment
• Excellent communication skills, both written and verbal
• Ability to adapt to evolving technologies and business needs

Education & Qualifications:
• Bachelor’s degree in Computer Science, Engineering, or a related field
• Experience working in Agile or DevOps environments is a plus

Skills

Spark,Hadoop,Hive,Gcp

About UST

UST is a global digital transformation solutions provider. For more than 20 years, UST has worked side by side with the world’s best companies to make a real impact through transformation. Powered by technology, inspired by people and led by purpose, UST partners with their clients from design to operation. With deep domain expertise and a future-proof philosophy, UST embeds innovation and agility into their clients’ organizations. With over 30,000 employees in 30 countries, UST builds for boundless impact—touching billions of lives in the process.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=HmeNN7msAPgJddeeAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1WNMQrCQBAAsc0TrLaWmBPBRitBCVGwSazDJi53J5fdcHsRX-fbjKXNFDPFZJ9FdjuqSu8xEVRVBWs4YUI4s_VMFD3bWdXEXuJ_mfVFOlDC2DsQhlLEBloeXEqj7o1RDYXVhMn3RS-DEaZO3uYpnf7QqsNIY5jH7Xa3eRcj25W51w2UQToM4Bka5-P0QkZODscp4pDDlSIGzKHih8cvA2FoR8EAAAA&shmds=v1_AUFQtONxHepqY1zIy2oESzKUHl8okASkMR0yCH6QNKdlK2vEyA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=HmeNN7msAPgJddeeAAAAAA%3D%3D,7 days ago
"Data Engineer II, Amazon Last Mile",ADCI HYD 13 SEZ - H84,"Telangana, India","• 3+ years of data engineering experience
• Experience with data modeling, warehousing and building ETL pipelines

As part of the Last Mile Science & Technology organization, you’ll partner closely with Product Managers, Data Scientists, and Software Engineers to drive improvements in Amazon's Last Mile delivery network. You will leverage data and analytics to generate insights that accelerate the scale, efficiency, and quality of the routes we build for our drivers through our end-to-end last mile planning systems. You will develop complex data engineering solutions using AWS technology stack (S3, Glue, IAM, Redshift, Athena). You should have deep expertise and passion in working with large data sets, building complex data processes, performance tuning, bringing data from disparate data stores and programmatically identifying patterns. You will work with business owners to develop and define key business questions and requirements. You will provide guidance and support for other engineers with industry best practices and direction. Analytical ingenuity and leadership, business acumen, effective communication capabilities, and the ability to work effectively with cross-functional teams in a fast-paced environment are critical skills for this role.

Key job responsibilities
• Design, implement, and support data warehouse / data lake infrastructure using AWS big data stack, Python, Redshift, Quicksight, Glue/lake formation, EMR/Spark/Scala, Athena etc.
• Extract huge volumes of structured and unstructured data from various sources (Relational /Non-relational/No-SQL database) and message streams and construct complex analyses.
• Develop and manage ETLs to source data from various systems and create unified data model for analytics and reporting
• Perform detailed source-system analysis, source-to-target data analysis, and transformation analysis
• Participate in the full development cycle for ETL: design, implementation, validation, documentation, and maintenance.
• Drive programs and mentor resources to build scalable solutions aligning to team's long term strategy
• Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
• Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=GCVH5M4yl719qsZQAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNuw6CQBBA0djyCVbTapD1mfioiBDBaKWNNmTAybJmmSHsFsTf8UfF5januMF3FOwS9Agpa8NEHeR5CHGDH2G4oPNwNZZgBmcpwRF2VQ2DnES0pfGh9r51e6Wcs5F2Hr2pokoaJUyl9OotpfuncDV21Fr0VCw38z5qWU8ncXLMIXsksFjBLX0Ok2y7BsNwJ4uskTGEnF8Gf2HD0tqmAAAA&shmds=v1_AUFQtOOaHaRrikwMIz_aehC4zsTqK6ou5g8cUGS4BL9_cL97bQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=GCVH5M4yl719qsZQAAAAAA%3D%3D,N/A
Business Title Data Engineer,Equifax,"Thiruvananthapuram, Kerala, India","Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.

Equifax is seeking creative, high-energy and driven software engineers with hands-on development skills to work on a variety of meaningful projects. Our software engineering positions provide you the opportunity to join a team of talented engineers working with leading-edge technology. You are ideal for this position if you are a forward-thinking, committed, and enthusiastic software engineer who is passionate about technology.

What you’ll do
• Design requirements on small systems or modules of medium systems (large scale) environment and technical documentation.
• Apply basic principles of software engineering and follow instructions.
• Provide meaningful feedback on the release process, code review, and design review.
• Easily absorb and apply new information.
• Display a cooperative attitude and share knowledge.
• Apply modern software development practices (serverless computing, microservices architecture, CI/CD, infrastructure-as-code, etc.)
• Work across teams to integrate our systems with existing corporate product platforms
• Participate in technology roadmap and architecture discussions to turn business requirements and vision into reality.
• Participate in a tight-knit engineering team employing agile software development practices.
• Leverage automation within scope of effort

What experience you need
• Bachelor's degree or equivalent experience
• 6 months+ experience with Java software programming experience
• 6 months+ experience with Cloud technology: GCP, AWS, or Azure

What could set you apart
• Self-starter that identifies/responds to priority shifts with minimal supervision
• UI development (e.g. HTML, JavaScript, Angular and Bootstrap)
• Experience with backend technologies such as JAVA/J2EE, SpringBoot, SOA and Microservices
• Source code control management systems (e.g. SVN/Git, Github) and build tools like Maven & Gradle.
• Agile environments (e.g. Scrum, XP)
• Relational databases (e.g. SQL Server, MySQL)
• Atlassian tooling (e.g. JIRA, Confluence, and Github)
• Developing with modern JDK (v1.7+)
• Automated Testing: JUnit, Selenium, LoadRunner, SoapUI

We offer a hybrid work setting, comprehensive compensation and healthcare packages, attractive paid time off, and organizational growth potential through our online learning platform with guided career tracks.

Are you ready to power your possible? Apply today, and get started on a path toward an exciting new career at Equifax, where you can make a difference!

Primary Location:
IND-Trivandrum-Equifax Analytics-PEC

Function:
Function - Tech Dev and Client Services

Schedule:
Full time",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=bkiPpo3-MgjdAf0lAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIMQ7CMAwAQLH2CUyeUWkQiAU2RIWAtTtyi0mCUifEDupzeCqw3HDVZ1ZtDkU8kwh0XgPBERWhZfs7yrCES-xBCPPgIDKcYrSB5nunmmRnjEhorCiqH5ohjiYy9XEyz9jLn5s4zJQCKt3W29XUJLaLun0V_8AJPEPnfC5vZGR1mErGsYYrZQxYw5nvHr8GWaGfowAAAA&shmds=v1_AUFQtOPdSqdBwneJZjfiZW6CmykJI04JPHiMDGxnLL23V5eOGw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=bkiPpo3-MgjdAf0lAAAAAA%3D%3D,19 hours ago
"Python, PL/SQL, Perl, C - Sr. Data Engineer",Exusia,"Pune, Maharashtra, India","Department: Sales and Delivery Team - Empower

Industry: Information Technology & Services, Computer Software, Management Consulting

Location: India (Remote)

Experience Range: 5+ years

Basic Qualification: Master/Bachelor of Engineering or Equivalent

Travel Requirements: Not required

Website: www.exusia.com

Exusia, a cutting-edge digital transformation consultancy, and is looking for top talent in Data Engineering space with specific skills in Python / PL/SQL / C / Unix / PERL and Github to join our global delivery team's Industry Analytics practice in India.

What’s the Role?

Full-time job to work with Exusia's clients to design, develop and maintain large scale data engineering solutions. The right candidates will also get a chance to work across the entire data landscape including Data Governance, Metadata Management and will work closely with client stakeholders to capture the requirements, design and implement Analytical reporting, Compliance and Data Governance solutions.

Criteria for the Role!

Key Responsibilities:

1. Develop and maintain PL/SQL scripts and Unix shell scripts.

2. Work with Oracle databases for data extraction and ingestion.

3. Create and manage Pro*C and PERL scripts for specific tasks.

4. Utilize Github Copilot for code assistance and collaboration.

5. Develop PERL and Python interfaces for Excel data conversion.

6. Generate statements and plans as required.

Required Skills:

1. PL/SQL - Autonomous (Critical).

2. Oracle - Autonomous (Critical).

3. Unix scripting - Autonomous (Critical)

3.1. PERL - Can perform with limited supervision.

3.2. Python - Can perform with limited supervision.

4. Pro*C - Can perform with limited supervision.

4.1. C language - Can perform with limited supervision.

Optional Skills:

1. Excel data conversion - Can perform with supervision.

2. Github Copilot - Can perform with limited supervision.

Soft Skills:

1. Excellent problem-solving skills.

2. Assertive communication.

3. Very proactive.

Requirements:

About Exusia

Exusia (http://exusia.com/) is a global technology consulting company that empowers its clients to gain a competitive edge by accelerating business objectives and providing strategy and solutions in data management and analytics. The company has established its leadership position by solving some of the world's largest and most complex data problems in the financial, healthcare, telecommunications and high technology industries.

Exusia’s mission is to transform the world through the innovative use of information. Exusia was recognized by Inc. 5000 and by Crain’s publications as one of the fastest growing privately held companies in the world. Since the company’s founding in 2012, Exusia has experienced an impressive seven years of revenue growth and has expanded its operations in the Americas, Asia, Africa and UK. Exusia has recently also been recognized by publications such as the CIO Review, Industry Era, Insight Success and the CIO Bulletin for the company’s innovation in IT Services, the Telecommunications and Healthcare industries and its entrepreneurship. The company is headquartered in Miami city of Florida, United States with development centers in Pune, Hyderabad, Bengaluru and Gurugram, India.

Interested applicants should apply by forwarding

their CV to: elevate@exusia.com",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=nvccq3tFpbcwZLHMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWKQQrCMBAA8doneFo8SmxE8KI3tYhSodIHyLYuTSTulmwK9UH-03qZgWGy7yw7VJ_khA1Upa3v5WSKwcARVlDHHE6YEAruPBPFqV2lASWMrQNhOIt0geZ7l1KvO2tVQ95pwuTbvJW3FaZGRvuSRv94qMNIfcBEj812PeY9d8tFMQ7qETxDNTAZuOF0oboU0cCFnx5_-ByGDqgAAAA&shmds=v1_AUFQtOPrQkkPnfw4sSGleoAYQyYZTJS9x4VLLkF3L_puVP44zQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=nvccq3tFpbcwZLHMAAAAAA%3D%3D,4 days ago
Senior Data Engineer & Analyst,Hewlett Packard Enterprise,"Bengaluru, Karnataka, India","Welcome to Hewlett Packard Enterprise! We are currently seeking a highly skilled and experienced Senior Data Engineer & Analyst to join our dynamic team. In this role, you will have the opportunity to utilize your technical expertise and analytical skills to drive data-driven decision making and provide valuable insights to our organization. We are looking for a candidate who is passionate about data, has a strong attention to detail, and thrives in a fast-paced and collaborative environment. If you are a self-motivated and results-driven individual with a deep understanding of data engineering and analysis, we encourage you to apply for this exciting opportunity.

Design and implement data architecture and systems to support data analysis and reporting.
Develop and maintain data pipelines, ETL processes, and data warehouses.
Conduct data cleaning, transformation, and validation to ensure accuracy and consistency.
Collaborate with cross-functional teams to identify and define data needs and requirements.
Utilize advanced analytical techniques to extract insights from large and complex datasets.
Develop and maintain data models, dashboards, and visualizations to effectively communicate insights.
Identify trends, patterns, and anomalies in data to inform business decisions.
Continuously monitor and improve data quality and integrity.
Stay current with industry trends and advancements in data engineering and analysis.
Mentor and train junior data engineers and analysts.
Communicate findings and recommendations to stakeholders in a clear and concise manner.
Collaborate with IT teams to ensure data security and compliance with company policies.
Participate in the development and implementation of data governance policies and procedures.
Identify opportunities for process improvement and automation in data engineering and analysis.
Handle ad-hoc data requests and provide support to other teams as needed.

Hewlett Packard Enterprise is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=CwCuW2qIuoRPRgyQAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCUAxAYVz7CE6ZHERbERzUSVH8WwQfQNI23F69JiWJWJ_KV1SXs34n-_Sy2YU4isIGHWHLITKRwgBWjOltDmM4SglGqFUDwrATCYn6y8a9tUVRmKU8mKPHKq_kUQhTKV1xk9L-uVqDSm1Cp-t0NunylsNwvqdXInc4Y3VHrX-uk7YajSAyrIkDpqc-R3BC5d_YHUdw4DriF59vGgKyAAAA&shmds=v1_AUFQtOOOtxkLKs6fizFYZqz7zyfeRQnE5X9v1N8UTTzHkhUVDA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=CwCuW2qIuoRPRgyQAAAAAA%3D%3D,N/A
Tredence Analytics Solutions - Senior Data Engineer - Google Cloud Platform,Tredence Analytics Solutions,"Pune, Maharashtra, India","Job Description

Job Title : Senior GCP Data Engineer

Skills required : GCP, GCS, Dataproc, Airflow, Python/ Java/ Spark

Notice period : Only Immediate to 15 days joiners

Job Requirement
• Have Implemented and Architected solutions on Google Cloud Platform using the components of GCP
• Experience with Apache Beam/Google Dataflow/Apache Spark in creating end to end data pipelines.
• Experience in some of the following : Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, Machine Learning.
• Experience programming in Java, Python, etc.
• Expertise in at least two of these technologies : Relational Databases, Analytical Databases, NoSQL databases.
• Certified in Google Professional Data Engineer/ Solution Architect is a major Advantage.

Skills Required
• Experience in IT or professional services experience in IT delivery or large-scale IT analytics projects
• Candidates must have expertise knowledge of Google Cloud Platform; the other cloud platforms are nice to have.
• Expert knowledge in SQL development.
• Expertise in building data integration and preparation tools using cloud technologies (like Snaplogic, Google Dataflow, Cloud Dataprep, Python, etc).
• Experience with Apache Beam/Google Dataflow/Apache Spark in creating end to end data pipelines.
• Experience in some of the following : Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, Machine Learning.
• Experience programming in Java, Python, etc.
• Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
• Implement data pipelines to automate the ingestion, transformation, and augmentation of data sources, and provide best practices for pipeline operations.
• Capability to work in a rapidly changing business environment and to enable simplified user access to massive data by building scalable data solutions
• Advanced SQL writing and experience in data mining (SQL, ETL, data warehouse, etc.) and using databases in a business environment with complex datasets

About Tredence

Tredence is a global data science solutions provider founded in 2013 by Shub Bhowmick, Sumit Mehra, and Shashank Dubey focused on solving the last-mile problem in AI. Headquartered in San Jose, California, the company embraces a vertical-first approach and an outcome-driven mindset to help clients win and accelerate value realization from their analytics investments. The aim is to bridge the gap between insight delivery and value realization by providing customers with a differentiated approach to data and analytics through tailor-made solutions. Tredence is 2,200+ employees strong with offices in San Jose, Foster City, Chicago, London, Toronto, and India (Bangalore, Pune, Chennai, Gurugram & Kolkata), with the largest companies in retail, CPG, hi-tech, telecom, healthcare, travel, and industrials as clients.

As we complete 11 years of Tredence this year, we are on the cusp of an ambitious and exciting phase of expansion and growth. Tredence recently closed a USD 175 million Series B funding, which will help us build on growth momentum, strengthen vertical capabilities, and reach a broader customer base.

Apart from our geographic footprint in the US, Canada & UK, we plan to open offices in Kolkata. In 2023, we also plan to hire more than 1000 employees across markets. - Tredence is a Great Place to Work- (GPTW) certified company that values its employees and creates a positive work culture by providing opportunities for professional development and promoting work-life balance.

At Tredence, nothing is impossible; we believe in pushing ourselves to limitless possibilities and staying true to our tagline, Beyond Possible.

(ref:hirist.tech)",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=0gauPAJaGxUgWWHsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_32OPQrCQBCFsc0RrKaWmBVBEK1ERRSEQOxlkozJymYm7GwgntXLuIK1zSs-3l_yniTlzVNNXBHsGN0r2EqhEDcEK6wwh4LYiocDBoQjN5aJfMQnkcYR7J0MNeQOw0N8F_lFSlBCX7Ug_HNNt20IvW6MUXVZowHjSlZJZ4SplNE8pdSv3LVFT31so_tytRiznpvZ-u9By5APTClcMWZR2-AxhTPXFj8E2qe_3gAAAA&shmds=v1_AUFQtOMWE3qFQFwW1x-yOo5kavI4S-IZ9un9f-UoldIBEgjEbQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=0gauPAJaGxUgWWHsAAAAAA%3D%3D,22 days ago
Sr Engineer- MLOps,Target India,"Bengaluru, Karnataka, India","About us:

As a Fortune 50 company with more than 400,000 team members worldwide, Target is an iconic brand and one of America's leading retailers. Joining Target means promoting a culture of mutual care and respect and striving to make the most meaningful and positive impact. Becoming a Target team member means joining a community that values diverse voices and lifts each other up. Here, we believe your unique perspective is important, and you'll build relationships by being authentic and respectful.

Overview about Target In India

At Target, we have a timeless purpose and a proven strategy. And that hasn’t happened by accident. Some of the best minds from diverse backgrounds come together at Target to redefine retail in an inclusive learning environment that values people and delivers world-class outcomes. That winning formula is especially apparent in Bengaluru, where Target in India operates as a fully integrated part of Target’s global team and has more than 4,000 team members supporting the company’s global strategy and operations.

Pyramid overview

A role with Target Data Science & Engineering means the chance to help develop and manage state of the art predictive algorithms that use data at scale to automate and optimize decisions at scale. Whether you join our Statistics, Optimization or Machine Learning teams, you’ll be challenged to harness Target’s impressive data breadth to build the algorithms that power solutions our partners in in Marketing, Supply Chain Optimization, Network Security and Personalization rely on

About The role

As Senior Engineer, you will join a Target Tech team responsible for Promotion forecasting and optimization. You will play crucial role in designing, implementing, and optimizing the machine learning solutions in production. Additionally, you’ll apply best practices in software design, participate in code reviews, create a maintainable well-tested codebase with relevant documentation. At an organizational level, you will conduct training sessions, present work to technical and non-technical peers/leaders, build knowledge on business priorities/strategic goals and leverage this knowledge while building requirements and solutions for each business need. Core responsibilities of this job are articulated within this job description. Job duties may change at any time due to business needs.

About you:
• 4-year degree in Quantitative disciplines (Science, Technology, Engineering, Mathematics) or equivalent experience
• MS in Computer Science, Applied Mathematics, Statistics, Physics or equivalent work or industry experience
• 4 plus years of experience in end-to-end application development, data exploration, data pipelining, API design, optimization of model latency
• Expertise in MLOps frameworks and hands on experience in MLOps tools preferably Google Vertex ai
• 2 plus years of experience deploying Machine Learning algorithms into production environments - including model and system monitoring and troubleshooting
• Highly proficient programming in Scala and/ or Python/Pyspark
• Good understanding of Big Data tech - specifically Hadoop, Kafka, Spark
• Experience in handling streaming data and real-time forecasting solutions deployment
• Solid understanding of data analysis techniques, including data cleaning, preprocessing, and visualization
• Demonstrated ability collaborating with data scientists, software engineers and product managers to understand the business requirements and translate to machine learning solutions at scale
• Excellent communication skills with the ability to clearly tell data driven stories through appropriate visualizations, graphs, and narratives Self-driven and results oriented - able to meet tight timelines
• Motivated, team player with ability to collaborate effectively across global team
• Understanding of retail industry and pricing concepts is added advantage

Bonus Points:
• Extensive experience with Deep Learning frameworks TensorFlow, Pytorch or Keras
• PhD in Computer Science, Applied Mathematics, Statistics, Physics or related quantitative field
• Extensive experience developing highly distributed ML systems at scale
• Familiarity with Vertex AI and Cloud ML ecosystem will be desirable
• Experience in mentoring the junior team members ML skillset and career development

Useful Links-

Life at Target- https://india.target.com/

Benefits- https://india.target.com/life-at-target/workplace/benefits

Culture- https://india.target.com/life-at-target/diversity-and-inclusion",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=F0XIXIO2ulgX-eOEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yXJvQrCQAwAYFz7CE5Z1bYngotugoh_OOguaQ3X0zM5kiv0RXxfEZdv-YrPqJhcFbbsAxNpBefTJRlUcJAGjFDbDoRhJ-IjjdddzslWzpnF2lvGHNq6lbcTpkYG95TGftytQ6UUMdN9sZwPdWI_nd1QPWXY8yMgBIYNscfYa1_CEZUx4wvLf38BqxH8DZgAAAA&shmds=v1_AUFQtOOWLbmK7m26YlG-9tRkHiq3d1kCJbyV1wAjyYLdYQiIww&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=F0XIXIO2ulgX-eOEAAAAAA%3D%3D,N/A
data-engineer-13373,Visa,"Bengaluru, Karnataka, India",No job summary available.,https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Jp8mHI5V0K7CSFK3AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBAAUFz7CU4HbqVp1CKCbi6i7q7l2h5JNN6VXAr9Cr_Zurz1Fd9VUQ6Y0RC7wETJ7Jrm2ICBu3SghKn3IAxXERdpffY5j3qyVjXWTjPm0Ne9fKwwdTLbl3T6p1WPicaImdr9YTvXI7ty8wyKEBguS4ZxSlMFD0y89G-s4MZDwB84aWm_kQAAAA&shmds=v1_AUFQtOOX7Doi_-mkVB8pLZuh5Mr1EH-kRrJlGJh8ERoy20Z-mw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Jp8mHI5V0K7CSFK3AAAAAA%3D%3D,8 days ago
Data Engineer III,PDI Technologies,"Chennai, Tamil Nadu, India","At PDI Technologies, we empower some of the world's leading convenience retail and petroleum brands with cutting-edge technology solutions that drive growth and operational efficiency.

By “Connecting Convenience” across the globe, we empower businesses to increase productivity, make more informed decisions, and engage faster with customers through loyalty programs, shopper insights, and unmatched real-time market intelligence via mobile applications, such as GasBuddy. We’re a global team committed to excellence, collaboration, and driving real impact. Explore our opportunities and become part of a company that values diversity, integrity, and growth.

Role Overview

PDI is seeking a talented and motivated Full Time Data Engineer III to join our elite agile data services team responsible for developing and maintaining our industry-leading cloud-based big data and data analytics infrastructure serving major global fortune 500 companies.

The ideal candidate will have hands-on experience in coding data pipelines, administering databases, and working with business users to understand and meet their data requirements. This role involves maintaining high performance and security of our data systems, performing quality assurance, and supporting the company’s data infrastructure, primarily using AWS, Snowflake and DBT.

Key Responsibilities
• Design and manage complex data architectures.
• Lead the development and optimization of data pipelines and ETL processes.
• Mentor junior engineers and provide technical guidance.
• Collaborate with cross-functional teams to understand and meet data requirements.
• Ensure the reliability and performance of data systems.
• Conduct data validation and quality assurance.
• Document data workflows and technical specifications.
• Participate in agile development processes.
• Implement industry standards and best practices.
• Maintain data security and compliance.
• Provide on-call support as required.
• Estimate and plan data engineering projects.
• Develop strategies for data storage, processing, and archiving.
• Troubleshoot and resolve complex data issues.

Qualifications
• Advanced SQL skills and proficiency in multiple programming languages.
• Extensive experience with data warehousing, specifically Snowflake.
• Proficiency in DBT (Data Build Tool).
• Extensive experience in Cloud, such as AWS, GCP or Azure
• Strong problem-solving and project management skills.
• Excellent communication and leadership abilities.
• Bachelor’s or Master’s degree in Computer Science, Information Technology, or a related field.

Preferred Qualifications
• Certifications such as Snowflake SnowPro Core Certification, dbt Certification, AWS Certified Data Analytics are a plus.

Behavioral Competencies
• Ensures Accountability
• Manages Complexity
• Communicates Effectively
• Balances Stakeholders
• Collaborates Effectively

PDI is committed to offering a well-rounded benefits program, designed to support and care for you, and your family throughout your life and career. This includes a competitive salary, market-competitive benefits, and a quarterly perks program. We encourage a good work-life balance with ample time off [time away] and, where appropriate, hybrid working arrangements. Employees have access to continuous learning, professional certifications, and leadership development opportunities. Our global culture fosters diversity, inclusion, and values authenticity, trust, curiosity, and diversity of thought, ensuring a supportive environment for all.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=raxsCrd44OU8T-zHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43itREBBcdW5E4iEP3ck2PJJLelV6E_om_K77hVd9NtWuxINw4JCZawDkHB3jIAEq4-AjCcBcJmbbXWMqsF2tVswlasCRvvExWmAZZ7VsG_ddrxIXmjIX60_m4mpnD3rxaBx35yJIlJFJIDE0kZkw1dDilDE8cPzU4HhP-AImGkLOaAAAA&shmds=v1_AUFQtONASqh9PeEbATiAmcwSSYVfABA-8Ap6ltF64L2vLx46lg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=raxsCrd44OU8T-zHAAAAAA%3D%3D,N/A
Data Engineer,RPASoft,"Karnataka, India","Job Title

Data Engineer

Job Description

Responsibilities:
• Keep up to date with technology and best practices of Business Intelligence and Process Mining implementations.
• Implement the UiPath Process Mining and solutions at our customers.
• Make sure the right things are implemented. Have procedures for validating this.
• Define, measure, and maintain sufficient implementation quality.
• Second line of support for software and solutions support.
• Detect opportunities and risks in projects. Make sure those are addressed.
• Actively provide feedback to Engineering and Product Management.

Requirements:
• BE/B.Tech/MCA or other equivalent degree or other related fields
• Total experience of 3 to 6 years
• (2+ years) of experience in Relational Database, and related data structures and schemas
• Strong knowledge of Data structures and relational databases
• Excellent skills with SQL queries, procedures, and functions
• Passionate about evaluating complex data, with excellent analytical skills and knowledge about data warehouse schemas
• Excited about Data Mining and Process Mining
• Excellent verbal and written communication skills
• Proactive and strong problem-solving skills
• Ability to document requirements and specifications
• Experience with Agile and Scrum development methodologies
• Ability to learn quickly and work independently or as part of a team
• Knowledge in any of the data visualization tools (MS PBI, tableau) would be an added advantage
• Exposure to O2C & P2P domain would be an added advantage clients and global teams.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=W9WY-f-L4VEIF2pEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQ4BQRAA0GjvE2imUAm7ItFQSYigET7gMnvm9pY1c9mZ4kqfjua1r_qMqukeDeHAMTFRgQWcJYASlqYDYTiKxEzjbWfW68Z71eyiGlpqXCNvL0xBBv-UoH9q7bBQn9GoXq2Xg-s5zia36-4urUFiuGDhX_jCOZz4kfALscWhl4MAAAA&shmds=v1_AUFQtOMhUbWT8OE8BKxDDZdfqooI8zW6Z58C48sdnzyO_hufyg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=W9WY-f-L4VEIF2pEAAAAAA%3D%3D,N/A
Senior Data Analytics Engineer,PTSI Prudential Technology and Services India Private Limited,"Bengaluru, Karnataka, India","Prudential’s purpose is to be partners for every life and protectors for every future. Our purpose encourages everything we do by creating a culture in which diversity is celebrated and inclusion assured, for our people, customers, and partners. We provide a platform for our people to do their best work and make an impact to the business, and we support our people’s career ambitions. We pledge to make Prudential a place where you can Connect, Grow, and Succeed. Job Summary: We are seeking a highly skilled Data Engineer to join our global organization. The ideal candidate will have a minimum of 5 years of experience in data engineering, with a strong background in Spark, Python, Azure Databricks, SQL, and Scala. The role involves working within a global team, leveraging cloud platforms like Azure or GCP, and contributing expertise to multiple projects. Key Responsibilities: Develop and maintain scalable data pipelines using Spark, Python, and Scala within Azure Databricks or similar cloud-based environments. Collaborate with cross-functional teams across the globe to understand data requirements and implement efficient solutions. Utilize SQL and other querying languages to manipulate and extract data from various sources, ensuring accuracy and efficiency. Design, build, and optimize data models and architectures to support data processing and analytics needs. Ensure data quality, integrity, and security while adhering to best practices and compliance standards. Work closely with stakeholders to understand business needs and translate them into technical requirements for data solutions. Mentor and guide junior team members, fostering a collaborative and knowledge-sharing environment. Communicate effectively with global teams, providing updates, insights, and support as needed. Minimum Experience, Behaviors and Education Requirements: Bachelor's or master's degree in computer science, Engineering, or a related field. 5+ years of hands-on experience as a Data Engineer, preferably in a global organization. Proficiency in Spark, Python, Azure Databricks, SQL, and Scala. Experience working with cloud platforms such as Azure or GCP, leveraging their data services. Strong understanding of data modeling, ETL processes, and data warehousing concepts. Ability to work effectively in a global, multicultural team environment, across different time zones. Proven track record of delivering on multiple projects with a focus on scalability, reliability, and performance. Excellent problem-solving skills and a proactive approach to tackling complex data challenges. Knowledge of life insurance industry preferred. Prudential is an equal opportunity employer. We provide equality of opportunity of benefits for all who apply and who perform work for our organisation irrespective of sex, race, age, ethnic origin, educational, social and cultural background, marital status, pregnancy and maternity, religion or belief, disability or part-time / fixed-term work, or any other status protected by applicable law. We encourage the same standards from our recruitment and third-party suppliers taking into account the context of grade, job and location. We also allow for reasonable adjustments to support people with individual physical or mental health requirements. We are Prudential. For Every Life, For Every Future. Prudential plc provides life and health insurance and asset management to 18 million customers across 24 markets in Asia and Africa. We are headquartered in London and Hong Kong and are focused on four strategic regions: Greater China, ASEAN, India and Africa.  We are served by around 68,000 average monthly active agents and more than 200 bank partners. Prudential plc is not affiliated in any manner with Prudential Financial, Inc., a company whose principal place of business is in the United States of America or with the Prudential Assurance Company, a subsidiary of M&G plc, a company incorporated in the United Kingdom. Find out more at www.prudentialplc.com Follow us on LinkedIn",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=adyH5NeDaDAl2QmZAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWOMWsCQRBGSetPSPXVwdxJwCapIhFRUwhnGZC5vWFv4jpz7OyJ_s38olxI86r34M1-HmbLhlUs44MK4V0p3YsEx1qjKHPGM3bWwply6GGKjVlM_PjWlzL4a127pyp6oamqgl1qU27tVn9b6384eU-Zh0SFTy_Lxa0aND59HY7NFoc8dqxFKOHIoVdLFu8g7dBwvkpgx1Y7oUmU69TjUy5SuIMoVqyR0pjHOfaUdXo_0_xf_wWxNjCe1QAAAA&shmds=v1_AUFQtOOkHKDvsoprPPxLKSpTvt8LE1JWDiYlq1EeN0VJe7rv3g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=adyH5NeDaDAl2QmZAAAAAA%3D%3D,N/A
"Technical Program Manager, Data Analytics",Google,"Bengaluru, Karnataka, India","Technical Program Manager for Data Analytics at Google Cloud, leading complex projects and driving data-driven decision-making.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=LPrcumhkytya1rvKAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_yWNsQrCMBBAce0nON3iIrURwUUXFUFUBAf3co1HEk3vSi5C_SG_04rLmx7vFZ9RsbmR9RwsRrgmcQlbuCCjo1TCHjPCljG-c7AKMzhJA0qYrAdhOIi4SOO1z7nTlTGqsXKacZArK60RpkZ685BGf6jVY6IuYqZ6sZz3VcduOvlHIDDsiB3GV3qVcMbEw_uJJRz5HvALLdUCv6kAAAA&shmds=v1_AUFQtOPGq0Z1eB0Lpsvt8knUxw6td0nhvgf_fYU2zmOMSigiag&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=LPrcumhkytya1rvKAAAAAA%3D%3D,N/A
"Data Engineering - Platform | Bengaluru, India",Degreed,"Bengaluru, Karnataka, India","Degreed is the upskilling platform that fuels growth and innovation through lifelong learning. We bring together everything you need to learn and advance—LMSs, courses, videos, articles, projects, and real-world skill insights—and match you with opportunities that align with your skills, role, and goals.

For businesses, Degreed helps build a culture of learning that attracts, develops, and retains top talent—driving both individual and company success.

We believe learning is the key to unlocking opportunities. Our mission is to discover, empower, and celebrate the next generation of global expertise.

Join us in shaping the future of learning and workforce development.

Working at Degreed
At Degreed, we believe that learning is the key to unlocking opportunities. As the premier upskilling platform, we empower individuals to connect their skills and ambitions with meaningful growth opportunities. By integrating everything people use to learn—skill insights, courses, videos, articles, and projects—we create personalized pathways for career advancement. Our mission is to discover, empower, and celebrate the next generation of global expertise. Join us in shaping the future of learning and workforce development.
Your future team
Our Data Engineering team tackles complex data challenges at scale, building the foundation for data-driven innovation of the Degreed Platform. You will have the opportunity to:
• Develop Pipelines, Data Models and Data Solutions for Reporting and Analytics in close collaboration for Product Teams
• Architect platforms and Enable Data Science Teams
• Ensure high quality Data within the DE scape for all data producers and consumers
• Participate in securing and governing Data in the Organization

Responsibilities:

As a Data Engineer, you will play a crucial role in building and maintaining the data infrastructure that fuels product innovation. Your contributions will span the entire data lifecycle, including:
• Metric Definition and Instrumentation: Collaborate with product and engineering teams to define key performance indicators (KPIs) and implement robust logging and instrumentation strategies
• Data Acquisition and Ingestion: Design and implement scalable data ingestion pipelines from diverse sources
• Data Architecture and Modeling: Develop and maintain efficient and scalable data models (e.g., star schema, data vault) within data warehouses (Snowflake, Redshift, Clickhouse etc.,) and architecting and maintaining data lakes (Azure)
• Data Transformation and Processing: Implement complex data transformations using Python with DBT on multiple Data Sources and Targets
• Data Quality, Governance, and Enablement: Establish and enforce data quality standards, implement data governance policies, and develop data documentation to ensure data accuracy, consistency, and reliability. Enable self-service analytics by providing clear documentation and training to stakeholders.
• Alerting, Visualization, and Reporting: Develop automated alerting systems to monitor data quality and identify anomalies. Create interactive dashboards and reports using visualization tools like Gooddata
• Performance Optimization and Scalability: Continuously optimize data pipelines and infrastructure for performance, scalability, and cost-efficiency, leveraging cloud-native technologies and best practices.
• Collaboration and Leadership: Work autonomously and collaboratively within a cross-functional team, contributing technical expertise and driving data-driven decision-making. Lead technical projects, mentor junior team members (if applicable), and contribute to the team's technical roadmap.

Qualifications (Add specific qualifications based on the role's seniority):
• Bachelor's/Master's degree in Computer Science, Data Science, or a related field
• 7+ years of experience in data engineering
• Experience in Data Modelling for complex business models
• Technical hands on with Python and SQL (DBT and Snowflake or other comparable warehouse like Redshift/Bigquery/Clickhouse)
• Experience with one or more Clouds (AWS, Azure, GCP)
• Good experience with Data Lakes and best practices involved in Data Lake architecture
• Strong understanding of data governance principles and best practices.
• Excellent analytical, problem-solving, and communication skills.

Benefits

We take care of our people with a comprehensive benefits package designed to support your well-being, growth, and success.
View the full details here: https://px.sequoia.com/globalcompanybenefits

At Degreed, We Value

🌍 Diversity & Inclusion – We celebrate diverse perspectives and backgrounds, fostering an inclusive environment where everyone can thrive and contribute.

📈 Growth Mindset – Learning is at the heart of what we do. We empower our employees to continuously develop their skills and grow their careers in alignment with their unique strengths and aspirations.

🤝 Collaboration – The best ideas come from working together. We cultivate a culture of open communication, teamwork, and shared success.

By joining Degreed, you’ll be part of a community that values learning, collaboration, and meaningful impact. If you’re passionate about driving change through upskilling and workforce transformation, we encourage you to apply and contribute to our mission.

Work Environment & Physical Demands

Degreed offers flexible work arrangements tailored to each role. Some positions are fully remote, while others follow a hybrid model for employees near an office. Please check the job details for role-specific requirements.

For remote and hybrid roles, you’ll collaborate virtually using tools like Zoom and Slack. This role may require prolonged computer use and stationary work, with the ability to interpret written and verbal communication effectively.

We are committed to creating an inclusive and adaptable work environment that enables every team member to thrive and do their best work.

Additional Information

Degreed is an equal opportunity employer committed to fostering a workplace free from discrimination and harassment. We do not discriminate based on race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.

This policy applies to all aspects of employment, including recruitment, hiring, career development, compensation, and training.

💡 Accessibility & Accommodations

We are dedicated to full inclusion and will provide reasonable accommodations for applicants with disabilities throughout the hiring process. If you need assistance, please let us know.

🔍 Fair Hiring Practices

In compliance with the San Francisco Fair Chance Ordinance, we consider qualified applicants with arrest and conviction records.

📄 E-Verify Participation

Degreed participates in the E-Verify employment verification program.

Global Data Privacy Notice for Job Candidates & Applicants

If you are applying from certain locations, your data may be processed in accordance with regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA).

For more details on how we process applicant data, please review our Global Data Privacy Notice for:

https://explore.degreed.com/privacy/

https://degreed.zendesk.com/hc/en-us/articles/4409131149458-General-Data-Protection-Regulation-GDPR-Degreed-Update

By submitting your application, you acknowledge and agree to our use and processing of your data in compliance with applicable laws.

Fraudulent Recruitment Warning 🚨

Beware of fraudulent recruitment scams using the Degreed name. Scammers may impersonate our company, website, or hiring team.

⚠️ Degreed will never:

❌ Conduct recruitment via WhatsApp, Telegram, or direct-messaging platforms.

❌ Request sensitive personal or financial information in unsolicited communications.

❌ Offer jobs requiring upfront payments or promising unrealistic returns.

✅ Official Degreed communications will always come from a @degreed.com email address or phone number during the hiring process.

If you encounter suspicious activity, please report it immediately. Stay vigilant and protect yourself from fraud.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=2fEiqRyHcbie9LmEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_03OMQrCQBCFYWxzBKupLER3RbDRThJEbbxBmCTjZHUzE3Y3kMIjeUgjWNi85i--l71nWZFjQiiEnRAFJwxruHlMdw0dvOBIwuiHMKzgLI3DqV60gkgY6hZU4KTKnuaHNqU-7q2N0RuOCZOrTa2dVaFKR_vQKn6njC0G6ieAyu1uM5peeLnIiQNRA07-wSsGmc498Wd_ADMlKiKvAAAA&shmds=v1_AUFQtOM1nDnYTPSvpNYhJbahopluwUThnpdzmaPQIbUk1AN41w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=2fEiqRyHcbie9LmEAAAAAA%3D%3D,14 days ago
Software Engineer I - Data Engineer (BI),Uplight,"Maharashtra, India","The Position

We are seeking a seasoned engineer with a passion for changing the way millions of people save energy. You’ll work within the Engineering team to build and improve our platforms to deliver flexible and creative solutions to our utility partners and end users and help us achieve our ambitious goals for our business and the planet.

We are seeking a skilled and passionate Data Engineer - Business Intelligence with expertise in Data Engineering and BI Reporting to join our development team. As a Data Engineer, you will play a crucial role developing different components, harnessing the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, data processing and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. You will also work on creating BI reports as well as development of a Business Intelligence platform that will enable users to create reports and dashboards based on their requirements. You will coordinate with the rest of the team working on different layers of the infrastructure. Therefore, a commitment to collaborative problem solving, sophisticated design, and quality product is important.

You will own the development and its quality independently and be responsible for high quality deliverables. And you will work with a great team with excellent benefits.

Responsibilities & Skills

You should:
• Be excited to work with talented, committed people in a fast-paced environment.
• Have a proven experience as a Data Engineer with a focus on BI reporting..
• Be designing, building, and maintaining high performance solutions with reusable, and reliable code.
• Use a rigorous approach for product improvement and customer satisfaction.
• Love developing great software as a seasoned product engineer.
• Be ready, able, and willing to jump onto a call with stakeholders to help solve problems.
• Be able to deliver against several initiatives simultaneously.
• Have a strong eye for detail and quality of code.
• Have an agile mindset.
• Have strong problem-solving skills and attention to detail.

Required Skills (Data Engineer):
• You ideally have 2+ or more years of professional experience.
• Design, build, and maintain scalable data pipelines and ETL processes to support business analytics and reporting needs.
• Strong Experience with SQL for querying and transforming large datasets, and optimizing query performance in relational databases.
• Proficiency in Python for building and automating data pipelines, ETL processes, and data integration workflows.
• Familiarity with big data frameworks such as Apache Spark or PySpark for distributed data processing.
• Strong Understanding of data modeling principles for building scalable and efficient data architectures (e.g., star schema, snowflake schema).
• Good to have experience with Databricks for managing and processing large datasets, implementing Delta Lake, and leveraging its collaborative environment.
• Knowledge of Google Cloud Platform (GCP) services like BigQuery, Dataflow, Pub/Sub, and Cloud Storage for end-to-end data engineering solutions.
• Familiarity with version control systems such as Git and CI/CD pipelines for managing code and deploying workflows.
• Awareness of data governance and security best practices, including access control, data masking, and compliance with industry standards.
• Exposure to monitoring and logging tools like Datadog, Cloud Logging, or ELK stack for maintaining pipeline reliability.
• Ability to understand business requirements and translate them into technical requirements.
• Inclination to design solutions for complex data problems.
• Ability to deliver against several initiatives simultaneously as a multiplier.
• Demonstrable experience with writing unit and functional tests.

Required Skills (BI Reporting):
• Strong experience in developing Business Intelligence reports and dashboards via tools such as Tableau, PowerBI, Sigma etc.
• Ability to analyse and deeply understand the data, relate it to the business application and derive meaningful insights from the data.

The following experiences are not required, but you'll stand out from other applicants if you have any of the following, in our order of importance:
• You are an experienced developer - a minimum of 2+ years of professional experience.
• Work experience & strong proficiency in Python, SQL and BI Reporting and its associated frameworks (like Flask, FastAPI etc.).
• Experience with cloud infrastructure like AWS/GCP or other cloud service provider experience
• CI/CD experience
• You are a Git guru and revel in collaborative workflows
• You work on the command line confidently and are familiar with all the goodies that the linux toolkit can provide
• Familiarity with Apache Spark and PySpark.

Qualifications
• Bachelor's or Master's degree in Computer Science, Engineering, or a related field.

Uplight provides equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type without regard to race (including hair texture and hairstyles), color, religion (including head coverings), age, sex, national origin, caste, disability status, genetics, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=YoU2AE2qscVjCdbsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WNsQrCMBBAce0nCMKNKtqI4KKDIIpUcBLncm1jEol3IXdgf8Z_tU4ub3g8eMVnVOxv_NA3ZgsncoGszVDBEo6o-DfTQzUb5IUbEIu59cAEZ2YX7XjnVZNsjRGJpRNFDW3Z8ssw2YZ78-RGfqjFD5cUUW293qz6MpGbT-4pBucVAsEVhwDFa8YFVNQF_AL9Keh_oAAAAA&shmds=v1_AUFQtOO0ze7EDAMTdFBzQmqXkGGGJ52jmI6n6yMxC465EmyT_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=YoU2AE2qscVjCdbsAAAAAA%3D%3D,N/A
AI Data Engineer,NTT DATA North America,India,"Job Title: Data Science & AIML, GenAI Lead/Engineer

Key Responsibilities:
• Develop and implement traditional machine learning algorithms.
• Deploy at least one model in a production environment.
• Write and maintain Python code for data science and machine learning projects.

Preferred Qualifications:
• Knowledge of Deep Learning (DL) techniques.
• Experience working with Generative AI (GenAI) and Large Language Models (LLM).
• Exposure to Langchain.

#GenAINTT",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=89TZnLg_pL1gqFEfAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CS7eJgg2IrjoFKhIHTp1L9d4JCntXcnd0D_wt8U3vOq7q46-hQYN4ckxM1GBM7xlBCUsIYEwvETiTPtHMlv17pzqXEc1tBzqIIsTplE2N8mo_wZNWGid0Wi43i5bvXI8Hbq-h8b3HjoplsAvVHJAyAwtfzL-AJ_NTACKAAAA&shmds=v1_AUFQtONHUWzbGXDEnmaIwjSAKLf97eIb5ALnVHHsX435Nu5tXw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=89TZnLg_pL1gqFEfAAAAAA%3D%3D,29 days ago
Senior Lead Engineer - Investment Data Platform,M&G Global Services,"Mumbai, Maharashtra, India","We are M&G Global Services Private Limited (formerly known as 10FA India Private Limited, and prior to that Prudential Global Services Private Limited). We are a fully owned subsidiary of the M&G plc group of companies, operating as a Global Capability Centre providing a range of value adding services to the Group since 2003. At M&G our purpose is to give everyone real confidence to put their money to work. As an international savings and investments business with roots stretching back more than 170 years, we offer a range of financial products and services through Asset Management, Life and Wealth. All three operating segments work together to deliver attractive financial outcomes for our clients, and superior shareholder returns. M&G Global Services has rapidly transformed itself into a powerhouse of capability that is playing an important role in M&G plc’s ambition to be the best loved and most successful savings and investments company in the world. Our diversified service offerings extending from Digital Services (Digital Engineering, AI, Advanced Analytics, RPA, and BI & Insights), Business Transformation, Management Consulting & Strategy, Finance, Actuarial, Quants, Research, Information Technology, Customer Service, Risk & Compliance and Audit provide our people with exciting career growth opportunities. Through our behaviours of telling it like it is, owning it now, and moving it forward together with care and integrity; we are creating an exceptional place to work for exceptional talent. Job Description Job Title Senior Lead Engineer – Investment Data Platform Grade 2A Level Experienced colleague Job Function Asset Management Tech & Change Job Sub Function Investments Data Platform Reports to Lead/Sr. Manager – Investments Data Platform (India) Location Mumbai Business Area M&G Global Services Overall Job Purpose Working with M&G Plc. means becoming part of a brand with a global reputation and our purpose is to help people manage and grow their savings and investments, responsibly. M&G plc is a firm built on a rich and long history and with a commitment to an innovative future centred on the needs of customers and clients. There is a genuine opportunity to drive competitive advantage with value creation through the formation of this new organisation. This role is for an Sr. Lead – Data Engineer(Investment Data Platform). The Engineer role will encompass building high quality, scalable software that aligns to business outcomes. Working in a highly collaborative, agile environment, you will work in a cross functional team to refine user stories and subsequently implement the necessary functionality. You will be responsible for sufficiently unit testing and releasing your changes in a controlled manner. You will utilise critical thinking skills to solve business and technical problems. Individual with the right blend of software engineering skills, data exploration, and domain knowledge to help us expand our data services by building end to end solutions for our stakeholders. Accountabilities/Responsibilities Key accountabilities and responsibilities Contributing to the delivery of software that delivers a business outcome and is designed, implemented and tested to the team’s standards as per the SDLC for the Data Platform. Providing a quality service and product to customers and stakeholders, further developing skills built through significant practical experience or training. Understanding and following data engineering best practise. Offering peer review and challenge within the team, to ensure that the most appropriate solution is delivered. Demonstrate a strong sense of product ownership and commitment to build scalable / extensible / robust software/data platform. Working within established frameworks and procedures, with the freedom to interpret them to solve a range of problems. Delivering outcomes that are clearly defined, using discretion over how to achieve them. Contributing to team improvement discussions. Participating in the development of complete solutions to the end consumer, being part of a cross functional team comprising of requirements gathering, data modelling, data integration, software engineering, testing and release oversight. Building and maintaining strong relationships with key stakeholders across the business and other teams across Asset Management Technology & Change. Key Stakeholder Management Internal All M&G Plc Business Areas M&G Plc Support Groups External Partner(s)/Vendor(s) Knowledge, Skills, Experience & Educational Qualification Knowledge & Skills (Key): Demonstrate software engineering skills with experience in SQL, C# and the .Net Framework Working with data on Azure including SQL and no-sql DBs and Azure Data factory pipelines Working with some of the following programming and scripting languages: SQL, Python, Java, PowerShell, JavaScript. A good understanding of DevOps principles and experience in building CI/CD pipeline. Knowledge of software patterns and principles. Proactively manage own workload across multiple parallel initiatives. Data exploration and analytical skills, that enable you to solve new problems and understand existing software through investigation. Experience in dealing with large data sets and how to analyse them. Good interpersonal skills, with the ability to communicate clearly and effectively, both written and orally, within a project team. Knowledge & Skills (Desirable): Experience to financial markets & asset management processes and understand analysis into a wide variety of asset classes and associated analytics (e.g. Equity, Fixed Income, Private Assets etc.) Experience in using Databricks for data ingestion or transformation. Experience in Snowflake. Experience: 10+ years of total experience in software engineering. 5+ years of latest/recent experience in a data engineering role. Experience of contributing to the delivery of software in a team that utilises Azure PaaS technologies, security and delivery via Azure DevOps. Experience of API gateways such as Apigee or Azure API Management. Experience of delivering change using Agile methodologies. Foundational understanding of the components of Investment Data (Transactions, Positions, Instrument T&Cs, etc.) Educational Qualification: Graduate in any discipline M&G Behaviours relevant to all roles: Note: *We are in Hybrid working with min. three days’ work from office (subject to policy change) We have a diverse workforce and an inclusive culture at M&G Global Services, regardless of gender, ethnicity, age, sexual orientation, nationality, disability or long term condition, we are looking to attract, promote and retain exceptional people. We also welcome those who take part in military service and those returning from career breaks. M&G plc is a leading international savings and investments business, managing money for around 4.6 million individual clients and more than 900 institutional clients in 38 offices worldwide. As at 31 December 2023, we had £343.5 billion of assets under management and administration. Our purpose is to give everyone real confidence to put their money to work. With a heritage dating back more than 175 years, M&G plc has a long history of innovation in savings and investments, combining asset management and insurance expertise to offer a wide range of solutions. Our three distinct operating segments, Asset Management, Life and Wealth, work together to provide access to balanced, long-term investment and savings solutions.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=2WLR-BWiRsb2FeOmAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOMQrCQBBFsc0RrKaykJioYKOtGhQDggeQ2WRMVjYzYWcMOZrHMza_ebzHT76z5Pwg9hLhRljDiRvPRBFWcOGB1DpigyMawj2gvSR2E7qKAyWMVQvCUIg0geaH1qzXfZ6rhqxRQ_NVVkmXC5OTMX-L0_88tcVI_VSj53a3HrOem-WmXBRQBHEY4EFx8BUpeIby0zn0KZQ4SaitRUynZ7XHH5dMqfC7AAAA&shmds=v1_AUFQtOOxQ5BQMdue7sRKoAFPtWA2C_o5AwiFuw6QRosHhDTh1A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=2WLR-BWiRsb2FeOmAAAAAA%3D%3D,12 days ago
Data Engineering,Hitachi Energy,"Chennai, Tamil Nadu, India","Business Information: The Hitachi Energy Indian Operations Center (INOPC) is a competence center with around 2600+ skilled engineers who focus on tendering, engineering, planning, procurement, functional system testing, installation supervision and commissioning. However, over the last decade, it has evolved to become the largest engineering hub. The India Operations Centre is a key aspect of the Power Grids business Power Up transformation program. The INOPC team at Chennai supports Hitachi Energy’s units in more than 40 countries across a wide portfolio of all the four business units in Power Grid business. To date, the team has executed engineering and commissioning for projects in more than 80 countries. Mission Statement: As a Data Engineer you will be part of Operation Center, India (INOPC-PG), aiming to develop a global value chain, where key business activities, resources and expertise are shared across geographic boundaries in order to optimize value for Hitachi Energy customers across markets. It provides high quality engineering and commissioning support to Business Units and Center of Excellence across the Hitachi Energy world. This is an important step from Hitachi Energy's Global Foot Print strategy.PG Operation Center Services are System design, Primary side design, Secondary side design, Sourcing, Installation and commissioning. Your Responsibilities: Display technical expertise in AI data analytics focusing to a team of diversified technical competencies. Build and maintain accurate and scalable data pipeline and infrastructure such as SQL Warehouse, Data Lakes etc. using Cloud platforms (e.g.: MS Azure, Databricks) Proactively work with business stakeholders to understand data lineage, definitions and methods of data extraction. Write production grade SQL and PySpark code to create the data architecture. Consolidate SQL databases from multiple sources, data cleaning and manipulation in preparation for analytics and machine learning. Use data visualization tools such as Power BI to create professional quality dashboards and reports. Write good quality documentation for data processing for different projects to ensure reproducibility. Living Hitachi Energy’s core values of safety and integrity, which means taking responsibility for your own actions while caring for your colleagues and the business. Your background: BE / BS in Computer Science, Data Science, or related discipline and at least 5 years of related working experience 5 years of data engineering experience, with understanding of lakehouse architecture, data integration framework, ETL/ELT pipeline, orchestration/monitoring, star schema data modeling. 5 years of experience with Python/PySpark and SQL 2 years of experience in Microsoft Power BI, 3 years of experience in utilizing Azure-based environment such as Databricks, ADLS, and Azure Machine Learning for data engineering purposes. Basic understanding of Machine Learning algorithms Ability to quickly grasp concepts from a field that is not one's core competency; A fast learning generalist capable of solving problem independently and resourceful in matrixed corporate environment. Proficiency in both spoken & written English language is required.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=BzhAhurvvydMySQxAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNvQrCMBAAYFz7CE63CVIbEVx0VPFncHIv1_RITtK7kotQ38MHVpdv_arPrFocsSCcJLAQZZYAK7hpB0aYfQQVOKuGRPN9LGW0nXNmqQlWsLBvvA5OhTqd3FM7-9NaxExjwkLtZruemlHCsr5wQR_5F1EOb2CBQyQR5BoeOHCCO_avGq7SM34BYl3A15cAAAA&shmds=v1_AUFQtOMFxtVbk56PULe1yvX6xa0dZsaXw67KyiBV8cBjhHXPxQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=BzhAhurvvydMySQxAAAAAA%3D%3D,N/A
Altimetrik - Cloud Data Engineer - AWS/Python,Altimetrik India,"Hyderabad, Telangana, India","Location : Job Type : Full-Time

Skills
• Primary : Databricks, Python, AWS.
• Secondary : Snowflake, MongoDB.

Job Description
• The Candidate must have 4 Year of relevant of experience in Databricks, AWS ,Python.
• Hands on experience on AWS Cloud platform especially S3 Glue Lamda.
• Experience on spark scripting.
• Has working knowledge on migrating relational and dimensional databases on AWS Cloud platform.
• Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.
• Strong experience with relational databases and data access methods especially SQL.
• Knowledge of Amazon AWS architecture and design.

Key Responsibilities
• Design and develop data pipelines using Databricks, Spark, and Python.
• Extract, transform, and load data from various sources (e.g., databases, APIs, files) into data warehouses and data lakes.
• Leverage AWS services (S3, Glue, Lambda) to build cloud-native data solutions.
• Monitor and optimize cloud infrastructure costs.
• Design and implement data models to support business analytics and reporting.
• Perform data analysis and visualization using SQL, Python, and visualization tools.
• Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions
• Adhere to best practices for data engineering, including data quality, security, and performance.
• Stay up-to-date with the latest trends and technologies in the data engineering field.

Minimum Qualifications
• Bachelors degree or equivalent training with data tools techniques and manipulation.
• Four years of data engineering or equivalent experience.

(ref:hirist.tech)",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=D7jOkvVEA5eENXMjAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_02OuwrCQBBFsc0nWE0tMSuCjVZixEclKFiGSTJsVjczYXeE5L_8QCM2NgfOuc1N3pMk33p1LWlwT5jDzsurhhwVYc_WMVEY6_Z-NZdBG-FRzlJCJAxVA6MfRKyn6aZR7eLamBh9ZqOiuiqrpDXCVEpvHlLGL4rYYKDOo1KxXC36rGM7M38XTlw7BMdwHGoKWGKdwo08skXG9Dd_AE_osh-3AAAA&shmds=v1_AUFQtOMIZ3p7R-D7Zf7H3gjV5JvQ4N-hlslS_AOfnxqg2rW-aA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=D7jOkvVEA5eENXMjAAAAAA%3D%3D,N/A
"Engineer, Data Engineering",Azira,"Karnataka, India","Description

You will be joining Azira, a global Consumer Insights platform, helping marketing and operational leaders improve their effectiveness with actionable intelligence to drive business results. Its mission is to create a more relevant world where brands are empowered to reach and build relationships with their consumers, but also give them the freedom and the tools to do so.

This role provides an opportunity to be a part of the Engineering team at Azira. You will get exposure to work on a huge scale of data, and cutting-edge tech stack, and leverage your skills to help us build a high-value and scalable product. You will be responsible for developing techniques to enhance data, You will need to collaborate with Data Scientists, SW Engineers, and UI Engineers and work as a part of a high-performance team and solve problems.

A Day in the Life
• Design and implement our data processing pipelines for different kinds of data sources, formats, and content for the Azira Platform. Working with huge Data Lakes, Data Warehouse and Data Marts are part of this challenging role.
• Design and develop solutions that are scalable, generic, and reusable.
• Responsible for collecting, storing, processing, and analyzing huge sets of data we receive from different sources.
• Develop techniques to analyze and enhance both structured/unstructured data and work with big data tools and frameworks.
• Collaborate closely with Data Scientists and Business Analysts to understand data and functional requirements.
• Design, build, and support existing data pipelines to standardize, clean, and ingest data.
• Participate in product design and development activities supporting Aziras suite of products.
• Liaise with various stakeholders across teams to understand business requirements.

What You Bring to the Role
• Should hold a Bachelors/masters degree in computer science or a related field.
• Must have 2-4 years of experience with at least 1 year of experience in a data-driven company/platform.
• Prior experience with distributed data processing frameworks such as Apache Spark, Apache Flink, or Hadoop is a must.
• Strong Proficiency in Java & Spark is essential, with additional experience in Scala being advantageous.
• Demonstrated teamwork is crucial, along with a flexible mindset in approaching problem-solving using the right tools and technologies, collaboratively with the team.
• Proficiency in frameworks, and distributed systems, strong algorithmic skills, and knowledge of design patterns are expected.
• An in-depth understanding of big data technologies and NoSQL databases (e.g., Kafka, HBase, Spark, Cassandra, MongoDB) is necessary.
• Additional experience with the AWS cloud platform, Spring Boot, and API development is a valuable plus.
• Exceptional problem-solving and analytical abilities, coupled with organizational skills and meticulous attention to detail, are essential traits.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=AnuJz1DSiwP2i1HMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_zXNMQoCMRBAUWz3BlpNLWuigo1WgiLqIZZJHJJonAmZFItX8NJqYfPhVb97T7rVkUNiotrDARvCn4kDLOAiDpSw-gjCcBIJmWa72FrRrbWq2QRt2JI3Xp5WmJyM9i5Ofxk0YqWSsdGw3ixHUzjMp_tXqgiJ4YqVv8cH9nDmW8IPRecnao4AAAA&shmds=v1_AUFQtOPZ3MM430ymw0H1daaqrJgZihnxrNyWpgXVjzsdRKXV0w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=AnuJz1DSiwP2i1HMAAAAAA%3D%3D,2 days ago
Data Engineer - Engg,GEP Worldwide,"Secunderabad, Telangana, India","What you will do
• Design, develop, and maintain robust data pipelines and ETL processes to support analytics and reporting for Generative AI and LLM-based solutions.
• Collaborate with cross-functional teams to integrate AI-driven insights into business processes, ensuring the delivery of high-quality, data-driven reports.
• Leverage Databricks and other cloud-based platforms to optimize data workflows, ensuring scalability and performance in handling large datasets.
• Implement best practices in data management, including data governance, security, and compliance, within the context of Generative AI and LLM platforms.
• Stay updated with the latest advancements in Generative AI, LLMs, and related technologies, and apply this knowledge to improve the efficiency and effectiveness of analytics projects.

What you should bring
• Proven experience in data engineering, with a focus on analytics and reporting in a SaaS environment.
• Hands-on experience with Databricks and other big data technologies.
• Strong understanding of Generative AI, LLMs, and their applications in data-driven projects.
• Proficiency in SQL, Python, and other programming languages used in data engineering and analytics.
• Excellent problem-solving skills, with a keen eye for optimizing data workflows and processes.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=NZj7yJmGbHCx3H1bAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXGsQrCMBCAYVz7CE43a21EcNHVUnQSFBzLJTnSSLwruYh9F19Wu_zfX30X1fqEBaHlEJkow2be8OciFpQwuwGEoRMJiZbHoZRRD8aopiZowRJd4-RlhMnKZJ5idU6vA2YaExbqd_vt1IwcVqZrr_CQnPwneoLIcCP3Zk8ZLfoa7pSQAzLWcGYf8Qec9rZ0ngAAAA&shmds=v1_AUFQtONJWlCN_fecgL8AK6G83xmj1f_AbCdNo7HPjSzVJmwk-A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=NZj7yJmGbHCx3H1bAAAAAA%3D%3D,N/A
Statistical Analytics Engineer (F/M/D),Freudenberg Group,"Bengaluru, Karnataka, India","Working at Freudenberg: We will wow your world!

Responsibilities:
• Performs projects in statistics, data science and artificial intelligence in the technical field of KL under the guidance of experts, evaluates and comments on results
• Implements data processes and data products into operations and works on their operation and optimization
• Coordinates regularly with experts in statistics, data science and artificial intelligence at KL
• Maintains a balance between effort (time, cost) and expected benefit of incoming requests
• Contributes ideas for KL-relevant developments and methods, especially in the field of data analysis and artificial intelligence, and implements them as needed
• Supports the improvement and automation of processes/workflows in own & adjacent work areas through statistical methods and evaluations
• Works on cross-functional project teams and projects with external partners
• Keeps own expertise up to date (self-study, external training, meetings, specialist groups)
• Supports technical colleagues in the introduction, use and quality assurance of data analysis tools
• Presents contributions to statistics and data science in the context of Klüber internal and customer training
• Strictly observes the current requirements for know-how protection

Qualifications:
• Engineering Graduate /Masters in Chemistry, Physics or equivalent degree
• Overall 7 -10 years of experience in data modelling, analysis and visualization in manufacturing or industrial environment.
• Completed higher education in science, focus on chemistry, physics, mathematics or related discipline
• Strong experience in methods for processing, evaluating and modeling data
• In depth programming knowledge with a focus on data science and data engineering, preferably in Python
• Experience using cheminformatics, materials informatics, generative AI and machine learning
• Experience in developing interactive data applications with frameworks like Shiny, Dash or streamlit
• good knowledge of relational data (SQL), experience in tools like JupyterLab, Gitlab, Airflow, MLFlow, knowledge of chemical analysis and organic chemistry

The Freudenberg Group is an equal opportunity employer that is committed to diversity and inclusion. Employment opportunities are available to all applicants and associates without regard to race, color, religion, creed, gender (including pregnancy, childbirth, breastfeeding, or related medical conditions), gender identity or expression, national origin, ancestry, age, mental or physical disability, genetic information, marital status, familial status, sexual orientation, protected military or veteran status, or any other characteristic protected by applicable law.

Klüber Lubrication India Pvt. Ltd.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=HgCbXO7ZaBnOdrVPAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMMQrCQBBAUWxzBKspVWI2CDaKhaIJKlYeIEySYbO6zoSdDcRLeUa1-bzqJ59JsrtHjE6ja9DDntG_f1Q4sXVMFGBWmJs5zmEJF6lBCUPTgTCUItbTdNvF2OvGGFWfWf2_mqyRlxGmWkbzkFr_qbTDQL3HSNVqnY9Zz3aRF4GGlrimYKEMMvTgGA7EFv0QhhSuGBgjPjGFM7cOv3UVbFmxAAAA&shmds=v1_AUFQtONVFVeiK7A_OJy4G9NxkQVEHM6N1Ypu5qI4gn_4JXDbxA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=HgCbXO7ZaBnOdrVPAAAAAA%3D%3D,9 days ago
Data Engineer - AWS Services,SheThink Pvt. Ltd.,"Kolkata, West Bengal, India","Role: AWS Data Engineer.

Good To Have Skills: AWS Certifications.

Must Have Skills: Client Management, Pyspark, Glue, EMR Cluster, Lambda, DMS, ETL, AWS.

Experience Level: 8-12 Years.

Employee Type: Salaried.

The Data platform associate will work in a variety of settings to build the data systems including design, implementation, ongoing maintenance, and management of the businesses data assets to assist the business in achieving its goals regarding reporting and mining. These data systems are responsible for collecting and converting raw data into usable information for data scientists, business analysts, and other stakeholders in the organization responsible for managing end-to-end data.

Additional responsibilities include collaborating with Architects and DBAs in architecture, development, and maintenance of all system databases.

Their goal is to make data accessible and easy so that organizations can use it to evaluate and optimize decision-making.
Responsibilities:
• Assist in the design and implementation of Snowflake-based analytics solution (data lake and data warehouse) on AWS.
• Requirements definition, source data analysis and profiling, logical and physical design of the data lake and data warehouse as well as the design of data integration and publication pipelines.
• Develop Snowflake deployment and usage best practices.
• Build and maintain data pipelines adhering to suggested enterprise architecture principles and guidelines.
• Design, build, test, and maintain data management systems.
• Work in sync with internal and external team members like data architects, data scientists, and data analysts to handle all sorts of technical issues.
• Work in Agile/Lean model.
• Deliver quality deliverables on time.
• Troubleshoot and resolve pipeline and infrastructure issues promptly.
• Able to understand and translate complex functional requirements into technical solutions with the help of product managers/business analysts.
Required Skills:
• 6-9 years of experience in Data Engineering.
• Creation and maintenance of optimum data pipeline architecture for ingestion and processing of data.
• Creation of necessary infrastructure for ETL jobs from a wide range of data sources using S3, Snowflake, and AWS data services.
• Experience in data storage technologies like Amazon S3, SQL, NoSQL.
• Hands-on experience in writing complex SQL.
• Hands-on experience in developing pipelines using AWS data services like Glue and Lambda.
• Develop, manage, and optimize data pipelines using AWS Glue and PySpark.
• Hands-on experience in optimizing AWS services for cost-effectiveness and performance.
• Data modeling technical awareness.
• Experience in working with stakeholders in different time zones.
Good to Have:
• Working knowledge of using Big Data technologies.
• Experience in collaborating with data quality and data governance teams.
• Payments domain knowledge.
• CRM, Accounting, etc. in-depth understanding.
• Regulatory reporting exposure.
Other Skills:
• Good communication skills.
• Team player.
• Problem solver.
• Willing to learn new technologies, share ideas, and assist other team members as needed.
• Strong analytical and problem-solving skills; ability to define problems, collect data, establish facts, and draw conclusions.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=MlKO86KY5T2YVolEAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOuwrCQBBAUWzzCVZTS9z4wEYrRREfhRAhpWySYXfNOhMyQ8gP-Z_G5sLpbvKdJOujVQsncoEQO5jDvsghx64PFcrIK5cgaLvKAxOcmV3E6c6rtrLNMpFonKjVUJmKPxkTljxkby7ln5d422EbreJrtVkMpiU3W-Yenz5QA49eDdy1NhAIbhybcSWFAkXhgORsTOFCdbA_qxocIqgAAAA&shmds=v1_AUFQtOMn7y4X6uFD5NlikOOYecGhMZXLTSW0U79UizO6wmUvYg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=MlKO86KY5T2YVolEAAAAAA%3D%3D,3 days ago
Snowflake Developer/Snowflake Data Engineer(8-13),Covenant Consultants,"Kolkata, West Bengal, India","Required skills - Snowflake ,Data warehouse,SQL

Role Overview:

As a Snowflake Data Engineer , you will be instrumental in building and maintaining advanced data infrastructure to support large-scale analytics projects. You will work alongside a team of data scientists, analysts, and business stakeholders to deliver optimized data solutions on the Snowflake platform. This role is ideal for a data engineer who thrives in a fast-paced environment and is passionate about leveraging Snowflake to unlock insights.

Key Responsibilities:
• Snowflake Data Modeling: Design and implement scalable Snowflake data models, optimized for data ingestion and analytics requirements.
• ETL Pipeline Development: Build and maintain robust ETL pipelines to integrate data from multiple sources into Snowflake, ensuring data integrity and consistency.
• Performance Optimization: Optimize Snowflake usage and storage, tuning query performance and managing data partitions to ensure quick, reliable access to data.
• Data Security & Governance: Implement best practices in data security, role-based access control, and data masking within Snowflake to maintain compliance and data governance standards.
• Automation & Workflow Management: Utilize tools such as dbt and Apache Airflow to schedule data processing and automate pipeline monitoring.
• Collaboration & Troubleshooting: Partner with data scientists, business analysts, and other stakeholders to address complex data challenges and troubleshoot Snowflake-related issues effectively.
• Documentation & Reporting: Develop comprehensive documentation for data structures, ETL workflows, and system processes to ensure transparency and knowledge sharing within the team.

Qualifications:
• Education: Bachelors degree in Computer Science, Information Systems, or a related field.
• Experience:
• 3+ years of experience in data engineering, with a specific focus on Snowflake or other cloud-based data warehousing technologies.
• Hands-on experience in building and managing Snowflake data warehouses.
• Technical Skills:
• Proficiency in Snowflake SQL and Snowflake features (Snowpipe, Data Sharing, etc.).
• Strong SQL and experience in designing efficient data models.
• Familiarity with ETL/ELT tools such as dbt, Apache Airflow, or Talend.
• Working knowledge of Python or other scripting languages for automation.
• Experience with cloud platforms like AWS, Azure, or GCP, particularly in data integration with Snowflake.
• Knowledge of data governance, security best practices, and compliance requirements.
• Preferred Skills:
• Snowflake certifications, such as SnowPro Core.
• Experience in additional analytics or BI tools like Tableau, Looker, or Power BI.

Soft Skills:
• Analytical thinker with strong problem-solving skills.
• Clear communication and ability to work collaboratively with cross-functional teams.
• Detail-oriented and committed to data accuracy and quality.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=eGKlMWMSqjYtocqkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_02OOQ7CQAwARcsTqFwCSrJAhISg4xBXSUEZOcFsAosdxeZ4HI8DOprRaKppv1vtzYHleQ54JVjSg4LU1Li_hoawYl8xUdOdxMO0BzHsJAclbIoShGEt4gN1ZqVZrVPnVEPi1dCqIink5oQpl5e7SK4_ZFpiQ3VAo2w0HrySmn0_XciDGNlgIaz3YF9VqBj2Eq7fhwiOpAZzYo8hgi2fKvwApwULh78AAAA&shmds=v1_AUFQtOOVl5WmBDdP-Lt4-ha6-R0e0x53KO4dlo7a3Sn6YJ3ofA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=eGKlMWMSqjYtocqkAAAAAA%3D%3D,N/A
"Data Engineer , Amazon Music DISCO",ADCI - Karnataka,"Bengaluru, Karnataka, India","- 3+ years of data engineering experience

Amazon Music is an immersive audio entertainment service that deepens connections between fans, artists, and creators. From personalized music playlists to exclusive podcasts, concert livestreams to artist merch, Amazon Music is innovating at some of the most exciting intersections of music and culture. We offer experiences that serve all listeners with our different tiers of service: Prime members get access to all the music in shuffle mode, and top ad-free podcasts, included with their membership; customers can upgrade to Amazon Music Unlimited for unlimited, on-demand access to 100 million songs, including millions in HD, Ultra HD, and spatial audio; and anyone can listen for free by downloading the Amazon Music app or via Alexa-enabled devices. Join us for the opportunity to influence how Amazon Music engages fans, artists, and creators on a global scale.
Key job responsibilities
- Engage in collaborative efforts with cross-functional teams, including data scientists and business intelligence engineers, to architect a state-of-the-art data analytics platform on AWS, employing the AWS Cloud Development Kit (CDK).
- Construct resilient and scalable data pipelines using SQL/PySpark/Airflow to effectively ingest, process, and transform substantial data volumes from diverse sources into a structured format, ensuring data quality and integrity.
- Devise and implement an efficient, scalable data warehousing solution on AWS, utilizing appropriate NoSQL/SQL storage and database technologies for both structured and unstructured data.
- Automate ETL/ELT processes to streamline data integration from diverse sources, enhancing the platform's reliability and efficiency.
- Develop data models to support business intelligence, delivering actionable insights and interactive reports to end-users.
- Enable advanced analytics and machine learning capabilities within the platform, extracting predictive and prescriptive insights through tools like EMR/SageMaker Notebooks.
- Continuously monitor and optimize the performance of data pipelines, databases, and applications, ensuring low-latency data access for analytics and machine learning tasks.
- Implement robust security measures and ensure data compliance with internal requirements, industry standards, and regulations to safeguard sensitive information.
- Collaborate closely with data scientists and business intelligence engineers to comprehend their requirements and work together on data-related projects.
- Generate comprehensive technical documentation covering the platform's architecture, data models, and APIs, promoting knowledge sharing and ease of maintainability.
About the team
Embark on an exhilarating journey with the DISCO team at Amazon Music! We're seeking a dynamic Data Engineer to be a vital part of propelling our success story. In this role, you'll be instrumental in crafting extraordinary Analytics & Science infrastructure for DISCO teams. Join us in championing a culture of inclusivity and a data-driven mindset, where every team member at Amazon Music is empowered to make informed decisions and measure their impact. Be the driving force behind delivering top-notch, accessible data and democratizing data access through user-friendly self-service tools. Join us on this exciting quest to redefine how we approach insights and decision-making for all!
• Master's degree

Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=G8-_-wIb2MXUhZKTAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOMQrCQBBFsc0RrKaWmBXBQq1iIhJFLDxAmKzDZnUzE3Y3ELyRtzRWNr94Dx4_-cySbYkR4cjGMpGHFPIO38JwHYLVUFb34gZLOEsDgdDrFiZ3EjGO5vs2xj7slArBZSZEjFZnWjolTI2M6ilN-E0dWvTUO4xUrzerMevZLFReFtVUvqDn6cELwTIciA26wQ_pn6dQ8cPiF1Qkv1OsAAAA&shmds=v1_AUFQtOM3whYY8A_Wr46C1hfHzFSArQN7GgAuY55CwK3xIVvAXQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=G8-_-wIb2MXUhZKTAAAAAA%3D%3D,27 days ago
Data Engineer III,Zinnia,"Pune, Maharashtra, India","WHO WE ARE:

Zinnia is the leading technology platform for accelerating life and annuities growth. With innovative enterprise solutions and data insights, Zinnia simplifies the experience of buying, selling, and administering insurance products. All of which enables more people to protect their financial futures. Our success is driven by a commitment to three core values: be bold, team up, deliver value – and that we do. Zinnia has over $180 billion in assets under administration, serves 100+ carrier clients, 2500 distributors and partners, and over 2 million policyholders.

WHO YOU ARE:

As a seasoned Data Engineer specializing in data engineering, you bring extensive expertise in optimizing data workflows using various database tools like Oracle, BigQuery, and SQL Server. You possess a deep understanding of ELT/ETL processes, data integration, and have a strong command of Python for data manipulation and automation tasks. You will possess advanced expertise in working with data platforms like Google Big Query, DBT, Python, and Airflow. Responsible for designing and maintaining scalable ETL pipelines, optimizing complex data systems, and ensuring smooth data flow across different platforms. As a Senior Data Engineer, you will also be required to work collaboratively in a team and contribute to building data infrastructure that drives business insights

WHAT YOU'LL DO:
• Design, develop, and optimize complex ETL pipelines that integrate large data sets from various sources.
• Build and maintain high-performance data models using Google BigQuery and DBT for data transformation.
• Develop Python scripts for data ingestion, transformation, and automation.
• Implement and manage data workflows using Apache Airflow for scheduling and orchestration.
• Collaborate with data scientists, analysts, and other stakeholders to ensure data availability, reliability, and performance.
• Troubleshoot and optimize data systems, identifying issues and resolving them proactively.
• Work on cloud-based platforms, particularly AWS, to leverage scalability and storage options for data pipelines.
• Ensure data integrity, consistency, and security across systems.
• Take ownership of end-to-end data engineering tasks while mentoring junior team members.
• Continuously improve processes and technologies for more efficient data processing and delivery.
• Act as a key contributor to developing and supporting complex data architectures.

WHAT YOU'LL NEED:
• Bachelor's degree in computer science, Information Technology, or a related field.
• 6+ years of hands-on experience in Data Engineering or related fields, with a strong background in building and optimizing data pipelines
• Strong proficiency in Google Big Query, including designing and optimizing queries.
• Advanced knowledge of DBT for data transformation and model management.
• Proficiency in Python for data engineering tasks, including scripting, data manipulation, and automation.
• Solid experience with Apache Airflow for workflow orchestration and task automation.
• Extensive experience in building and maintaining ETL pipelines.
• Familiarity with cloud platforms, particularly AWS (Amazon Web Services), including tools like S3, Lambda, Redshift, or Glue.
• Java knowledge is a plus.
• Excellent problem-solving and troubleshooting abilities.
• Strong communication and collaboration skills with the ability to work effectively in a team environment.
• Self-motivated, detail-oriented, and able to work with minimal supervision.
• Ability to manage multiple priorities and deadlines in a fast-paced environment.
• Experience with other cloud platforms (e.g., GCP, Azure) is a plus.
• Knowledge of data warehousing best practices and architecture.

WHAT'S IN IT FOR YOU?

At Zinnia, you collaborate with smart, creative professionals who are dedicated to delivering cutting-edge technologies, deeper data insights, and enhanced services to transform how insurance is done. Visit our website at www.zinnia.com for more information. Apply by completing the online application on the careers section of our website. We are an Equal Opportunity employer committed to a diverse workforce. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability

#LI-SC1",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=hpBkorVmowKdyip5AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7CU6Hk0htRHDRVZEIgrNLucQjidS7kjuh_-BPq8ubXvOZNcsjGsKJU2GiCt57WMNFAihhjRmE4SySBpofstmoe-dUhy6poZXYRXk5YQoyuacE_dNrxkrjgEb9dreZupHTanEvzAWhMNzeTC1c8bdQs1VswfOj4BfinTGkjgAAAA&shmds=v1_AUFQtOOji2OSo8RdAbTZI54Fp0h-ZAFu4NauRm_K8V3YRHAiqQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=hpBkorVmowKdyip5AAAAAA%3D%3D,16 days ago
Consultant : Data Engineer,Confidential,"Chennai, Tamil Nadu, India","Our client, a leading chain of hospitals, is looking for independent consultants who can deliver the following responsibilities:
- Analyze and organize raw data
- Build data systems and pipelines
- Evaluate business needs and objectives
- Interpret trends and patterns
- Conduct complex data analysis and report on results
- Prepare data for prescriptive and predictive modeling
- Build algorithms and prototypes
- Combine raw information from different sources
- Explore ways to enhance data quality and reliability
- Identify opportunities for data acquisition
- Develop analytical tools and programs
- Collaborate with data scientists and architects on several projects",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=519gXhapFEYPLPMtAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNMQ6CUAwA0LhyBKfOBvlq4oIjGqODkzspUPk1n5bQknAYD6sub33ZZ5XtKxWbk6M4lHBGR7hIz0I0wRbu2oARTm0EFbiq9onWp-g-WhmCWSp6c3Rui1aHoEKNLuGtjf2pLeJEY0Kn-nDcLcUo_Wbz-17ckThjAhaoIokg5_DEgRM8sJtzuEnH-AVA9AkKnwAAAA&shmds=v1_AUFQtOM3mOkAAo-fnPT6DA5RsWisvgzDfGtZAVnGDmfWmaHS9Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=519gXhapFEYPLPMtAAAAAA%3D%3D,N/A
Lead Data Engineer(Snowflake/DBT),Careers at Tide,"Delhi, India","ABOUT TIDEAt Tide, we are building a finance platform designed to save small businesses time and money. We provide our members with business accounts and related banking services, but also a comprehensive set of connected administrative solutions from invoicing to accounting.

Launched in 2017, Tide is now used by over 1 million small businesses across the world and is available to UK, Indian and German SMEs. Headquartered in central London, with offices in Sofia, Hyderabad, Delhi, Berlin and Belgrade, Tide employs over 2000 employees.

Tide is rapidly growing, expanding into new markets and always looking for passionate and driven people. Join us in our mission to empower small businesses and help them save time and money.
ABOUT THE ROLE:

As part of the team, you will be responsible for building and running the data pipelines and services that are required to support business functions/reports/dashboard..

We are heavily dependent on BigQuery/Snowflake, Airflow, Stitch/Fivetran, dbt , Tableau/Looker for our business intelligence and embrace AWS with some GCP.

As a Data Engineer you’ll be:

●Developing end to end ETL/ELT Pipeline working with Data Analysts of business Function.
● Designing, developing, and implementing scalable, automated processes for data extraction, processing, and analysis in a Data Mesh architecture
● Mentoring Fother Junior Engineers in the Team
● Be a “go-to” expert for data technologies and solutions
● Ability to provide on the ground troubleshooting and diagnosis to architecture and design challenges
● Troubleshooting and resolving technical issues as they arise
● Looking for ways of improving both what and how data pipelines are delivered by the department
● Translating business requirements into technical requirements, such as entities that need to be modelled, DBT models that need to be build, timings, tests and reports
● Owning the delivery of data models and reports end to end
● Perform exploratory data analysis in order to identify data quality issues early in the process and implement tests to ensure prevent them in the future
● Working with Data Analysts to ensure that all data feeds are optimised and available at the required times. This can include Change Capture, Change Data Control and other “delta loading” approaches
● Discovering, transforming, testing, deploying and documenting data sources
● Applying, help defining, and championing data warehouse governance: data quality, testing, coding best practices, and peer review
● Building Looker Dashboard for use cases if required

WHAT WE ARE LOOKING FOR:

• You have 4+ years of extensive development experience using snowflake or similar data warehouse technology
• You have working experience with dbt and other technologies of the modern data stack, such as Snowflake, Apache Airflow, Fivetran, AWS, git ,Looker
• You have experience in agile processes, such as SCRUM
• You have extensive experience in writing advanced SQL statements and performance tuning them
• You have experience in Data Ingestion techniques using custom or SAAS tool like fivetran
• You have experience in data modelling and can optimise existing/new data models
• You have experience in data mining, data warehouse solutions, and ETL, and using databases in a business environment with large-scale, complex datasets
• You have experience architecting analytical databases (in Data Mesh architecture) is added advantage
• You have experience working in agile cross-functional delivery team
• You have high development standards, especially for code quality, code reviews, unit testing, continuous integration and deployment
• You have strong technical documentation skills and the ability to be clear and precise with business users
• You have business-level of English and good communication skills
• You have basic understanding of various systems across the AWS platform ( Good to have )
• Preferably, you have worked in a digitally native company, ideally fintech
• Experience with python, governance tool (e.g. Atlan, Alation, Collibra) or data quality tool (e.g. Great Expectations, Monte Carlo, Soda) will be added advantage

Our Tech Stack:

2. DBT
4. Snowflake
6. Airflow
8. Fivetran
10. SQL
12. Looker

WHAT YOU’LL GET IN RETURN:

Make work, work for you! We are embracing new ways of working and support flexible working arrangements. With our Working Out of Office (WOO) policy our colleagues can work remotely from home or anywhere in their assigned Indian state. Additionally, you can work from a different country or Indian state for 90 days of the year. Plus, you’ll get:

• Competitive salary
• Self & Family Health Insurance
• Term & Life Insurance
• OPD Benefits
• Mental wellbeing through Plumm
• Learning & Development Budget
• WFH Setup allowance
• 15 days of Privilege leaves
• 12 days of Casual leaves
• 12 days of Sick leaves
• 3 paid days off for volunteering or L&D activities
• Stock Options

TIDEAN WAYS OF WORKING:At Tide, we champion a flexible workplace model that supports both in-person and remote work to cater to the specific needs of our different teams.

While remote work is supported, we believe in the power of face-to-face interactions to foster team spirit and collaboration. Our offices are designed as hubs for innovation and team-building, where we encourage regular in-person gatherings to foster a strong sense of community.

#LI-NN1

TIDE IS A PLACE FOR EVERYONEAt Tide, we believe that we can only succeed if we let our differences enrich our culture. Our Tideans come from a variety of backgrounds and experience levels. We consider everyone irrespective of their ethnicity, religion, sexual orientation, gender identity, family or parental status, national origin, veteran, neurodiversity or differently-abled status. We celebrate diversity in our workforce as a cornerstone of our success. Our commitment to a broad spectrum of ideas and backgrounds is what enables us to build products that resonate with our members’ diverse needs and lives.

We are One Team and foster a transparent and inclusive environment, where everyone’s voice is heard.

At Tide, we thrive on diversity, embracing various backgrounds and experiences. We welcome all individuals regardless of ethnicity, religion, sexual orientation, gender identity, or disability. Our inclusive culture is key to our success, helping us build products that meet our members' diverse needs. We are One Team, committed to transparency and ensuring everyone’s voice is heard.

You personal data will be processed by Tide for recruitment purposes and in accordance with Tide's Recruitment Privacy Notice.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=6vLV52Oi8uFssmbFAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXIsQrCMBAAUFz7CS7eqKKNCELRTSuiuNm9XNszica7kgvYT_Fz1eUNL_uMsuJK2EGJCeHI1jNRnN5Y3veATzLlvprBEi7SgBLG1oEwnERsoPHOpdTr1hjVkFtNmHybt_IywtTIYB7S6J9aHUbqAyaq15vVkPds55PD7ygqYILKdwSeoaTg_ALO3Hn8AiokirSbAAAA&shmds=v1_AUFQtOMZqfR81UizOknJFS6Ko92oATLYZdPWnmUEcDlJQaN0nQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=6vLV52Oi8uFssmbFAAAAAA%3D%3D,24 days ago
Data Engineering Manager,Roblox,"Gurugram, Haryana, India","Every day, tens of millions of people come to Roblox to explore, create, play, learn, and connect with friends in 3D immersive digital experiences– all created by our global community of developers and creators.

At Roblox, we’re building the tools and platform that empower our community to bring any experience that they can imagine to life. Our vision is to reimagine the way people come together, from anywhere in the world, and on any device. We’re on a mission to connect a billion people with optimism and civility, and looking for amazing talent to help us get there.

A career at Roblox means you’ll be working to shape the future of human interaction, solving unique technical challenges at scale, and helping to create safer, more civil shared experiences for everyone.

Roblox Operating System (ROS) is our internal productivity platform that governs how Roblox operates as a company. Through an integrated suite of tools, ROS shapes how we make talent and personnel decisions, plan and organize work, discover knowledge, and scale efficiently.

We are seeking a hands-on Engineering Manager to establish and lead a high-performing data engineering team for ROS in India. Collaborating with US-based ROS and Data Engineering teams, as well as the People Science & Analytics team, you will build scalable data pipelines, robust infrastructure, and impactful insights. Collaborating with the US ROS Engineering Manager, you will set high technical standards, champion leadership principles, and drive innovation while shaping the future of data engineering at Roblox.

You Will:
• Build and Lead: Attract, hire, mentor, and inspire a team with varied strengths of exceptional engineers. Cultivate a collaborative and inclusive environment where everyone thrives.
• Set the Bar: Establish and maintain a high standard for technical excellence & data quality. Ensure your team delivers reliable, scalable, and secure solutions that adhere to Roblox's engineering principles. Prepared to be hands-on with the ability to code & contribute to reviews and technical design discussions
• Cross-Functional Collaboration: Partner with data scientists & analysts, product & engineering, and other stakeholders to understand business needs and translate them into technical solutions.
• Strategic Planning: Contribute to the overall engineering strategy for Roblox India. Find opportunities for innovation and growth, and prioritize projects that deliver the most value to our users.
• Continuous Improvement: Cultivate a culture of learning and continuous improvement within your team. Encourage experimentation, knowledge sharing, and adoption of new technologies.

You Have:
• Proven Leadership: Demonstrated experience leading and scaling data engineering teams, ideally in a high-growth environment.
• Technical Expertise: Solid understanding of data engineering principles, and best practices for data governance. Experience building scalable data pipelines (Airflow or similar orchestration frameworks). Proficiency in SQL and relational databases. Familiarity with data warehouse solutions (e.g., Snowflake, Redshift, BigQuery) and data streaming platforms (e.g., Kafka, Kinesis, Spark). Knowledge of containerization (e.g., Docker) and cloud infrastructure (e.g., AWS, Azure, GCP)
• Roblox Alignment: Strong alignment with Roblox's leadership principles, including a focus on respect, safety, creativity, and community.
• Excellent Communication: Exceptional communication and interpersonal skills. Ability to build rapport with team members, stakeholders, and leaders across the organization.
• Problem-Solving: Strong analytical and problem-solving skills. Ability to break down complex challenges and develop creative solutions.
• Passion for Roblox: A genuine excitement for our platform and the possibilities of the metaverse.

Roles that are based in our San Mateo, CA Headquarters are in-office Tuesday, Wednesday, and Thursday, with optional in-office on Monday and Friday (unless otherwise noted).

You’ll Love:
• Industry-leading compensation package
• Excellent medical, dental, and vision coverage
• A rewarding 401k program
• Flexible vacation policy (varies by exemption status)
• Roflex - Flexible and supportive work policy
• Roblox Admin badge for your avatar
• At Roblox HQ:
• Free catered lunches five times a week and several fully stocked kitchens with unlimited snacks
• Onsite fitness center and fitness program credit
• Annual CalTrain Go Pass

Roblox provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Roblox also provides reasonable accommodations for all candidates during the interview process.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=4nxhkHvm6mibnxOnAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNPQrCQBBAYWxzBKvBUmJWBBttFX_AxguE2ThMVjYzy84G4jU8sbF55feq76JyJywIZ-EgRDkIwwMFmTJs4K4ejDB3PajARZUjLY99KckOzpnFhq1gCV3T6eBUyOvk3urtn9Z6zJQiFmp3--3UJOH16qk-6gRh5sY8csahhivmz_ys4SavgD8WP3PHlQAAAA&shmds=v1_AUFQtOME6ZDM6DyJBwaA7aQR4hF5KW5Wj-8Qp09blxAPPY-GHQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=4nxhkHvm6mibnxOnAAAAAA%3D%3D,N/A
Expert SAP BW- Data Engineer (Remote),Soltia,Anywhere,"This a Full Remote job, the offer is available from: India

Company Description

We are a consulting company with a bunch of technology-interested and happy people!

We love technology, we love design and we love quality. Our diversity makes us unique and creates an inclusive and welcoming workplace where each individual is highly valued.

With us, each individual is her/himself and respects others for who they are and we believe that when a fantastic mix of people gather and share their knowledge, experiences and ideas, we can help our customers on a completely different level.

We are looking for you who is immediate joiner and want to grow with us!

With us, you have great opportunities to take real steps in your career and the opportunity to take great responsibility.

Job ﻿Description:

We are now looking for a Senior Data Engineer who is passionate about building analytical solutions on SAP Data. You will help the company to derive the most of our data assets - with ultimate accountability for the business and customer value it delivers.

As a senior Data Engineer in the SAP Analytics Platform product, you will be working with one of the larger and volume intense SAP BW installations in the world. You interact with a high number of stakeholders to ensure the best utilization of our SAP BW investment. As a senior Data engineer, you are good at coaching others like leading programming sessions, presenting and aligning solutions to complex problems, and having a wide perspective on engineering practices to build scalable and robust platforms.

You will deploy different Analytical features, on SAP BW, and on other SAP OLTP systems as well. But not only this, but you will also use several products from Microsoft, Google, and other tools in order to fulfill the Analytical need of your Stakeholders.

As a Senior Data Engineer, you have great leadership skills believe in a non-hierarchical culture of collaboration, transparency, safety, and trust.

Working closely with the Product Area leadership and other Engineers to ensure overall alignment & key results. We believe you have a strong agile and DevOps mindset with Scrum Master experience so that you can drive Agile development practices.

This dual role offers a unique opportunity to combine SAP BW skills with a passion for Agile methodologies and team collaboration.

Qualifications
• A Data engineer with a minimum of 10 years experience in SAP BW development
• Experience in at least 3 full life cycle implementations on SAP BW or BW/4HANA with HANA modelling
• A person having previous extensive experience from SAP BW Retail scenarios, including experience from the following Data Sources:

- General Ledger, Billing (SAP ECC)

- Inventory Management (SAP ECC)

- POS-data (SAP CAR)

- Warehouse transactions (SAP EWM)
• Experience with Datasphere
• Experience with frontend analytics using SAP Analytics Cloud, PowerBI.
• Experience with CDS Views based data modeling, Data integration, and provisioning to-and-from SAP BW.
• Experience in Data Volume Management. Preferable project experience with large data volume
• Experience in Integrated Planning/BPC
• A great team player in an Agile team setup, willing to take different kinds of tasks to meet sprint commitment and to reach our goal together.
• Facilitating the events requidred by Scrum and ensuring that the team is actively driving development.
• Help everyone in the team, or interacting with the team, to understand Scrum theory, practices, rules and values (teaches, coaches, facilitates).

We are also looking for you who have
• Have several years of experience as a Data Engineer with strong abilities in modern engineering practices
• Advanced knowledge in SAP data analytics
• Scrum master experience
• Have competence within different agile frameworks and supporting tools
• Great collaboration and communication skills

Have strong problem-solving skills able to find solutions in complex situations and help it come alive by coding

Required cloud certification: NA

We are looking for you who:

Start: Immediately
Location: Remote, India
Form of employment: Full-time until further notice, we apply 6 months probationary employment

We interview candidates on an ongoing basis, do not wait to submit your application.

This offer from ""Soltia"" has been enriched by Jobgether.com and got a 72% flex score.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Ue15FF-bRbm11qoKAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCQAwAUFy7uDtlVKE9EVzURbGITmIHx5LWcD25JsclQz_Cjxbf8IrvrDjWU6Js0JwecH6VcEFDqNkHJsqwfNIoRiso4S4dKGHuBxCGq4iPtDgMZkn3zqnGyquhhb7qZXTC1MnkPtLpv1YHzJQiGrXb3WaqEvv1vJFoASEw3Pgd8AcRxzojjwAAAA&shmds=v1_AUFQtONeP9Ii2ixsnxPyeSjcJ-OFD2RnxPk5YAnSx8LAsmMi3g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Ue15FF-bRbm11qoKAAAAAA%3D%3D,N/A
TechStar Group - Data Engineer - Python/PySpark,Techstar Group,"Chennai, Tamil Nadu, India","Bachelor's in engineering / Master's degree in Computer Science, Information Systems and preferred to have domain knowledge in Mortgage

Job Description :

- Minimum of 7 of IT experience in data engineering.

- Strong technical expertise with data models in PostgreSQL/Snowflake.

- Extensive experience in Pyspark and performance tuning.

- Experience with databases and data warehousing concepts (Preferred - PostgreSQL, Snowflake)

- Strong SQL skills with experience in writing complex queries.

- Working knowledge of Data warehousing, Governance, and Data Architecture

- Ability to handle large scale structured and unstructured data from internal and third-party sources.

- Experience in python is essential.

- Knowledge in Kafka streaming and spark is an added advantage.

- Experience with data engineering tools/technologies in GCP Cloud environment

- Proficiency in designing and maintaining scalable data architectures.

- Experience with CI/CD Tools like GitHub",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=9h46e1Cmd8jzSVgiAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WOuw6CQBAAY8snWG1tkDMmNlr6IFoYEujJApu7U9i93B0JfJc_qFQ2k8xUk3xWya2i1pQRPeReRgdbuGBEuLK2TOR_XszRCKtiLh369y88pIFA6FsDwpCL6J7WJxOjC0elQugzHSJG22atDEqYGpnUS5qwoA4GPbkeI9X7w27KHOtNukyE_4RlOBtiRptChYPt4YndmMKdO4tfBw0kTbYAAAA&shmds=v1_AUFQtOPF2tia8_9j5toYGX8-rLVwRykW939ywntevDiUtmfXwA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=9h46e1Cmd8jzSVgiAAAAAA%3D%3D,N/A
IN Senior Associate  GCP Data Engineer  Advisory Corporate  Advisory Bangalore,PwC,India,"Line of ServiceAdvisoryIndustry/SectorNot ApplicableSpecialismSAPManagement LevelSenior AssociateJob Description & SummaryA career within…. A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.*Why PWC At PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purpose-led and values-driven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us. At PwC, we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm’s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations. "" Responsibilities:Design and implement scalable, efficient, and secure data pipelines on GCP, utilizing tools such as BigQuery, Dataflow, Dataproc, Pub/Sub, and Cloud Storage.Collaborate with cross-functional teams (data scientists, analysts, and software engineers) to understand business requirements and deliver actionable data solutions. Develop and maintain ETL/ELT processes to ingest, transform, and load data from various sources into GCP-based data warehouses.Build and manage data lakes and data marts on GCP to support analytics and business intelligence initiatives. Implement automated data quality checks, monitoring, and alerting systems to ensure data integrity. Optimize and tune performance for large-scale data processing jobs in BigQuery, Dataflow, and other GCP tools.Create and maintain data pipelines to collect, clean, and transform data for analytics and machine learning purposes.Ensure data governance and compliance with organizational policies, including data security, privacy, and access controls.Stay up to date with new GCP services and features and make recommendations for improvements and new implementations.Mandatory skill sets:GCP, Big query , Data ProcPreferred skill sets:GCP, Big query , Data Proc, AirflowYears of experience required:4-7Education qualification:B.Tech / M.Tech / MBA / MCAEducation (if blank, degree and/or field of study not specified)Degrees/Field of Study required: Master of Business Administration, Bachelor of Engineering, Master of EngineeringDegrees/Field of Study preferred:Certifications (if blank, certifications not specified)Required SkillsData Processing, Google BigQueryOptional SkillsAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline, Data Quality, Data Transformation, Data Validation {+ 18 more}Desired Languages (If blank, desired languages not specified)Travel RequirementsAvailable for Work Visa Sponsorship?Government Clearance Required?Job Posting End Date",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=uMAp0G-Q2sBMcMXkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2MOwrCQBBAsQ2ewGpqwawINlpplBALCXiAMEmGzco6s-wsGq_lCf0UNq94PF72mmRUneFC7CTCTlU6h4kAyqKGAyaEI1vHRBFg19-dSnxCITFI_GV_t0e26CUSLOAkLShh7AYQhlLEeppth5SCboxR9bnVhMl1eSc3I0ytjOYqrX7R6ICRgv_sm9V6OeaB7XxaPwpwDBX3Dt8X5J96tQAAAA&shmds=v1_AUFQtOOljxmL3GIsfJAVyPZiEdpokLRZ7PVz6IAzIGqsWRNs1Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=uMAp0G-Q2sBMcMXkAAAAAA%3D%3D,25 days ago
Data Engineer at Blackstraw AI,Blackstraw AI,"Chennai, Tamil Nadu, India","Blackstraw AI is hiring with Peerlist for a Full-time Data Engineer. Required skills: Hive,spark,Hadoop,Scala,Sql,Java,Python,Azure,RDBMS,Algorithms",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=K80oPQXI73IVt5sBAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1XNOw7CMAwAULH2CEyeoTQIqQtM_ITKwMReuamVBFK7SozodbgpYmR56ys-s6I-oSKc2QUmSoAKh4j2mTXhG_YNrOAqHWTCZD0Iw0XERZrvvOqYt8bkHCuXFTXYyspghKmTyTykyz_a7DHRGFGp3dTrqRrZLZb_RWA4emLGUMIdhxDhhv2rhIb7gF9z075wpAAAAA&shmds=v1_AUFQtOMaiseDKvM7I1gDi3Ri4gKwt9UOwdA3PN7uHOS5u5aTdQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=K80oPQXI73IVt5sBAAAAAA%3D%3D,N/A
Manager-Data/AI Engineering,AT&T,"Secunderabad, Telangana, India","Job Description:

AT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office). AT&T's Chief Data Office (CDO) is harnessing data and making AT&T's data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T's place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more. CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers. Employees at CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.

This is a Leadership role, the ideal candidate should have experience in technology, people management and financial acumen.

Candidate should have proven experience of working in product development environment with a proven experience of developing enterprise scale products in a highly agile/scrum environment.

Supervises a team of data insight employees that are responsible for the development and/or delivery of IT-related work relative to Application Development, Data - Engineering/Science/Warehousing/Visualization/ Analytics.

Exposure to requirement gathering, technology Solutions design, Business Management, Vendor Management, Systems Engineering, Software Delivery Project Management and Release Management.

Oversee daily operations in collaboration with other managers and department leaders, and perform administrative tasks such as tracking schedules, generating correspondence, maintaining reports, planning, and coordinating meeting.

Roles and Responsibilities:

Drive the team on innovation & implementation, creating & reviewing architectural designs, mentoring the team, and honing its engineering skills.

Guide team in creating technical specifications, prototypes, and presentations to communicate your ideas.

Well-versed in emerging industry technologies & trends.

The incumbent interviews and selects employees, maintains proper staffing levels, allocates resources, supervises teams daily operations, productivity, identifies areas for improvement, develops action plans to improve performance, develops teams technical and managerial expertise through on-the-job and formal training opportunities, enforces Company policies, takes necessary disciplinary action, evaluates individual performance for annual performance review, merit increases, promotions and other employment status changes, and conducts long-range planning for the team.

Responsible for People Management activities like Skill management, advancement, promotion, and other status changes for employees under their supervision.

Identify and manage Risks

Coordinate the execution of strategic initiatives

Participate in regular meetings with various stake holders, develops action plans to improve performance.

Enforces Company policies, set the team culture and ensure the motivated work environment

defining business and/or technical requirements, defining the projects scope and work schedules, estimating necessary resources, allocating capital and expense funding, and reconciling the actual expenses to estimates to keep the project within budget.

Key Competencies and Skills:

Required / Desired Skills
• Data Warehousing/Data Lake/ Data science
• Tech Leadership/ People Management
• Excellent communication skills
• Good power point and excel skills.
• Ability to multitask and keep deadlines.
• Impeccable managerial and interpersonal skills
• Proven track record of effectively interacting with senior management.
• Ability to work strategically and collaboratively across departments.
• Effective, versatile, and action-oriented

Weekly Hours:

40

Time Type:

Regular

Location:

Hyderabad, Andhra Pradesh, India

It is the policy of AT&T to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, AT&T will provide reasonable accommodations for qualified individuals with disabilities.

JobCategory:BigData",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=c_SXEILqN4m3OjPRAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOsQrCMBAAUFz7CU43dZC2kYKLTgVFKjjZvVzS4xqJdyWJ0F_xb9Xlza_4bIr2joJMsT5jRtP1cBH2QhS9MNRwUwuJMLoZVOCqyoG2pznnJR2NSSk0nDJm7xqnL6NCVlfzVJv-jGnGSEvATGN72K_NIrwru6EcwAs8yL1loogWpwoGCij8u1TQy-TxCzfMgOicAAAA&shmds=v1_AUFQtONKt6cM55iksYwb3GvjBv1w49aO9GK52fWsDv2HenXGgg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=c_SXEILqN4m3OjPRAAAAAA%3D%3D,N/A
"Manager, Cloud Engineer",MSD,"Hyderabad, Telangana, India","Job Description

Manager, Cloud Engineer

The Opportunity
• Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.
• Be part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.
• Drive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the world's greatest health threats.

Our Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company’s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.

A focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.

Role Overview

As a Migration Cloud Engineer, you will play a crucial role in moving an organization's data, applications, and infrastructure from on-premises environments to cloud platforms. This role involves a combination of technical expertise, strategic planning, and project management.

What will you do in this role
• Evaluate the existing IT infrastructure and applications to determine cloud readiness.
• Create a comprehensive migration strategy and roadmap.
• Select the appropriate cloud platform and deployment model.
• Execute the migration plan, including data transfer, application rehosting, replat forming, or refactoring.
• Develop, document, and implement changes based on the requests for change. Apply change control procedures, tools, techniques, and processes to manage and report on change requests.
• Contribute to the availability management process and its operation. Perform defined availability management tasks.
• Contribute to capacity modelling and planning while supporting the design of service component capacity.
• Continuously monitoring and optimizing the cloud environment for performance, cost-efficiency, and resource utilization.
• Work within a matrix organizational structure, reporting to both the functional manager and the project manager.
• Participate in project planning, execution, and delivery, ensuring alignment with both functional and project goals.

What should you have
• Bachelors’ degree in Information Technology, Computer Science or any Technology stream.
• 3+ Hands on technical experience in migrating workloads to cloud based environments.
• Strong expertise in AWS (90%) and Azure (10%), with skills in RDS, EC2, EMS, CI/CD, and GitHub in cloud migration.
• Hands-on experience with Terraform and IAC Tools.
• Expertise in cloud migration strategies (On-prem to Cloud / Cloud to Cloud).
• Maintains current industry knowledge of cloud native concepts, best practices, and technologies
• Industry experience in Bio-Science, Healthcare, Pharma, Retail, and Logistics will be plus.
• AWS certification preferred.

Our technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.

Who we are

We are known as Merck & Co., Inc., Rahway, New Jersey, USA in the United States and Canada and MSD everywhere else. For more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the world's most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.

What we look for

Imagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us—and start making your impact today.

#HYDIT2025

Current Employees apply HERE

Current Contingent Workers apply HERE

Search Firm Representatives Please Read Carefully
Merck & Co., Inc., Rahway, NJ, USA, also known as Merck Sharp & Dohme LLC, Rahway, NJ, USA, does not accept unsolicited assistance from search firms for employment opportunities. All CVs / resumes submitted by search firms to any employee at our company without a valid written search agreement in place for this position will be deemed the sole property of our company. No fee will be paid in the event a candidate is hired by our company as a result of an agency referral where no pre-existing agreement is in place. Where agency agreements are in place, introductions are position specific. Please, no phone calls or emails.

Employee Status:
Regular

Relocation:

VISA Sponsorship:

Travel Requirements:

Flexible Work Arrangements:
Hybrid

Shift:

Valid Driving License:

Hazardous Material(s):

Required Skills:
Availability Management, Capacity Management, Change Controls, Design Applications, High Performance Computing (HPC), Incident Management, Information Management, Information Technology (IT) Infrastructure, IT Service Management (ITSM), Release Management, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, System Administration, System Designs

Preferred Skills:

Job Posting End Date:
03/14/2025
• A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.

Requisition ID:R333915",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=OzYiuCWTqHUbeWWtAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOvQrCMBAAYFz7CE6Ho9REBBcdq_gDnXQvl-ZII_Gu5CLUx_CN1eWbv-ozq0yLjIFyDU2Sl4cjh8hEGVZwFQdKmPsBhOEkEhLN90Mpo-6sVU0maMESe9PL0wqTk8k-xOmfTgfMNCYs1G2268mMHJaL9naAyHB-e8ro0Ndwp4QcfocaLuwjfgFJBx_NlAAAAA&shmds=v1_AUFQtON-taHtED1eSDnCmvIEJBWUFGXUQJuMsqTAYUhs48gzmA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=OzYiuCWTqHUbeWWtAAAAAA%3D%3D,19 days ago
Data Engineer(AWSPYSPARKSQL),ProtoGene Consulting Private Limited,"Mumbai, Maharashtra, India","Data Engineer + Integration engineer + Support specialistExp – 5-8 years

Necessary Skills:· SQL & Python / PySpark

· AWS Services: Glue, Appflow, Redshift

· Data warehousing

· Data modelling

Job Description:· Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform. Design/ implement, and maintain the data architecture for all AWS data services

· A strong understanding of data modelling, data structures, databases (Redshift), and ETL processes

· Work with stakeholders to identify business needs and requirements for data-related projects

Strong SQL and/or Python or PySpark knowledge

· Creating data models that can be used to extract information from various sources & store it in a usable format

· Optimize data models for performance and efficiency

· Write SQL queries to support data analysis and reporting

· Monitor and troubleshoot data pipelines

· Collaborate with software engineers to design and implement data-driven features

· Perform root cause analysis on data issues

· Maintain documentation of the data architecture and ETL processes

· Identifying opportunities to improve performance by improving database structure or indexing methods

· Maintaining existing applications by updating existing code or adding new features to meet new requirements

· Designing and implementing security measures to protect data from unauthorized access or misuse

· Recommending infrastructure changes to improve capacity or performance

Experience in Process industry

Data Engineer + Integration engineer + Support specialistExp – 3-5 years

Necessary Skills:· SQL & Python / PySpark

· AWS Services: Glue, Appflow, Redshift

· Data warehousing basics

· Data modelling basics

Job Description:· Experience of implementing and delivering data solutions and pipelines on AWS Cloud Platform.

· A strong understanding of data modelling, data structures, databases (Redshift)

Strong SQL and/or Python or PySpark knowledge

· Design and implement ETL processes to load data into the data warehouse

· Creating data models that can be used to extract information from various sources & store it in a usable format

· Optimize data models for performance and efficiency

· Write SQL queries to support data analysis and reporting

· Collaborate with team to design and implement data-driven features

· Monitor and troubleshoot data pipelines

· Perform root cause analysis on data issues

· Maintain documentation of the data architecture and ETL processes

· Maintaining existing applications by updating existing code or adding new features to meet new requirements

· Designing and implementing security measures to protect data from unauthorized access or misuse

· Identifying opportunities to improve performance by improving database structure or indexing methods

· Designing and implementing security measures to protect data from unauthorized access or misuse

· Recommending infrastructure changes to improve capacity or performance

Skills:- PySpark, Amazon Web Services (AWS), SQL, AWS GLUE and Apache Airflow",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=hOFaCEyVvtA7XLssAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMu4rCUBCAYbb1EbaaUkUTcbFZK2-IN4imWKxkEoeTkWQmnBnFJ_P5dJu_-vg7r6_OzxIdYSWBhSh2Z395ds6z2WmXH_c9GMJWCzDCWFagAmvVUNP3tHJv7TdNzeokmKNzmZTapCpU6DO9aWH_uViFkdoanS7jyeiZtBL68yyq65qEYKFi99pZAmSRHx8Ge27Y6QoscLg3BfIADvi5oFUecQAbuTK-AZtvayS5AAAA&shmds=v1_AUFQtONJEy8_Y5w-9mlULVv_UmFbJaWT10G-ziUGZDBINFP5KA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=hOFaCEyVvtA7XLssAAAAAA%3D%3D,N/A
"Backend, Frontend and Data Engineer",Vumonic Datalabs,"Goa, India","🚀 We’re Hiring! (Frontend, Backend & Data Engineers) – Build, Innovate & Chill in Goa!

Love coding? Want to work on AI-powered SaaS while enjoying Goa’s legendary work-life balance? Join Inboox, where tech meets paradise! 🌴

Open Roles (Full-Time & Internships):

💻 Frontend Engineers – React.js, Vue.js, or similar JS magic

⚙️ Backend Engineers – Node.js, Python, LLM & backend wizardry

📊 Data Engineers – SQL, ClickHouse, LLM & scalable data systems

Why Join Us?

🌊 Live & work in Goa – A tech career where work-life balance actually exists

⚡ Build something BIG – AI-powered SaaS that’s shaping email marketing

🌎 Broad-minded team – No rigid hierarchies, just innovation & creativity

What You’ll Do:

🚀 Develop and scale a next-gen SaaS product

🛠️ Experiment, learn & push boundaries with real-world projects

🔥 Collaborate with top mentors & tech enthusiasts

This isn’t just a job—it’s an opportunity to grow, innovate, and live the good life! 🌴🚀 Write to hr@vumonic.com to Apply!",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=lShuvkmjbgvlWAEvAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2NOw7CMBAFRZsjUKCtUYgREk3oEB_BAWijtbNyHJzdyGukXIXbklC80VTziu-qqM_o3sRtCbcknGcDnHfBjHBlH5gowQ6eYkEJk-tAGO4iPtL61OU8am2Maqy8ZszBVU4GI0xWJtOL1QWNdphojJipORz3UzWy325en0E4uP9VRKsQljCW8OA24A9L6jNInAAAAA&shmds=v1_AUFQtOO7zWc8GqOze_lVegREupLCVpIbFjZ4K8MGAXKFUE4CTw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=lShuvkmjbgvlWAEvAAAAAA%3D%3D,22 days ago
Senior Data Engineer,Growexx,"Gujarat, India","Growexx is looking for smart and passionate Senior Data Engineer, who will design and fill a bespoke data warehousing environment for our company.

Key Responsibilities:
• Design architecture for data activation inclusive of an API gateway that can be consumed by client applications.
• Develop and maintain scalable pipelines that deploy data out to product applications, website data Layers, and digital advertising platforms (Salesforce, Google, Facebook, etc.).
• Develop and deploy scalable new features.
• Identify and fix bugs to resolve data quality issues in a timely manner.
• Develop, maintain, update Tableau reporting views and data sources.
• Implement a data monitoring framework that ensures production tables are always accurate.
• Write clean, maintainable, and efficient Java code for data transformation and integration tasks.
• Take data science/machine learning model prototypes and prepare them for production deployments.
• Improve real-time data availability on an as-needed basis for activation use-cases.
• Build pipelines that ingest digital advertising data on an as-needed basis.
• Document all pipelines and maintain a data catalogue.

Key Skills:
• Experience in data engineering or related fields.
• Proficiency in Java for developing scalable data solutions.
• Experience with big data technologies such as Hadoop, Apache Spark, Kafka, or similar frameworks.
• Understanding of relational and NoSQL databases
• Hands-on experience with AWS
• Knowledge of data modelling, data warehousing, and schema design.
• Proficiency in SQL and query optimization techniques.
• Familiarity with CI/CD pipelines and version control systems (e.g., Git)
• Experience with other programming languages such as Python or Scala.
• Knowledge of containerization and orchestration tools like Docker and Kubernetes.
• Familiarity with machine learning pipelines and tools.

Education and Experience:
• B Tech or B. E. (Computer Science / Information Technology)
• 5+ years as a Data Engineer or similar roles or similar roles.

Analytical and Personal skills:
• Must have good logical reasoning and analytical skills
• Good Communication skills in English – both written and verbal
• Demonstrate Ownership and Accountability of their work
• Attention to details",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=8FFUZgee-OrNmeFuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXMOw7CMAwAULH2BjB55pMgJBa6gipYOUDlFCtNVewoNiIX4N7A8sbXfBbN5k6cpMAZDeHCMTFRgR3cJIASlmEEYehE4kyrdjTLevJedXZRDS0NbpCnF6Yg1U8S9E-vIxbKMxr1h-O-usxxveyKvKlWSL_vNWFB28KVHwm_eow05IgAAAA&shmds=v1_AUFQtOOt4FDa5o67WvJejPJPxRXONzH0GWN1J8XWBidjkRdXuA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=8FFUZgee-OrNmeFuAAAAAA%3D%3D,N/A
Solution Architect -Data Engineering and Analytics,Eaton,"Pune, Maharashtra, India","Job Summary

As a Solutions Architect, the candidate will be responsible for understanding requirements and building solution architectures for the Data Engineering and Advanced Analytics Capability. The role will require a mix of technical knowledge and finance domain functional knowledge while the functional knowledge is not necessarily a must have.The candidate will apply best practices to create data architectures that are secure, scalable, cost-effective, efficient, reusable, and resilient. The candidate will participate in technical discussions, present their architectures to stakeholders for feedback, and incorporate their input. The candidate will evaluate, recommend, and integrate SaaS applications to meet business needs, and provide architectures for integrating existing Eaton applications or developing new ones with a cloud-first mindset. The candidate will offer design oversight and guidance during project execution, ensuring solutions align with strategic business and IT goals.

As a hands-on technical leader, the candidate will also drive Snowflake architecture. The candidate will collaborate with both technical teams and business stakeholders, providing insights on best practices and guiding data-driven decision-making. This role demands expertise in Snowflake’s advanced features and cloud platforms, along with a passion for mentoring junior engineers.

Job Responsibilities
• Collaborate with data engineers, system architects, and product owners to implement and support Eaton's data mesh strategy, ensuring scalability, supportability, and reusability of data products.
• Lead the design and development of data products and solutions that meet business needs and align with the overall data strategy, creating complex enterprise datasets adhering to technology and data protection standards.
• Deliver strategic infrastructure and data pipelines for optimal data extraction, transformation, and loading, documenting solutions with architecture diagrams, dataflows, code comments, data lineage, entity relationship diagrams, and metadata.
• Design, engineer, and orchestrate scalable, supportable, and reusable datasets, managing non-functional requirements, technical specifications, and compliance.
• Assess technical capabilities across Value Streams to select and align technical solutions following enterprise guardrails, executing proof of concepts (POCs) where applicable.
• Oversee enterprise solutions for various data technology patterns and platforms, collaborating with senior business stakeholders, functional analysts, and data scientists to deliver robust data solutions aligned with quality measures.
• Support continuous integration and continuous delivery, maintaining architectural runways for products within a Value Chain, and implement data governance frameworks and tools to ensure data quality, privacy, and compliance.
• Develop and support advanced data solutions and tools, leveraging advanced data visualization tools like Power BI to enhance data insights, and manage data sourcing and consumption integration patterns from Eaton's data platform, Snowflake.
• Accountable for end-to-end delivery of source data acquisition, complex transformation and orchestration pipelines, and front-end visualization.
• Strong communication and presentation skills, leading collaboration with business stakeholders to deliver rapid, incremental business value/outcomes.
• Lead and participate in the planning, definition, development, and high-level design of solutions and architectural alternatives.
• Participate in solution planning, incremental planning, product demos, and inspect and adapt events.
• Plan and develop the architectural runway for products that support desired business outcomes.
• Provide technical oversight and encourage security, quality, and automation.
• Support the team with a techno-functional approach as needed.

Qualifications:
• BE in Computer Science, Electrical, Electronics/ Any other equivalent Degree
• Education level required: 10 years
• Experience or knowledge of Snowflake, including administration/architecture.
• Expertise in complex SQL, Python scripting, and performance tuning.
• Understanding of Snowflake data engineering practices and dimensional modeling for performance and scalability.
• Experience with data security, access controls, and setting up security frameworks and governance (e.g., SOX).

Technical Skills
• Advanced SQL skills for building queries and resource monitors in Snowflake.
• Proficiency in automating Snowflake admin tasks and handling concepts like RBAC controls, virtual warehouses, resource monitors, SQL performance tuning, zero-copy clone, and time travel.
• Experience in re-clustering data in Snowflake and understanding micro-partitions.
• Excellent analysis, documentation, communication, presentation, and interpersonal skills.
• Ability to work under pressure, meet deadlines, and manage, mentor, and coach a team of analysts.
• Strong analytical skills for complex problem-solving and understanding business problems.
• Experience in data engineering, data visualization, and creating interactive analytics solutions using Power BI and Python.
• Extensive experience with cloud platforms like Azure and cloud-based data storage and processing technologies.
• Expertise in dimensional and transactional data modeling using OLTP, OLAP, NoSQL, and Big Data technologies.
• Familiarity with data frameworks and storage platforms like Cloudera, Databricks, Dataiku, Snowflake, dbt, Coalesce, and data mesh.
• Experience developing and supporting data pipelines, including code, orchestration, quality, and observability.
• Expert-level programming ability in multiple data manipulation languages (Python, Spark, SQL, PL-SQL).
• Intermediate experience with DevOps, CI/CD principles, and tools, including Azure Data Factory.
• Experience with data governance frameworks and tools to ensure data quality, privacy, and compliance.
• Solid understanding of cybersecurity concepts such as encryption, hashing, and certificates.
• Strong analytical skills to evaluate data, reconcile conflicts, and abstract information.
• Continual learning of new modules, ETL tools, and programming techniques.
• Awareness of new technologies relevant to the environment.
• Established as a key data leader at the enterprise level.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=5GwuEJdqTXtOk5z4AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWMQQrCQAwA8doneIpXsa0IXvRUsEgFQfABJW3D7sqalCaF-iY_6XqZ08xk31XWPCXOFoShmnofjHqD_IKGULMLTDQFdoA8QMUYPxZ6hRxu0oESpgJSeRVxkdZnbzbqqSxVY-HUMMlFL-9SmDpZypd0-kerHicaIxq1h-N-KUZ2202NllaB4TEz7eCOSUL1NuEOGh4C_gDR-LWArgAAAA&shmds=v1_AUFQtOP6eJCC4KflMTnBVXr-rsMjLEqjI9htqYAOkoc4yHDMZQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=5GwuEJdqTXtOk5z4AAAAAA%3D%3D,15 days ago
"Data Engineer - ETL, Python, SQL",UST,"Maharashtra, India","Role Description

The Position:

We are searching for a data/software engineer in ETL who can implement solutions per the teams goals and is passionate about changing how millions of people save energy.Youll work with the engineering team to build and improve our platforms to deliver flexible and creative solutions to our utility partners and end users. Youll be a part of developing a robust architecture to manage massive amounts of data and leveraging it to deliver a compelling experience to users across our platforms. You will own the development and its quality independently and be responsible for high quality deliverables.

Target Candidate You Should
Be excited to work with talented, committed people in a fast-paced environment.
Use a rigorous approach for product improvement and customer satisfaction.
Be ready, able, and willing to jump onto a call with a partner or customer to help solve problems.
Have a strong eye for detail.
Love developing great products as a seasoned engineer.
Have an analytical and agile mindset.

Required Skills
You are an experienced developer with 3-7 years of professional experience.
Excellent knowledge of Relational Databases(MySQL, PostgreSQL) and SQL or ORM technologies.
Expert in writing SQL queries - joins query tuning, etc.
Can work with team members to troubleshoot data issues and understand different phases during ETL - efficiently figure out constraints, duplicates & invalid data.
Strong experience in Cloud infrastructure and technologies (AWS or Google Cloud).
Any experience in Google Cloud Platform - BigQuery, Airflow, Cloud Data Fusion, and Cloud Functions will be a distinct advantage.
Well versed with Git & Github repository.
Familiar with Linux systems and their command-line interfaces.
Exposure to Python - preferably both 2.7 and 3.x - OOPs, data structures, algorithms & automation.
Experience in managing APIs to industry-accepted RESTful standards.
Expert in implementing security best practices.

Required

The following experiences are not required, but you'll stand out from other applicants if you have any of the following, in our order of importance:
Solid CI/CD experience.
Automation of multi-step repetitive tasks.
Serverless architecture, preferably AWS Lambda.
Pandas / SciPy / Numpy / R / SciKitLearn.

Educational Qualification MCA/MCS, B.Tech/B.E

Skills

Python 3.X,Sql,Git,",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=7p-BV5cK2VShjnTWAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXJsQrCMBCAYVz7BjrdLLURQQRdLaJUUFrncq0hidS7kLuhPoZvbF3-4fuz7yzbHVERSnKBrE2wgrKpcrh91DPlUN-riS7cgVhMvQcmODG7wS4OXjXK3hiRoXCiqKEven4bJtvxaF7cyT-teEw2Dqi23WzXYxHJLeePuoFAcMVponhNmMOZngF_p8eQrZQAAAA&shmds=v1_AUFQtOMOPJV6ZCZlPqjo0VnWdr5IR7mta9yyC8B8rkC15oAaiw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=7p-BV5cK2VShjnTWAAAAAA%3D%3D,N/A
Data Engineer: Data Platforms,Ibm (international Business Machines),"Pune, Maharashtra, India","Introduction
In this role, you ll work in one of our IBM Consulting Client Innovation Centers (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centers offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology.

A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.
You ll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.
Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you ll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.

Your Role and Responsibilities
As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.

Your primary responsibilities include:
• Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.
• Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.
• Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too.

Required Technical and Professional Expertise
• Must have 3-5 years exp in Big Data -Hadoop Spark -Scala ,Python
• Hbase, Hive Good to have Aws -S3,
• athena ,Dynomo DB, Lambda, Jenkins GIT
• Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).
• Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD s were used to apply business transformations and utilized Hive Context objects to perform read/write operations..

Preferred Technical and Professional Expertise
• Understanding of Devops.
• Experience in building scalable end-to-end data ingestion and processing solutions
• Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala

Qualification : Must have 3-5 years exp in Big Data -Hadoop Spark -Scala ,Python",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=S1Cjh0C49r2CxMU_AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_x2OwQrCQAxE8dpP8JSjSu2K6KWeFEUqCP2DktbYXWmTstlCv8zvc-tlmBmYxyTfRXK4YkC4ceuYyOfwj2WH4S2-V9jCQ2pQQt9YEIa7SNvR8mRDGDQ3RrXLWg0YXJM10hthqmUyH6l1lkotehoijqr9cTdlA7ebc1H3sHIcyHMcCmMHl1HjAVV4YmNntwbHUI5MaawiBNUGjykU_HL4A83lcee5AAAA&shmds=v1_AUFQtOPcY25KQYLSJ-gb4-daBe3EuzU0gm_t8tWWj4ybgnPGMA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=S1Cjh0C49r2CxMU_AAAAAA%3D%3D,N/A
"Principal Engineer Golang, Java, AWS & Data Engineering role at forcepoint in Mumbai ,Thane",forcepoint,"Thane, Maharashtra, India (+1 other)","Apply for Principal Engineer Golang, Java, AWS & Data Engineering role at Career Progress Consultants in Mumbai ,Thane with 1-4 year experience required. Find top opportunities in IT-Hardware/Networking, Telecom with a salary of Rs.3.60 - 6.80 Lakhs. Register on TimesJobs to apply today!",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=niNAGuEnD1BJsGJrAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_1WOsUpDQRBFsc0nWN0qgry8J4KNqQQlJBAQFCzDvM1kd2Qzs-xOQr7Vr_FpIdjc6nDPmX1dzeprFQ1SKONFoyhzxcoyaeywoTN1ePp4wxzP5PRHiEZUywxyHKwGLibqEMX2dBxJ0L0nUsYCGxvRmGpIMJ2OLWa-Xib30h6HobXcx-bkEvpgx8GUR7sMnza2n9m1RJVLJufd_cPdpS8ab2_-C39FHbY0odSS16l4rXuhb4CefB7dAAAA&shmds=v1_AUFQtOPPuVYXWuMqsddk3JMr4ODglFmPAbD0frduH4uQ2Kwxyg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=niNAGuEnD1BJsGJrAAAAAA%3D%3D,8 days ago
Technical specialist- Data Engineer,Deutsche Bank,"Pune, Maharashtra, India","Technical specialist- Data Engineer

Job ID: R0351780
Full/Part-Time: Full-time

Regular/Temporary: Regular
Listed: 2025-01-03

Location: Pune

Position Overview

Job Title- Technical specialist- Data Engineer

Location- Pune, India

Role Description

An Engineer is responsible for designing, developing and delivering significant components of engineering solutions to accomplish business goals. Key responsibilities of this role include active participation in the design and development of new features of application enhancement, investigating re-use, ensuring that solutions are fit for purpose and maintainable, and can be integrated successfully into the overall solution and environment with clear, robust and well tested deployments. Assists more junior members of the team and controls their work where applicable.

What we’ll offer you

As part of our flexible scheme, here are just some of the benefits that you’ll enjoy
• Best in class leave policy
• Gender neutral parental leaves
• 100% reimbursement under childcare assistance benefit (gender neutral)
• Sponsorship for Industry relevant certifications and education
• Employee Assistance Program for you and your family members
• Comprehensive Hospitalization Insurance for you and your dependents
• Accident and Term life Insurance
• Complementary Health screening for 35 yrs. and above

Your key responsibilities
• Develops and deploys source code, including infrastructure and application related configurations , for all Software Components in accordance with Detailed Software Requirements specification.
• Provides Development leading activities for Projects and technical infrastructure components (i.e., Java+Python, Apache Beam/ Spark configurations and Security, Data Workflows on-premise /cloud, Infrastructure as a Code) and source code development.
• Debugs, fixes and provides support to L3 team.
• Verifies the developed source code by reviews (4-eyes principle).
• Contributes to quality assurance by writing and conducting unit testing.
• Ensures architectural changes (as defined by Architects) are implemented. Drives low level designs
• Contributes to problem and root cause analysis.
• Integrates software components following the integration strategy.
• Verifies integrated software components by unit and integrated software testing according to the software test plan. Software test findings must be resolved.
• Drives technical continuous improvements across the modules regularly
• Where applicable, develops routines to deploy CIs to the target environments.
• Provides Release Deployments on non-Production Management controlled environments.
• Supports creation of Software Product Training Materials, Software Product User Guides, and Software Product Deployment Instructions.
• Checks consistency of documents with the respective Software Product Release.
• Where applicable, manages maintenance of applications and performs technical change requests scheduled according to Release Management processes.
• Fixes software defects/bugs, measures and analyses code for quality.
• Collaborates with colleagues participating in other stages of the Software Development Lifecycle (SDLC).
• Identifies dependencies between software product components, between technical components, and between applications and interfaces.
• Identifies product integration verifications to be performed based on the integration sequence and relevant dependencies.
• Suggests and implements continuous technical improvements on the applications (Scalability, Reliability, Availability , Performance)

Your skills and experience

General Skills
• Bachelor of Science degree from an accredited college or university with a concentration in Computer Science or Software Engineering (or equivalent) – with a minor in Finance, Mathematics or Engineering.
• Strong analytical skills.
• Proficient communication skills.
• Fluent in English (written/verbal).
• Ability to work in virtual teams and in matrixed organizations.
• Excellent team player.
• Open minded.
• Keeps pace with technical innovation.
• Understands the relevant business area
• Ability to share information, transfer knowledge and expertise to team members.
• Ability to design and write code in accordance with provided business requirements
• Ability to contribute to QA strategy and Architecture decisions.
• Knowledge of IT delivery and architecture including knowledge of Data Modelling ,data analysis.
• Relevant Financial Services experience.
• Ability to work in a fast-paced environment with competing and alternating priorities with a constant focus on delivery.
• Ability to balance business demands and IT fulfilment in terms of standardization, reducing risk and increasing IT flexibility.

Domain Specific Skills

Desirable are one or more of the following subject areas:
• Very Good knowledge of the following technologies are needed:
• Java / Scala
• Apache Spark / Apache Beam
• GCP (Data Engineering services)
• Workflow orchestrators like Airflow
• Automation through Python/ Terraform
• Good to have - AI/ ML, Data analysis, Python development
• Very good Knowledge about the core processes / tools such as HP ALM, Jira, Service Now, SDLC, Agile processes.

How we’ll support you
• Training and development to help you excel in your career
• Coaching and support from experts in your team
• A culture of continuous learning to aid progression
• A range of flexible benefits that you can tailor to suit your needs

About us and our teams

Please visit our company website for further information:

https://www.db.com/company/company.htm

We strive for a culture in which we are empowered to excel together every day. This includes acting responsibly, thinking commercially, taking initiative and working collaboratively.

Together we share and celebrate the successes of our people. Together we are Deutsche Bank Group.

We welcome applications from all people and promote a positive, fair and inclusive work environment.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=20mtlnbGIOqA-WEvAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKsQrCMBAAUFz7CU63Km0jgkvdpCIKgoN7ucYjicZLyF2hH-SHqsubXvVZVN2drOdgMYJksgFjEG2gR0U4sgtMVKCBSxpBCIv1kBhOKblIy71XzdIZIxJbJ4oabGvT2ySmMc3mmUb5M4jHQjmi0rDdbeY2s1uveppUrCc4IL8gMNwmphqu-MsoXgvWcOZHwC-4zd6opwAAAA&shmds=v1_AUFQtOOIhBqsQixXdaRjw9WS0_cA-VwseEW-ao5EKsmzbqzcvg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=20mtlnbGIOqA-WEvAAAAAA%3D%3D,N/A
Data Engineer III,JPMorgan Chase & Co.,"Mumbai, Maharashtra, India","Be part of a dynamic team where your distinctive skills will contribute to a winning culture.

Your goal is to become a key player among other imaginative thinkers who share a common commitment to continuous improvement and meaningful impact. Don’t miss this chance to collaborate with brilliant minds and deliver premier solutions that set a new standard.

As a Data Architecture & Engineering team member at JPMorgan Chase Wealth Management Technology , you are an integral part of a team that works to design, develop and implement high-quality data architecture solutions for various software products and applications on modern cloud-based technologies. As a core technical member and contributor yourself, you are responsible for carrying out critical data architecture solutions across multiple technical areas within various business functions in support of project goals.

Job responsibilities
• Engages technical teams and business stakeholders to discuss and propose data architecture approaches to meet current and future needs
• Defines the data architecture target state of their product and drives achievement of the strategy
• Ability to research and prototyping in iterative, feedback-led designs for data interfaces, APIs and pipelines for new and existing data products within JPMWM.
• Expertise in creating repeatable frameworks and processes for creating APIs and data pipelines that will enable standardized and transparent implementation patterns.
• Experience of identifying, prioritizing, aligning and negotiating features and functionality that deliver value or align with your wider business strategy.
• Evaluates recommendations and provides feedback for data, analytics and machine learning technologies
• Executes creative data architecture solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions and break down technical problems
• Develops secure, high-quality production code and reviews and debugs code written by others
• Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems
• Bring in knowledge of best of the breed industry practices, tools and policies based on rich consulting expertise in domain of data engineering, data modernization and data repurposing to make enterprise data fit for purpose of both human and machine analytics and decision making.

Required qualifications, capabilities, and skills
• Hands-on practical experience delivering system design, application development, testing, and operational stability
• MUST have Advanced knowledge of architecture and one or more data engineering programming languages Python, Pyspark , Scala or Java
• Proficiency in automation and continuous delivery methods and well known tools
• Demonstrated proficiency in software applications and technical processes within a technical discipline (., cloud, artificial intelligence, machine learning, mobile,
• In-depth knowledge of the financial services industry and their IT systems, their general interdependencies.
• Hands on experience on cloud IaC, PaaS and IaaS provisioning and configs, cost optimized capacity design, volumetrics and planning.
• Practical cloud native experience, especially on big-data, high volume data parallel processing and data lakes APIs
• Ability to evaluate current and emerging technologies to recommend the best data architecture solutions for the future state architecture",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=33r8nbap37TBAfjuAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCMBAAUFz7CU43iUhNpOCiYxVJoeAflEs9kkh7V3IR_Bd_Vl3e9qrPqtpesCBcOSQmyuCcgz104kEJ8xhBGG4iYaL1OZay6Mla1ckELVjSaEaZrTB5eduneP0zaMRMy4SFhuZ4eJuFw67p7r3kgAxtRCXYQCsGEkP_mj2mGnr8LdRYMtbg-JHwC8fVwcqeAAAA&shmds=v1_AUFQtOMJJO9BoMhsw309KhRWTxf98sr_lhqO8cg5For-ebbY2g&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=33r8nbap37TBAfjuAAAAAA%3D%3D,3 days ago
data-engineer-immd-bangalore-7-years-23167,Altimetrik,"Bengaluru, Karnataka, India",No job summary available.,https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=KdQNbAgvW6y0pMUkAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOQQ6CMBAA45UneNqrhlLFKIme8GLUR5AFNqVSdklbEnyTnxQuc5jLTPLbJGWLERWxsUzklR2GVtXIBp14UoX6Evqg8tPxUoCCl9QQFtN0IAwPEeNoe-tiHMNV6xBcZkLEaJuskUELUy2z_kgdVlShQ0-jw0hVfj7M2chmvytdtANFb3uwDHda05OfUnij5-WtxxSe3Fr8AwxNQjauAAAA&shmds=v1_AUFQtONkGSM737Ez3P7MuxQcq7qzZ6sBMLavVWOzjK7k4he1_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=KdQNbAgvW6y0pMUkAAAAAA%3D%3D,9 days ago
"Sr. Data Engineer(4+ Yrs, Hadoop, Scala, Python, Java",Visa,"Karnataka, India","Company DescriptionCompany DescriptionVisa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose – to uplift everyone, everywhere by being the best way to pay and be paid.Make an impact with a purpose-driven industry leader. Join us today and experience Life at Visa.Job DescriptionJob DescriptionTeam SummaryThe Risk Solution Services (RSS) team, part of the Acceptance Risk organization, includes data engineers who develop and support data applications for generating reports and insights for Risk Consultants working with global clients. These insights help tune fraud detection rules, maximizing automated acceptance while minimizing fraud losses.What a Sr Data Engineer does at Visa:The Sr Data Engineer will be a member of RSS team in Asia Pacific. The position will be based in Visa’s Bangalore office.The individual will be accountable for supporting and driving the design, development and implementation of analytics-driven strategies as well as high-impact solutions for Visa clients. He/she will bring in deep expertise from banking and payments with a strong background in data pipelines to solve complex problems and unlock business value.Responsibilities• Experience on developing, maintaining and supporting big data ETL pipelines (Hadoop, Hive, Spark)• Responsible for the implementation and test of scalable distributed ensuring security, timeliness and quality of data. • Experience supporting production systems• Work closely with other engineers and peers to design and develop ETL pipelines• Work with product managers in understanding and clarifying on the requirements• Demonstrate inc",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=xa2gwwi0qT0P-7htAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOsQrCMBCAYVz7BOJ0o2hMRXTRVVGrg1AQnOTahiRa70IuSH0g39O6_Ov3Z99Bdi6jhi0mhB1ZT8bE8XIKtygKDtgwBwVljS0quHySY1JQ4BthBgVXIAZj7YAJ9sy2NaONSynIOs9FWm0lYfK1rvmVM5mKu_zBlfxzF4fRhBaTuS9W804HspPh1QuCJzhhpP7o2aNHajz-AK8-34uoAAAA&shmds=v1_AUFQtOP7qN5oYjQKNCwmu5ZaqQuXu3RtbPS6ZZ1sw4VrO-qE9w&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=xa2gwwi0qT0P-7htAAAAAA%3D%3D,20 days ago
Azure Data Engineer - Synapse Analytics,People Konnect,"Mumbai, Maharashtra, India","Job Title : Data Engineer

Function : Information Technology

Location : Mumbai, Maharashtra, India

Education :

- BE / B-Tech in Computer Science from a premier institute

- MBA is preferred

- Azure Cloud Data Engineering Certifications

Experience:

- 10 years of overall exp and at least 5 years exp in Azure Data Engineering

Responsibility :

- Designing and implementing scalable and secure data processing pipelines using Azure Data Factory, Azure Data bricks, and other Azure services.

- Managing and optimizing data storage using Azure Data Lake Storage, Azure SQL Data Warehouse.

- Developing data models and maintaining data architecture to support data analytics and business intelligence reporting.

- Ensuring data quality and consistency through data cleaning, transformation, and integration processes.

- Monitoring and troubleshooting data-related issues within the Azure environment to maintain high availability and performance.

- Collaborating with data scientists, business analysts, and other stakeholders to understand data requirements and implement appropriate data solutions.

- Implementing data security measures, including encryption, access controls, and auditing, to protect sensitive information.

- Automating data pipelines and workflows to streamline data ingestion, processing, and distribution tasks.

- Utilizing Azure's analytics services, such as Azure Synapse Analytics, to provide insights and support data-driven decision-making.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=UGXMeO0_hbenxdDwAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXKQQrCMBBAUdz2CK5mLbUVwY2CUFBEpSB4gDKpQxpJZ0ImhdYreUnr5i8-L_susmP1GSLBCRPCma1joghreE6MQQkqRj8l1-r8bmJACWPbgTBcRKyn5aFLKei-LFV9YTXhjItW-lKYjIzlW4z-02iHkYLHRM12txmLwHaVP0iCJ7gLM7UJHEM99AZdDjXOHrVLEXO48svhD8T1q3iuAAAA&shmds=v1_AUFQtOMGTmLl0qngU1_f_VZ3iijzEFYELXeB6QnBNNIHWnbZqg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=UGXMeO0_hbenxdDwAAAAAA%3D%3D,23 days ago
Data Engineering Manager,Hewlett Packard Enterprise,"Pune, Maharashtra, India","Data Engineering Manager

This role has been designed as ‘Hybrid’ with an expectation that you will work on average 2-3 days per week from an HPE office.

Who We Are:

Hewlett Packard Enterprise is the global edge-to-cloud company advancing the way people live and work. We help companies connect, protect, analyze, and act on their data and applications wherever they live, from edge to cloud, so they can turn insights into outcomes at the speed required to thrive in today’s complex world. Our culture thrives on finding new and better ways to accelerate what’s next. We know diverse backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good. If you are looking to stretch and grow your career our culture will embrace you. Open up opportunities with HPE.

Job Description:

HPE Operations is our innovative IT services organization. It provides the expertise to advise, integrate, and accelerate our customers’ outcomes from their digital transformation. Our teams collaborate to transform insight into innovation. In today’s fast paced, hybrid IT world, being at business speed means overcoming IT complexity to match the speed of actions to the speed of opportunities. Deploy the right technology to respond quickly to market possibilities. Join us and redefine what’s next for you.

What you’ll do:

We are seeking a Data Engineering Manager to lead, manage, and deliver complex data engineering projects. This role requires a strong blend of technical expertise, project management skills, stakeholder collaboration, and leadership. The ideal candidate will ensure the successful delivery of data solutions aligned with business objectives, driving innovation and operational excellence across the organization. Also collaborate with cross-functional teams to understand data requirements and optimize systems for data analytics and machine learning applications.

Key Responsibilities:

Delivery Management: Plan, execute, and manage end-to-end delivery of data engineering projects, ensuring they meet quality, timeline, and budgetary requirements. Implement best practices in Agile, Scrum, or other relevant methodologies for iterative and efficient project delivery. Establish robust mechanisms to monitor project progress, identify risks, and ensure proactive resolution.

Technical Leadership: Provide guidance on the design, development, and deployment of scalable data pipelines, ETL/ELT processes, and data storage solutions. Oversee the integration of cloud-based or on-premises data platforms.

Team Leadership: Build, mentor, and lead a high-performing team of data engineers, fostering a culture of collaboration, innovation, and accountability. Conduct performance reviews, provide feedback, and develop professional growth plans for team members.

Stakeholder Engagement: Act as a primary point of contact for stakeholders, ensuring alignment of technical solutions with business needs. Translate business requirements into technical deliverables and prioritize tasks effectively. Collaborate with cross-functional teams, including product managers, data scientists, and analysts, to deliver cohesive solutions.

Technical Expertise:
• Architect and develop end-to-end data pipelines that efficiently ingest, process, and store structured and unstructured data from diverse sources.
• Leverage batch and real-time processing methods to handle various data flows, ensuring that both historical and streaming data are seamlessly integrated.
• Implement automated data validation and quality checks at each stage of the pipeline to ensure that data is accurate, complete, and consistent.
• Define and implement data transformation rules that standardize, clean, and enrich data to make it usable for analysis and reporting.
• Design efficient ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) processes that balance processing time with resource utilization.
• Set up monitoring and alerting systems to track the health and performance of data pipelines, ensuring that any failures or performance issues are quickly identified and resolved
• Collaborate with data architects and infrastructure teams to integrate data pipelines with other systems, ensuring seamless data movement across the organization.
• Ensure proper lineage and governance of the data as it flows through the pipeline, making it easier to track, audit, and comply with regulatory requirements.
• Develop comprehensive proofs of concept (POCs) for our core platforms, CDP and EDF, focusing on their ecosystem components. This will provide a solid foundation for most of our projects and accelerate initial setup and configuration.
• Design and implement reusable templates for common data pipeline patterns, covering ingestion, transformation, and loading processes. These templates will serve as starting points for new projects, reducing development time.

What you need to bring:
• The overall experience is between 10-15 years
• Bachelor's degree in Computer Science, Information Technology, or a related field.
• Proficiency in programming languages such as Python or Java.
• Extensive experience working with big data technologies such as Airflow, Hadoop, Spark, Kafka,
• Strong proficiency with SQL and NoSQL databases (e.g., MySQL, PostgreSQL, MongoDB, Cassandra, Cloudera).
• Experience in on prem data engineering open source tools.
• Strong experience in data modeling, data warehousing, and ETL frameworks.
• Familiarity with data governance practices, data privacy, and security standards.
• Experience with containerization and orchestration tools like Docker and Kubernetes.
• Good to have databricks and Snowflake experience
• Knowledge of CI/CD pipelines, version control systems, and agile methodologies.
• Good to have: Experience in building AI systems using data

Additional Skills:

Accountability, Accountability, Action Planning, Active Learning (Inactive), Active Listening, Bias, Business Growth, Business Planning, Cloud Computing, Cloud Migrations, Coaching, Commercial Acumen, Creativity, Critical Thinking, Cross-Functional Teamwork, Customer Experience Strategy, Data Analysis Management, Data Collection Management (Inactive), Data Controls, Design Thinking, Empathy, Follow-Through, Growth Mindset, Hybrid Clouds, Infrastructure as a Service (IaaS) {+ 10 more}

What We Can Offer You:

Health & Wellbeing

We strive to provide our team members and their loved ones with a comprehensive suite of benefits that supports their physical, financial and emotional wellbeing.

Personal & Professional Development

We also invest in your career because the better you are, the better we all are. We have specific programs catered to helping you reach any career goals you have — whether you want to become a knowledge expert in your field or apply your skills to another division.

Diversity, Inclusion & Belonging

We are unconditionally inclusive in the way we work and celebrate individual uniqueness. We know diverse backgrounds are valued and succeed here. We have the flexibility to manage our work and personal needs. We make bold moves, together, and are a force for good.

Let's Stay Connected:

Follow @HPECareers on Instagram to see the latest on people, culture and tech at HPE.

#india
#operations

Job:

Services

Job Level:

Manager_1

HPE is an Equal Employment Opportunity/ Veterans/Disabled/LGBT and Affirmative Action employer. We are committed to diversity and building a team that represents a variety of backgrounds, perspectives, and skills. We do not discriminate and all decisions we make are made on the basis of qualifications, merit, and business need. Our goal is to be one global diverse team that is representative of our customers, in an inclusive environment where we can continue to innovate and grow together. Please click here: Equal Employment Opportunity.

Hewlett Packard Enterprise is EEO F/M/Protected Veteran/ Individual with Disabilities.

HPE will comply with all applicable laws related to employer use of arrest and conviction records, including laws requiring employers to consider for employment qualified applicants with criminal histories.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=z2eTR3kcCcbcsqLvAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWOSwrCMBBAcesRXM1aaiOCLnSr-AGhNyjTdEiidSZkRuyJPKd183aP9-bf2dwd0RBOHBITlcQB7sgYqMAKbtKBEhYfQRjOImGgxSGaZd07pzrUQQ0t-drLywlTJ6N7SKd_tBqxUB7QqN1s12OdOSx3F_oMZAYN-ieWfgoblVySEiSG5s1UTQOTiRqtYAVX7hP-ABmR8_OpAAAA&shmds=v1_AUFQtOOLg-M-em1-JpXk2e29drTh1Ry9Aytn16MqJ-V40GfM5A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=z2eTR3kcCcbcsqLvAAAAAA%3D%3D,13 days ago
Software Engineer I - Data Engineer (BI),Uplight,"Maharashtra, India","The Position

We are seeking a seasoned engineer with a passion for changing the way millions of people save energy. You’ll work within the Engineering team to build and improve our platforms to deliver flexible and creative solutions to our utility partners and end users and help us achieve our ambitious goals for our business and the planet.

We are seeking a skilled and passionate Data Engineer - Business Intelligence with expertise in Data Engineering and BI Reporting to join our development team. As a Data Engineer, you will play a crucial role developing different components, harnessing the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, data processing and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets. You will also work on creating BI reports as well as development of a Business Intelligence platform that will enable users to create reports and dashboards based on their requirements. You will coordinate with the rest of the team working on different layers of the infrastructure. Therefore, a commitment to collaborative problem solving, sophisticated design, and quality product is important.

You will own the development and its quality independently and be responsible for high quality deliverables. And you will work with a great team with excellent benefits.

Responsibilities & Skills

You should:
• Be excited to work with talented, committed people in a fast-paced environment.
• Have a proven experience as a Data Engineer with a focus on BI reporting..
• Be designing, building, and maintaining high performance solutions with reusable, and reliable code.
• Use a rigorous approach for product improvement and customer satisfaction.
• Love developing great software as a seasoned product engineer.
• Be ready, able, and willing to jump onto a call with stakeholders to help solve problems.
• Be able to deliver against several initiatives simultaneously.
• Have a strong eye for detail and quality of code.
• Have an agile mindset.
• Have strong problem-solving skills and attention to detail.

Required Skills (Data Engineer):
• You ideally have 2+ or more years of professional experience.
• Design, build, and maintain scalable data pipelines and ETL processes to support business analytics and reporting needs.
• Strong Experience with SQL for querying and transforming large datasets, and optimizing query performance in relational databases.
• Proficiency in Python for building and automating data pipelines, ETL processes, and data integration workflows.
• Familiarity with big data frameworks such as Apache Spark or PySpark for distributed data processing.
• Strong Understanding of data modeling principles for building scalable and efficient data architectures (e.g., star schema, snowflake schema).
• Good to have experience with Databricks for managing and processing large datasets, implementing Delta Lake, and leveraging its collaborative environment.
• Knowledge of Google Cloud Platform (GCP) services like BigQuery, Dataflow, Pub/Sub, and Cloud Storage for end-to-end data engineering solutions.
• Familiarity with version control systems such as Git and CI/CD pipelines for managing code and deploying workflows.
• Awareness of data governance and security best practices, including access control, data masking, and compliance with industry standards.
• Exposure to monitoring and logging tools like Datadog, Cloud Logging, or ELK stack for maintaining pipeline reliability.
• Ability to understand business requirements and translate them into technical requirements.
• Inclination to design solutions for complex data problems.
• Ability to deliver against several initiatives simultaneously as a multiplier.
• Demonstrable experience with writing unit and functional tests.

Required Skills (BI Reporting):
• Strong experience in developing Business Intelligence reports and dashboards via tools such as Tableau, PowerBI, Sigma etc.
• Ability to analyse and deeply understand the data, relate it to the business application and derive meaningful insights from the data.

The following experiences are not required, but you'll stand out from other applicants if you have any of the following, in our order of importance:
• You are an experienced developer - a minimum of 2+ years of professional experience.
• Work experience & strong proficiency in Python, SQL and BI Reporting and its associated frameworks (like Flask, FastAPI etc.).
• Experience with cloud infrastructure like AWS/GCP or other cloud service provider experience
• CI/CD experience
• You are a Git guru and revel in collaborative workflows
• You work on the command line confidently and are familiar with all the goodies that the linux toolkit can provide
• Familiarity with Apache Spark and PySpark.

Qualifications
• Bachelor's or Master's degree in Computer Science, Engineering, or a related field.

Uplight provides equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type without regard to race (including hair texture and hairstyles), color, religion (including head coverings), age, sex, national origin, caste, disability status, genetics, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=YoU2AE2qscVjCdbsAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_0WNsQrCMBBAce0nCMKNKtqI4KKDIIpUcBLncm1jEol3IXdgf8Z_tU4ub3g8eMVnVOxv_NA3ZgsncoGszVDBEo6o-DfTQzUb5IUbEIu59cAEZ2YX7XjnVZNsjRGJpRNFDW3Z8ssw2YZ78-RGfqjFD5cUUW293qz6MpGbT-4pBucVAsEVhwDFa8YFVNQF_AL9Keh_oAAAAA&shmds=v1_AUFQtOO0ze7EDAMTdFBzQmqXkGGGJ52jmI6n6yMxC465EmyT_A&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=YoU2AE2qscVjCdbsAAAAAA%3D%3D,N/A
Big Data Engineer (Senior),Infogain,"Bengaluru, Karnataka, India","ROLES & RESPONSIBILITIES

Advanced knowledge of Azure data services such as Azure SQL Database, Azure Databricks, Azure Cosmos DB, and Azure Data Lake Storage. - Expertise in ETL (Extract, Transform, Load) processes and data integration. - Proficiency in SQL and database querying optimization. - Experience with data warehousing and data architecture.

Exposure on Spark mandatory

EXPERIENCE

6-8 Years

SKILLS

Primary Skill: Data Engineering

Sub Skill(s): Data Engineering

Additional Skill(s): Data Warehouse, Big Data, databricks, Apache Spark, Azure Datalake, Pyspark

ABOUT THE COMPANY

Infogain is a human-centered digital platform and software engineering company based out of Silicon Valley. We engineer business outcomes for Fortune 500 companies and digital natives in the technology, healthcare, insurance, travel, telecom, and retail & CPG industries using technologies such as cloud, microservices, automation, IoT, and artificial intelligence. We accelerate experience-led transformation in the delivery of digital platforms. Infogain is also a Microsoft (NASDAQ: MSFT) Gold Partner and Azure Expert Managed Services Provider (MSP).

Infogain, an Apax Funds portfolio company, has offices in California, Washington, Texas, the UK, the UAE, and Singapore, with delivery centers in Seattle, Houston, Austin, Kraków, Noida, Gurgaon, Mumbai, Pune, and Bengaluru.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=E5SSdz8mrCDncx3rAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXOPQrCQBBAYWxzBKvp_CFmVbDRLigSLT1AmMRxsrrOhJ0N5CreVoXHV7_sM8k2pWc4YkI4CXshijC_kXiNC1jBRRswwth2oAJnVQ40PXQp9bZ3ziwUbAmTb4tW306FGh3dUxv7U1uHkfqAiertbj0WvfByVslDGb3Ar5KEMQxxyOGKUX4XL8yhkrvHL8DHwlmcAAAA&shmds=v1_AUFQtONLF8qTyi0llB8mMMWwKa0s5ssohuoZzH1oeut1PFj1PQ&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=E5SSdz8mrCDncx3rAAAAAA%3D%3D,13 days ago
KGeN - Data Engineer - ETL,KGEN,"Karnataka, India (+1 other)","Company Description

KGeN (Kratos Gamer Network) is building a multi-chain gamer network that aims to become the home of global gamer data, owned by gamers. At the heart of it is the revolutionary Proof of Gamer (P.O.G) Engine that allows gamers to build, own & monetise their reputation. This data framework allows game publishers to engage with precise gamer cohorts, engaging in deep value exchange through our product stack.

Role Description

Join our dynamic and innovative gaming company, where we are committed to creating immersive experiences for players around the world. As we continue to grow, we are seeking a talented AWS Data Engineer to join our team and help us leverage cutting-edge cloud technologies to drive our data initiatives.

Key Responsibilities
• Design, develop, and maintain scalable ETL processes using AWS Glue.
• Create and manage data lakes on AWS S3 to store structured and unstructured data.
• Utilize AWS Athena for ad-hoc querying and data analysis.
• Implement and manage AWS Lambda functions for data processing and automation.
• Develop data processing workflows using PySpark.
• Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
• Monitor and optimize data pipelines for performance and cost-efficiency.
• Ensure data quality and integrity through rigorous testing and validation processes.
• Implement data security best practices to protect sensitive information.
• Stay up-to-date with the latest trends and best practices in cloud data engineering and gaming industry.

Qualifications
• Bachelor's degree in Computer Science, Information Technology, or a related field.
• 3+ years of experience as a Data Engineer, preferably in a AWS cloud environment.
• Proficiency in AWS services, particularly AWS Glue, Athena, S3, Lambda, and PySpark.
• Strong SQL skills and experience with relational databases.
• Experience with data modeling, ETL processes, and data warehousing concepts.
• Proficiency in programming languages such as Python.
• Familiarity with data visualization tools like AWS QuickSight or Tableau is a plus.
• Strong problem-solving skills and attention to detail.
• Excellent communication and collaboration skills.
• Experience in the gaming industry is a plus but not required.

(ref:hirist.tech)",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=7geGt3_eZfvnbIagAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNsQrCMBCAYVz7BOJ0s2ijgouulqCRTu7lEo80Wu9KLkOfwae2Lh_80199F9XeWWphCxcsCA3HxER57uZxn72JByXMoQdhsCJxoNW5L2XUkzGqQx21YEmhDvIxwuRlMi_x-qfTHjONAxbqDsfdVI8c10tnmxYSg8PM8_ONG7jyM-EP5XKSTY0AAAA&shmds=v1_AUFQtOO4EHOjHTepZFhJORaun3kPCQi2AQ7WA2js_LAekOSuZw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=7geGt3_eZfvnbIagAAAAAA%3D%3D,9 days ago
Snowflake Developer/Data Engineer,Ace Technologies,Anywhere,"Ace Technologies, a leading recruitment firm, is dedicated to sourcing exceptional talent for remote positions. Our focus spans various job domains, striving to connect top-tier professionals with our esteemed partners. Leveraging a range of scientifically designed assessments, inspired by extensive research on high-performance indicators, we meticulously identify outstanding candidates.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=yI4srl1nhv3KDf8_AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43izYiCKKTUBEdRFDnck3PJBrvQi5oP8nPFN_wqu-oWl1YPveIT4KG3hQlUTYNFoQdu8BEGWZwlA6UMFsPwrAXcZHGG19K0rUxqrF2WrAEW1t5GWHqZDAP6fRfqx4zpYiF2sVyPtSJ3cRsLcGVrGeJ4gIpBIaThB6ncCsFM5wz9qR-CgfuA_4A7sudk6sAAAA&shmds=v1_AUFQtOPUhKo4JKzARFeuaL0e4B1hs4TCNGeRtB0qvOkigLr2YA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=yI4srl1nhv3KDf8_AAAAAA%3D%3D,N/A
SDE 1 (Data Engineer),PhysicsWallah,"Noida, Uttar Pradesh, India","Requirements:
• Can Develop ETL (Extract, Transform, Load) processes to move and transform data between systems.
• Have good hands-on experience in any coding language Python or Scala.
• Good understanding of Data Engineering concepts such as data warehousing, datalakes, data modeling, data quality assurance, data lifecycle management, streaming data, metadata management, etc.
• Strong hold on cloud-based ecosystems(AWS preferred).
• Have an understanding of data pipeline orchestration tools such as Airflow or Azkaban (Airflow Preferable).
• Can work with spark, EMR, and Kubernetes cluster.
• Can write spark jobs, shell scripts, and optimized data pipelines.
• Experience with relational SQL and NoSQL databases, including Postgres, Amazon Redshift, and MongoDB.
• Have an understanding of OOPS-based data platforms, and can develop data platform modules.
• Good understanding of backend APIs and their use cases.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=uf7ARhRhH2nGwuDmAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXEsQrCMBAAUFz7CU43qsRGBRddW0QHEUQcyzU9kkjMldwN9VP8W_ENr_rOKnNvWtjCokFFaLOPmagsYQ0X7kEIiwvAGU7MPtH8GFRHOVgrkmovihpd7fhtOVPPk31xL_86CVhoTKjU7fabqR6zX5lb-Eh08sSUMEDMcOU4oIGHKha4FRxIgoFzHiL-AHMEngWcAAAA&shmds=v1_AUFQtOMVxxzINo2-lHrkkx5zYv-vwbVEjyv5WyiOpKwisDXvyg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=uf7ARhRhH2nGwuDmAAAAAA%3D%3D,N/A
"Data engineer(Databricks,pyspark, ADF,SQL)",Risk Resources,"Bhubaneswar, Odisha, India","Overview: The role of a Data Engineer is pivotal in transforming raw data into actionable insights that drive strategic decision-making processes. By leveraging advanced tools and technologies, the Data Engineer collaborates with data scientists, analysts, and IT teams to integrate, manage, and optimize data pipelines. With a focus on ensuring data integrity, scalability, and reliability, this position plays a critical role in supporting analytics and business intelligence initiatives across the organization. Utilizing tools such as Databricks, PySpark, Azure Data Factory (ADF), and SQL, the Data Engineer is responsible for constructing and maintaining the architecture necessary for large-scale data processing and analytics. This role requires a strong analytical mindset and the ability to work in a fast-paced environment with evolving technologies. Overall, the Data Engineer contributes significantly to the organizations ability to leverage data for competitive advantage, making it an integral part of our data-driven culture.

Key Responsibilities
Design, develop, and maintain robust data pipelines using Databricks and PySpark.
Implement data integration solutions utilizing Azure Data Factory.
Create and optimize SQL queries for data extraction and manipulation.
Collaborate with data scientists to understand data requirements for analytical projects.
Ensure data quality and integrity throughout the ETL process.
Monitor and troubleshoot data pipeline issues to ensure high availability and performance.
Develop and maintain data architecture and data modeling standards.
Manage version control and documentation of data workflows.
Participate in data governance initiatives to regulate data access and usage.
Work with cloud services to manage and scale data storage and processing capabilities.
Conduct performance tuning and optimization of data processes.
Stay updated on industry trends and emerging technologies in data engineering.
Engage in regular communication with stakeholders regarding data initiatives.
Implement best practices for data management and engineering.
Support ad-hoc data requests and reporting needs.

Required Qualifications
Bachelors degree in Computer Science, Engineering, or a related field.
5+ years of experience in data engineering or related roles.
Proficiency in Databricks and PySpark effectively.
Strong experience with Azure Data Factory (ADF).
Expertise in SQL and relational database management systems.
Hands-on experience with data warehousing concepts and solutions.
Experience with ETL processes and data integration tools.
Familiarity with big data technologies like Hadoop or Spark.
Knowledge of data modeling techniques and best practices.
Understanding of data governance and compliance standards.
Ability to work with cross-functional teams and communicate effectively.
Strong problem-solving skills and analytical thinking.
Experience in cloud computing platforms, especially Azure.
Familiarity with Python or other programming languages is a plus.
Ability to manage multiple tasks and meet deadlines.

Skills: python,data warehousing,cloud computing,azure data factory (adf),pyspark,cloud,databricks,data engineering,big data technologies,adf,etl processes,sql,data governance,data modeling,",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=ceLY1V-aFTaU_BRxAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLQQrCMBBAUdz2CK5mqRJbEdzoqlIURRDrAcokHdrYmgmZFOulPKO6-fAWP_lMkrzAiECusY4ozP7SwZpOlH-Lx9ApyIuDut8uc1jCmTUIYTAtsIMjc9PTdNfG6GWbZSJ92kjEaE1q-JmxI81j9mAt_1TSYiDfY6RqvVmNqXfNQpVWOihJeAiGBKyDfTtodCQvDAqutf1tCk6utvgFHCHtybEAAAA&shmds=v1_AUFQtOMG7-0ujVjHgDbtZM9tTECobupMJUbM-Kr8AK4HKHKBlw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=ceLY1V-aFTaU_BRxAAAAAA%3D%3D,29 days ago
Sr/ Staff/Sr. Staff/Principal Engineer(Python | Spark | BigData | Kubernetes |SQL),Zscaler,India,"About Zscaler

Zscaler (NASDAQ: ZS) accelerates digital transformation so that customers can be more agile, efficient, resilient, and secure. The Zscaler Zero Trust Exchange is the company’s cloud-native platform that protects thousands of customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.

With more than 10 years of experience developing, operating, and scaling the cloud, Zscaler serves thousands of enterprise customers around the world, including 450 of the Forbes Global 2000 organizations. In addition to protecting customers from damaging threats, such as ransomware and data exfiltration, it helps them slash costs, reduce complexity, and improve the user experience by eliminating stacks of latency-creating gateway appliances.

Zscaler was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. Zscaler’s purpose-built security platform puts a company’s defenses and controls where the connections occur—the internet—so that every connection is fast and secure, no matter how or where users connect or where their applications and workloads reside.

The ZPA Advanced Analytics team at Zscaler is looking for a Bigdata/Spark Engineer who will help us in the analysis of vast amounts of data, write production grade pipelines, and infrastructure optimization.

What You Will Do

You will be responsible for product features related to data transformations, enrichment, and analytics

You will discuss with product managers, UI, and other backend teams to understand requirements and translate the same into functional specifications

You will seize every opportunity to refactor code in the interests of maintainability and reusability

Lead and provide technical direction and mentorship to junior engineers and bring out the best in them

The ideal candidate has a proven track record of building large scale enterprise products, and is a creative thinker, problem solver, learner, and a fantastic manager of people, and is motivated with engineering excellence.

Responsibilities
• Data mining using state-of-the-art methods
• Enhancing data collection procedures to include information that is relevant for building analytic systems
• Processing, cleansing, and verifying the integrity of data used for analysis
• Ad-hoc analysis and presenting your results in a clear manner
• Creating automated anomaly detection systems and constant tracking of its performance
• Skills and Qualifications
• BTech/MTech in CSE or relevant field from a reputed university.

5+ years of experience in developing and deploying Big data solutions Experience with Python, PySpark (PySparkSQL), Spark (SparkSQL), SQL Experience with Data visualization tools / Data visualization algorithms
• Understanding and sense of data-warehousing and data-modeling techniques
• Strong understanding of distributed systems, real-time analytics
• Experience with stream-processing technologies like Apache Kafka and Apache Flink
• Skill of self/unit testing and feature verification
• Experience with data engineering tasks or working with data engineers
• Good interpersonal skills and positive attitude
• Experience with cloud environments AWS/Azure/GCP

Experience with Kubernetes, Dockers Basic knowledge of Java and Microservices
• (Plus+ and optional) Pipeline orchestration tool Kubeflow
• (Plus+ and optional) basic ML/AL knowledge and experience working with ML engineers or Data scientists

#LI-MS6

By applying for this role, you adhere to applicable laws, regulations, and Zscaler policies, including those related to security and privacy standards and guidelines.

Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws.

See more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link.

Pay Transparency

Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, click here .

Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=IzfGwdr7oq3671CHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_y2LsWoCQRBASWuV2mrKRPA2BNIknSiiplC2S3PMbca91XV2mRnBwH1Zvs4LpHnvNW_y-zBJXhx4w-PReWn-ay-JQ6qYYcUxMZE87X-sLwwD-IpyHr1IcYmGY-2uHQmTkcLgD5_PMIdt6UAJJfQwTutSYqbpR29W9d051dxENbQUmlAurjB15eZOpdM_tNqjUM1o1L6-vdyaynH2-KUBMwkkhg1_J7wD6W5dl70AAAA&shmds=v1_AUFQtOOQpb9H9Zz3EyjBAyMB4RSE23PKRtLfYIpjYOeCXn7Pjw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=IzfGwdr7oq3671CHAAAAAA%3D%3D,N/A
Data Engineer ( Apply only if interested in H1B visa for USA ),ShyraTech LLC,"Panchkula, Haryana, India","Urgent hiring for Data Engineer for our USA based client ( Apply only if interested in H1B visa petition )

Data Engineer -

Location :- Remote

Expertise:

· 5-9+ years of relevant industry experience with a BS/Masters, or 2+ years with a PhD

· Experience with distributed processing technologies and frameworks, such as Hadoop, Spark, Kafka, and distributed storage systems (e.g., HDFS, S3)

· Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions

· Expertise with ETL schedulers such as Apache Airflow, Luigi, Oozie, AWS Glue or similar frameworks

· Solid understanding of data warehousing concepts and hands-on experience with relational databases (e.g., PostgreSQL, MySQL) and columnar databases (e.g., Redshift, BigQuery, HBase, ClickHouse)

· Excellent written and verbal communication skills

A Typical Day:

· Design, build, and maintain robust and efficient data pipelines that collect, process, and store data from various sources, including user interactions, financial details, and external data feeds.

· Develop data models that enable the efficient analysis and manipulation of data for merchandising optimization. Ensure data quality, consistency, and accuracy.

· Build scalable data pipelines (SparkSQL & Scala) leveraging Airflow scheduler/executor framework

· Collaborate with cross-functional teams, including Data Scientists, Product Managers, and Software Engineers, to define data requirements, and deliver data solutions that drive merchandising and sales improvements.

· Contribute to the broader Data Engineering community at Airbnb to influence tooling and standards to improve culture and productivity

· Improve code and data quality by leveraging and contributing to internal tools to automatically detect and mitigate issues.

· Skill Sets - Python, SQL (expert level), Spark and Scala (intermediate).

Best & Regards

Sofiya

sofiya@shyratech.com

www.shyratech.com",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=rJGq69amu6Nagf9EAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNyw4BQRBFY-sTrGqJMIPEhpVXPGIhwVpqRpluWlWnq4n5O5-mbU7O5p7b_Daa5yVGhBVXlokCtGHmvatBOMHewHKkQBrpmhQ2wzm8rSLcJMD5OIMO9GEnBShhKE1awVqkctSamhi9TvJc1WWVRoy2zEp55sJUyCe_S6F_XNRgIO8w0mU0Hnwyz1W3ezR1wBOl4n6_-B8fkEvzeDnswQZDjZxky1eLPyn02NDDAAAA&shmds=v1_AUFQtOPfQU0RWXPYXWee8Bo3xW3JraXxTI9jYE9Fdk1o8E6Wiw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=rJGq69amu6Nagf9EAAAAAA%3D%3D,6 days ago
Azure Cloud Data Engineer,Shifastar,"Pune, Maharashtra, India","We are seeking a highly skilled Azure Cloud Data Engineer with strong experience in Azure Synapse Analytics, Azure Kubernetes Service (AKS), Azure Databricks, and Delta Lake.

The ideal candidate will be responsible for designing, implementing, and optimizing cloud-based data solutions that enable large-scale data analytics and machine learning workloads.

This role requires a deep understanding of Azure cloud technologies and a passion for building scalable and efficient data pipelines.

We are looking for a seasonal Azure Synapse Cloud Engineer to Develop data ingestion & enrichment and model development/execution on Azure cloud

Help us build state of the art solutions that underpin monitoring & surveillance models and alert generation using cutting edge technologies like Scala, Apache spark, Azure Data Lake, Databricks, SQL etc.

Demonstrate superior problem solving skills.

Demonstrate superior collaboration skills in working closely with other development, testing and implementation teams to roll-out important regulatory and business improvement programs.

Key Responsibilities:
• Design and implement end-to-end data pipelines using Azure Synapse, Azure Databricks, and Delta Lake.
• Develop and manage Azure Data Lake, Azure Synapse Analytics, and related services to support data integration and analytics.
• Work with Azure Kubernetes Service (AKS) to deploy, scale, and manage containerized data applications and services.
• Build and maintain data models, data warehousing solutions, and data integration frameworks.
• Implement and optimize ETL processes for structured and unstructured data.
• Collaborate with data scientists, analysts, and other stakeholders to support business analytics and machine learning workflows.
• Ensure high availability, scalability, and security of cloud data solutions.
• Troubleshoot and resolve issues in production environments, ensuring data integrity and performance.
• Write efficient, reusable code for data processing and automation using Python, Scala, or similar languages.
• Manage version control and continuous integration processes for data pipelines.
• Optimize data storage and retrieval to reduce costs and improve performance, particularly within Delta Lake environments.
• Stay current with new Azure services and tools, and continuously innovate to improve cloud-based data systems.

Required Skills and Experience:
• Proven experience as a Data Engineer or Cloud Data Engineer with expertise in Azure Cloud technologies.
• Strong hands-on experience with Azure Synapse Analytics for big data processing and data warehousing solutions.
• Experience with Azure Databricks for developing data pipelines and performing advanced analytics.
• Proficiency in working with Delta Lake for efficient data storage, versioning, and management on Azure.
• Experience with Azure Kubernetes Service (AKS), including container orchestration and deployment for data pipelines and applications.
• Solid understanding of data modeling, ETL processes, and data warehousing concepts.
• Strong programming skills in languages such as Python, Scala, or SQL for data manipulation, pipeline orchestration, and automation.
• Experience with data storage solutions such as Azure Data Lake, Azure Blob Storage, and Azure SQL Database.
• Familiarity with data processing frameworks like Apache Spark and Apache Kafka.
• Experience with monitoring, logging, and troubleshooting data solutions in cloud environments
• Knowledge of best practices for cloud security, compliance, and data privacy.
• Excellent problem-solving and communication skills, with the ability to collaborate across teams.

Mandatory Skills

Azure synapse, Azure AKS, DataBricks, Delta Lake. SQL and Oracle Pl/SQL Queries questions. And Python will be a big plus.

Job Location: Pune

Education: • B.E /B Tech / M Tech /B.Sc Comp/ M.Sc Comp

Experience: 5 to 10 years",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=XLJZT_eP5Bmvb20wAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXLsQrCMBCAYVz7CE63uEhNiuCik6iIgiD4AOXankkk3pVcCsUH8Xmtyz99f_GdFdX-MySCQ5ShgyNmhBO7wEQJVnCVBpQwtR6E4SziIs13Pudet9aqRuM0Yw6taeVthamR0b6k0X9q9Zioj5ipXm-q0fTslouHD0-cpgSB4T4wlXDDCaL6nLCEC3cBf6K4LiqZAAAA&shmds=v1_AUFQtONeuIKzL54OdGh_ORsJJs0ikPWL1Gw8Q0xklJ9-tVswMg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=XLJZT_eP5Bmvb20wAAAAAA%3D%3D,6 days ago
Big Data Engineer ( 6 - 9 years of experience only ),Hexaware Technologies,"Mount Abu, Rajasthan, India","Location : Chennai , Bengaluru , Mumbai , Pune

Job Description
• Solid Hands-on experience with Azure Databricks - Pyspark coding and Spark SQL coding - Must have
• Solid Hands-on experience with Delta Lake House with Delta tables - Must have
• Solid Hands-on experience with Data warehousing - Must have
• Solid Hands-on experience with Azure Data Factory - Must have
• CI / CD Pipelines Integrations - Must have
• 7+ Years of experience with Data Project
• London Insurance market data experience",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=zYXmnWujK1FU6CUMAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWKvW7CQBAGRetHoPpKQMSOEEGCVImC8oNoUHprfSx3hy67lvesmLfLo8U0U8xM8TcpDq_R440yYS8-CnOHGTZ4wBY3ps6gF_DQchdZHEMl3TAf85c2sHFwYXR4V_WJp88h59Z2VWWWSm-ZcnSl059KhRsdqqs2dkdtgTpuE2WuV0-PQ9mKX6w_eKDf0eObXRBN6iMbouCovWS8NP0SJ7qS5UCyxKecI_0DQkWlTsMAAAA&shmds=v1_AUFQtOONa1b2SUhHgFKg3buKg_k5bJcsNdmPjLsbufjmR_z0_Q&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=zYXmnWujK1FU6CUMAAAAAA%3D%3D,20 days ago
"Data Scientist, Amazon Prime India, Amazon",Amazon,"Bengaluru, Karnataka, India",Amazon Prime India is seeking a Data Scientist to lead business intelligence and drive member insights through analytics and machine learning.,https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=6l2Q7KRPvM7BERvzAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_z2OMQrCQBBFsc0RrKaxkZgVwUariCBqI3iAMFmHzepmNuxMIHghr2kUsfnwf_H-y16TrNyjIlytJ1YvmkPZ4jMyXJJvCY588_jfFnCKNQhhsg2M_RCjCzTdNqqdbIwRCYUTRfW2sLE1kamOg7nHWj5RSYOJuoBK1Wq9HIqO3Xz2Y3uGHbHD0Kc-hzMmHr0e4_dX4Q3W3tWCqgAAAA&shmds=v1_AUFQtOP1dBZZf0mMUl7kEfwuICTRk-ImLaEG54H5iFSZaoNQVg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=6l2Q7KRPvM7BERvzAAAAAA%3D%3D,N/A
Data Engineer  Snowflake,Nice software solutions,"Nagpur, Maharashtra, India","Full Time

Rotational Shift

Job Location: Nagpur / Pune

No. Of Positions: 3

Experience Required: 5 to 7 years

Job Purpose:
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and datascientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate willbe excited by the prospect of optimizing or even re-designing our companys data architecture to support our next generation of products and data initiatives.

Snowflake Data Engineer
Must Have: SQL, Python, Snowflake, Data Modelling, ETL, Snowpark
Good to Have: DBT (Data Build Tool), API Integration (AWS Lambda), Git, AWS S3 Integration

Knowledge, Skills and Experience:
Proficiency in crafting and optimizing complex SQL queries and Stored Procedures for data transformation, aggregation, and analysis within the Snowflake platform.
Experience with Snowflake cloud data warehousing service, including data loading, querying, and administration.
In-depth understanding of ETL processes and methodologies, leveraging Snowflake's capabilities.
Familiarity with DBT (Data Build Tool) for data transformation and modeling within Snowflake.
Expertise in integrating Snowflake with AWS S3 storage for data storage and retrieval.
Proficiency in Snowpark, enabling data processing using Snowflake's native programming language.
Skill in API integration, specifically integrating Snowflake with AWS Lambda for data workflows.
Adeptness in version control using GitHub for collaborative code management.
Adeptness in troubleshooting data-related issues within the Snowflake ecosystem, ensuring data quality and consistency.
Skill in creating clear and concise technical documentation, facilitating communication and knowledge sharing.
Designing efficient and well-structured data schemas within Snowflake.
Utilizing Snowflake's features for data warehousing, scalability, and performance optimization.
Leveraging Python programming for data processing, manipulation, and analysis in a Snowflake environment.
Implementing data integration and transformation workflows using DBT.
Writing and maintaining scripts for data movement and processing, using cloud integration.

We are on the lookout for dynamic individuals that bring energy and passion to their work, just like us. As

an innovation-driven organization, we offer high-impact careers and growth opportunities across global

locations. Our collaborative work environment is designed to help NICE thrive, learn, and grow through

targeted learning and development programs as well as generous benefits and perks.,",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=uYPoNlT7VQ-OHQ3wAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWLQQrCQAwA8doneMpZaleEXvSqiIK9-ICS1nS7uiZlk9J-yH9aL8PAMNl3lbkTGsKZfWCiBPBgmbqIb4It3KQBJUxtD8JwEfGR1sfebNCDc6qx8GpooS1a-ThhamR2L2n0j1p7TDRENKr35W4uBvabsgotgUpn0xIXiaMFYYXAUKEfxpTDHZcRtbeEOVz5GfAHLF-uo6gAAAA&shmds=v1_AUFQtOOkdbGrgeASyxRwbRABeJHIwLFqHLFREyQLFPwaQih7Qw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=uYPoNlT7VQ-OHQ3wAAAAAA%3D%3D,19 days ago
Data Engineer Deputy Manager,Philips,"Tamil Nadu, India","Job Title
Data Engineer Deputy Manager

Job Description

Job title:
Data Engineer Deputy Manager

Your role:

The Data Engineering manager needs to be well versed with Microsoft Business Intelligence stack having strong skills and experience in development and implementation of BI and advanced analytics solutions as per business requirements.
• Strong hands-on experience in Microsoft ADF pipelines, databricks notebooks, Pyspark.
• Adept in design and development of data flows using ADF.
• Expertise in implementing complex ETL logics through databricks notebooks
• Experience in implementing CI-CD pipelines through azure devops
• Experience in writing complex SQLs
• Experience in handling MDX cubes, SSIS packages
• Understand the business requirements and develop data models accordingl
• Have knowledge and experience in prototyping, designing, and requirement analysis.
• Excellent knowledge in data usage, Scheduling, Data Refresh and diagnostics
• Experience in tools such as Microsoft Azure, SQL data warehouse, Visual Studio, etc.
• Worked in an agile (scrum) environment with globally distributed teams
• Analytical bent of mind
• Business acumen and articulation skills; ability to capture business needs and translate into a solution
• Ability to manage interaction with business stakeholders and others within the organization
• Good communication and documentation skills
• Proven experience in interfacing with different source systems
• Proven experience in data modelling

How we work together
We believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.
Onsite roles require full-time presence in the company’s facilities.
Field roles are most effectively done outside of the company’s main facilities, generally at the customers’ or suppliers’ locations.

Indicate if this role is an office/field/onsite role.

About Philips
We are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.
• Learn more about our business.
• Discover our rich and exciting history.
• Learn more about our purpose.

If you’re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our commitment to diversity and inclusion here.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=n9G0UWDM6RZU--E9AAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXHsQrCQAwAUFz7CeKQWbQniouuFVFQHNxL2oa7yDU5mivUv_CT1eXBKz6zYldhRjiJZyEaoKI05jfcUND_uoarNmCEQxtABc6qPtL8GHJOdnDOLJbeMmZuy1Z7p0KNTu6ljf2pLeBAKWKmervfTGUSv1w8AkdOBizwxJ4j3LEbV3CRjvELAKWwv5MAAAA&shmds=v1_AUFQtOMUVyFjWEqhXopU6sth7lSU_hKSvQSKs7A2A5AtX5fIrg&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=n9G0UWDM6RZU--E9AAAAAA%3D%3D,N/A
Lead Software Engineer (Snowflake Data Platform enablement on AWS),Mastercard,"Pune, Maharashtra, India","Our Purpose

Mastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we’re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.

Title and Summary

Lead Software Engineer (Snowflake Data Platform enablement on AWS)

Overview

The Data & Analytics team is looking for a Cloud Big Data Lead Engineer to drive our mission to unlock potential of data assets on AWS cloud by consistently innovating, and moving data to our Cloud platforms and making our business engineering team consume it with the set standards and principles. Our Cloud Data Platforms (Snowflake and Databricks) are being used by various groups to derive insights, perform Machine Learning and Data Science activities which in turn supports various revenue generating initiatives across the organization.

Role

• Design and develop high quality, secure, scalable software solutions based on technical requirements specifications and design artifacts within expected time and budget
• Develop and build solutions on cloud data platform Snowflake, adhering to industry standards.
• Build governance framework for cloud data storage and usage for enterprise data warehouse.
• Collaborate on cross-functional agile teams that include Front-end Developers, Report/BI Developers and Product Owners
• Work closely with Product Owners to understand cloud use cases and determine the best way to implement those.
• Provide technology leadership in the cloud big data space to development and product teams.
• Provide technology leadership to other leaders in big data and cloud data platform space.
• Stay abreast of Analytics technology trends and industry best practices to hone and maintain your talent
• Participate in architectural discussions , iteration planning, and feature sizing meetings
• Adhere to Agile processes and participate actively in agile ceremonies
• Stakeholder management skills

All About You

Experience managing, developing and supporting warehouse applications on Snowflake.

•Experience working with development/engineering teams in a global environment
•Hands on experience with Cloud data platform - Snowflake
•In depth practical experience on cloud technology stack - AWS.
•Strong understanding of cloud devops practices and implementation.
•Experience in cloud data migration for large sized enterprise data warehouse environments.
•Understanding of cloud data governance include compute optimization, cost control, user profile management etc.
•In depth understanding of cloud operations strategy and user management across various cloud providers and cloud platforms.
•Strong understanding and hands-on on cloud security models, encryption strategy, network layers for on-prem + cloud hybrid model and other related concepts.
•Experience in establishing new engineering team and working towards taking the team to steady state
•Understanding of Exadata/Oracle/Netezza/SQL server databases
•Understanding of SDLC and experience in establishing processes, standards and governance to bring efficiency within development team
•In-depth understanding of Business Intelligence solutions for customers
•Experience in AGILE methodology
•Experience in working in global teams and should be technically strong

Corporate Security Responsibility

All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
• Abide by Mastercard’s security policies and practices;
• Ensure the confidentiality and integrity of the information being accessed;
• Report any suspected information security violation or breach, and
• Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=NRb87d8mqQ8-XlHbAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xWNvQrCYAwAcfURnDKJirYiuOgkKKIoFDqIU0nb9Ee_JuVLxD6ej2a73HAc3Pg3Gj9vhDnEUtgXPcGJy5qJPMxilm_h8E1wREOIHFohvgFiTB01xAbCcHjEc1jBVVJQQp9VgzyLlI4m-8qs1V0YqrqgVEOrsyCTJhSmVLrwJakOSLTq120_oGSzXXdBy-Viekc18hn6HGqG6MO0hDv2JWplHpdw4bzGPyIna7XDAAAA&shmds=v1_AUFQtOP7PDO4MvEDob1kU1UCMbJpUGc1o6fdrW5P4OHn1ifNkA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=NRb87d8mqQ8-XlHbAAAAAA%3D%3D,8 days ago
SQL Server Data Engineers,Axcess.io,"Bengaluru, Karnataka, India","About us

We offer Cloud Managed Services and DevOps Automation to improve the agility and velocity of businesses.

We are a cloud-native services company with clients in multiple geographies and industries. We specialize in cloud transformation, DevOps automation, managed services and cloud-native application development. With offices in USA and India, we empower organizations to transform and secure their IT infrastructure, scale up their operations, and manage customer workloads from ISO 27000-certified Global Network Operations Centers (GNOC).

Job Description

The SQL Server Data Engineer will be responsible for the development and sustainment of the SQL Server Warehouse, ensuring its operational readiness (security, health and performance), executing data loads, and performing data modeling in support of multiple development teams. The data warehouse supports an enterprise application suite of program management tools. Must be capable of working independently and collaboratively.

Responsibilities:
• Manage SQL Server databases through multiple product lifecycle environments, from development to mission-critical production systems.

Troubleshoot issues related to memory-optimized tables in MS SQL Server 2019
• Configure and maintain database servers and processes, including monitoring of system health and performance, to ensure high levels of performance, availability, and security.
• Apply data modeling techniques to ensure development and implementation support efforts meet integration and performance expectations
• Independently analyze, solve, and correct issues in real time, providing problem resolution end-to-end.
• Assist developers with complex query tuning and schema refinement.

Requirements

The ideal candidates must have:
• Minimum 5 years of relevant experience in data modelling, database & datawarehouse management, performance tuning on Microsoft SQl server Databases
• 3+ years of experience with programming and database scripting; and

experience with clustered configurations, redundancy, backup and recovery
• Good knowledge of SSIS development, T-SQL, Views, Stored Procedures, Functions and Triggers.
• Experience in AWS and Azure Cloud will be an added advantage

Benefits

As per market standards

Job Information

Job Title

SQL Server Data Engineers

Industry

IT Services

Work Experience

5+ years

City

Bengaluru

State/Province

Karnataka

Country

India

Zip/Postal Code

560102",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=Y3O1tTLAahZJXfsGAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNvQrCMBAAYFz7CE43itSkCC46KYr4s0gfoFzjkUbjXcml0lfxbdXlW7_iMymq-naFmtKbEuwxIxzYByZKCgs4SwtKmFwHwnAU8ZGmmy7nXtfWqkbjNWMOzjh5WWFqZbQPafVPox0m6iNmaparajQ9-_lsOzpSNUEgMOyIPcYhDSVcMPGvf2IJJ74H_AL3cg8pnAAAAA&shmds=v1_AUFQtOMwYu-C-CMSEovaS1GwJhBT1Ejl4hPEZSAWRgEsl0qEIA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=Y3O1tTLAahZJXfsGAAAAAA%3D%3D,11 days ago
"Braincranx IT solutions Looking for Cloud Data Engineer for IT Data Analytics Team at Pune, Mah[...]",Braincranx IT solutions,Anywhere,"Job Title: Cloud Data Engineer for IT Data Analytics Team
Type: Full Time / Permanent Role
Work from home

Experience: 6+ years

Timezone: IST
Job Description
• Act as a subject matter expert in data engineering and GCP data technologies.
• Closely work with various business teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
• Work with Agile and DevOps techniques and implementation approaches in the delivery.
• Showcase your GCP Data engineering experience when communicating with business teams on their requirements, turning these into technical data solutions.
• Build and deliver Data solutions using GCP products and offerings.
Qualifications
• Liaise and be part of our extensive GCP community, contributing to the knowledge exchange learning programme of the platform.
• Any Bachelor Degree in Computer Science or related fields.
• Minimum 5 years of experience as a data engineer in a banking environment.
• Possess analytical skills, mental resilience, and the ability to think systematically under stressful conditions.
• Highly accountable and takes ownership. Outstanding work ethic, high integrity, team player, and a lifelong learner.
• Mentor other engineers to define our technical culture and help build a fast-growing team.
Skills
• Hands-on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc, AI Building Blocks, Looker, Cloud Data Fusion, Dataprep, etc.).
• Experience in Spark/Scala/Python/Java/Kafka.
• Experience in MDM, Metadata Management, Data Quality, and Data Lineage tools.
• E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.
• Regulatory and Compliance work in Data Management.
• E2E Solution Design skills – Prototyping, Usability testing, and data visualization literacy.
• Experience with SQL and NoSQL modern data stores.
• Work on diversified cloud platforms consisting of Databases and CICD, Logging, and monitoring tools to provide stable, and reliable DevOps service.
• Hands-on experience with terraform is a plus.
• Build CI/CD pipeline; both design and implementation is an added advantage.
Job Types

Full-time, Regular / Permanent
Benefits
• Cell phone reimbursement
• Health insurance
• Internet reimbursement
• Life insurance
• Work from home
Schedule
• Day shift
• Monday to Friday
Supplemental Pay
Experience
• Total work: 6 years (Required)
• Data Developers: 6 years (Required)",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=cjaOBtTM3pCNbFBHAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_3WOMUsDQRCFsc1PsHq1xL0QsdFKjYQEBYt0ImHuMu5u3MwcO3Nw4o_2LxjTWNm84oP3vTf5Ppt83VfK0lWSEasNTMvgWcXwpPqRJeJdKx6KDjssyAmPErMw1xM_Fk7wTqh8eu4MG6YDyPEyCE_xTOk1hPCGS6y1hTHVLkEFS9VY-Pw2ufd20zRmJURzOjpCp4dGhVsdm7229htbS1S5L-S8nV_PxtBLvLj673mWv3mqZMkrTbGSXaYfaKCvg_IAAAA&shmds=v1_AUFQtOP9iZQ66Kv7INMi_RlEChm8JRsoZtQdm-mHC2A1bMCEjw&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=cjaOBtTM3pCNbFBHAAAAAA%3D%3D,6 days ago
Lead Data Engineer ( YC startup),Founding Teams,"Bareilly, Uttar Pradesh, India","Company Description

FoundingTeams.ai is an AI accelerator and talent platform. We enable AI and Tech founders to hire commercial and technical teams for their startups. We also hold a demo day in front of Angel and VC investors.

We work with startups from top accelerators like Ycombinator and TechStars. We are currently hiring for avsolutions.org.uk . This is a Ycombinator funded startup. They are building the future of face recognition payment systems for both retail and e-commerce.

Role Description

This is a part-time remote role for a Lead Data Engineer at avsolutions.org.uk . The Lead Data Engineer will be responsible for managing and developing data pipelines, conducting data analysis, implementing data modeling, and leading a team of data engineers. Additionally, the Lead Data Engineer will collaborate with cross-functional teams to design and implement data solutions.

Qualifications
• Min 6+ years in Python, MLL, AI, and LLM good with Machine learning languages.
• Good to have experience working with Payment gateways and Fintech
• Good to have experience in NLP / MFA
• Good to have Bio-metric knowledge
• Data Engineering, Data Analysis, and Data Modeling skills
• Experience in managing and developing data pipelines
• Leadership and team management skills
• Collaboration and communication skills
• Experience with database technologies and cloud platforms
• Bachelor's degree in Computer Science, Engineering, or related field
• Knowledge of machine learning and AI concepts is a plus",https://www.google.com/search?ibp=htl;jobs&q=Data+Engineer&htidocid=rohE04C_aHuZDpLmAAAAAA%3D%3D&hl=en-US&shndl=37&shmd=H4sIAAAAAAAA_xXNvwrCMBCAcVz7CE43qtRGBBF08y-Kg4MOTuXaHkkkzYVcCvWNfEzr8k0_-LLvKFvfCBs4YEI4em09UYQJvPYgCWPqwhTmcOUKhDDWBtjDmVk7Gm9NSkE2Som4Qg862bqouVXsqeJevbmSf0oxGCk4TFQuV4u-CF7PFifufGO9hgdhK2A97AZlnfvk8EzDGu4RGxKTw2WA-ANbniqjqwAAAA&shmds=v1_AUFQtOMuvvX7xyHqBzQxvXvJI-yXeOVc9Ovl_Q3rptfpGleXsA&source=sh/x/job/li/m1/1#fpstate=tldetail&htivrt=jobs&htiq=Data+Engineer&htidocid=rohE04C_aHuZDpLmAAAAAA%3D%3D,19 days ago
